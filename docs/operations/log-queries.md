# Audit Logging Queries - Story 3.7

This document provides practical examples for querying structured JSON audit logs generated by the AI Agents enhancement system. All logs are emitted in JSON format to stdout/stderr for Kubernetes log aggregation.

## Prerequisites

- Kubernetes cluster with log aggregation enabled (Fluentd, Fluent Bit, or Grafana Loki)
- `kubectl` CLI access to the cluster
- `jq` command-line JSON processor installed
- Access to tenant ID and ticket ID information

---

## Basic Queries

### 1. Find All Enhancements for a Specific Tenant

```bash
kubectl logs -n ai-agents -l app=ai-agents --since=24h | \
  jq 'select(.tenant_id == "tenant-abc")'
```

**Use Case:** Audit compliance for a specific customer.
**Output:** All log events for the tenant, sorted by timestamp.

---

### 2. Trace a Single Ticket's Complete Journey

```bash
kubectl logs -n ai-agents -l app=ai-agents --since=24h | \
  jq 'select(.ticket_id == "TKT-12345")' | \
  jq -s 'sort_by(.timestamp)'
```

**Use Case:** Debug a specific enhancement; see full lifecycle of a ticket.
**Output:** All events for TKT-12345 in chronological order.

---

### 3. Find All Failures in Last 24 Hours

```bash
kubectl logs -n ai-agents -l app=ai-agents --since=24h | \
  jq 'select(.status == "failure")'
```

**Use Case:** Identify issues requiring attention or support escalation.
**Output:** All ERROR-level events where status="failure".

---

### 4. Track a Correlation ID Across All Services

```bash
kubectl logs -n ai-agents --all-containers --since=1h | \
  jq 'select(.correlation_id == "550e8400-e29b-41d4-a716-446655440000")'
```

**Use Case:** Distributed tracing; follow a request through all services.
**Output:** All logs for that correlation ID (API, Worker, Workflow, ServiceDesk).

---

### 5. Identify High-Latency Enhancements (>60s)

```bash
kubectl logs -n ai-agents -l app=ai-agents --since=24h | \
  jq 'select(.duration_ms > 60000) | {ticket_id, tenant_id, duration_ms, timestamp}'
```

**Use Case:** Performance analysis; identify slow enhancements.
**Output:** Ticket ID, tenant, duration, and timestamp of long-running enhancements.

---

### 6. Count Enhancements by Status

```bash
kubectl logs -n ai-agents -l app=ai-agents --since=24h | \
  jq 'select(.operation == "enhancement_completed" or .operation == "enhancement_failed")' | \
  jq -s 'group_by(.status) | map({status: .[0].status, count: length})'
```

**Use Case:** Success/failure rate reporting.
**Output:** Breakdown of completed vs. failed enhancements.

---

### 7. Find API Call Failures

```bash
kubectl logs -n ai-agents -l app=ai-agents --since=24h | \
  jq 'select(.operation == "api_call" and .status == "failure")'
```

**Use Case:** ServiceDesk Plus API integration issues.
**Output:** All failed API calls with status codes and error messages.

---

### 8. Filter by Log Level

```bash
# Show only ERROR-level logs
kubectl logs -n ai-agents -l app=ai-agents --since=24h | \
  jq 'select(.level == "ERROR")'

# Show only WARNING logs
kubectl logs -n ai-agents -l app=ai-agents --since=24h | \
  jq 'select(.level == "WARNING")'
```

**Use Case:** Focus on problems without noise.
**Output:** Only specified severity level events.

---

## Advanced Analysis

### 1. Average Enhancement Duration Per Tenant

```bash
kubectl logs -n ai-agents -l app=ai-agents --since=24h | \
  jq 'select(.operation == "enhancement_completed") | {tenant_id, duration_ms}' | \
  jq -s 'group_by(.tenant_id) | map({
    tenant: .[0].tenant_id,
    count: length,
    avg_duration_ms: (map(.duration_ms) | add / length),
    min_duration_ms: (map(.duration_ms) | min),
    max_duration_ms: (map(.duration_ms) | max)
  })'
```

**Use Case:** Performance SLA monitoring by customer.
**Output:** Average, min, max enhancement duration per tenant.

---

### 2. Error Rate by Operation Type

```bash
kubectl logs -n ai-agents -l app=ai-agents --since=24h | \
  jq 'select(.operation != null) | {operation, status}' | \
  jq -s 'group_by(.operation) | map({
    operation: .[0].operation,
    success_count: map(select(.status == "success")) | length,
    failure_count: map(select(.status == "failure")) | length,
    total: length,
    error_rate_pct: ((map(select(.status == "failure")) | length) / length * 100 | round)
  })'
```

**Use Case:** Operational health dashboard; identify weak points.
**Output:** Success/failure counts and error rate by operation.

---

### 3. Worker Processing Latency

```bash
kubectl logs -n ai-agents -l component=worker --since=24h | \
  jq 'select(.operation == "enhancement_completed") | {worker_id, duration_ms, timestamp}' | \
  jq -s 'group_by(.worker_id) | map({
    worker: .[0].worker_id,
    task_count: length,
    avg_duration_ms: (map(.duration_ms) | add / length | round),
    p95_duration_ms: (map(.duration_ms) | sort | .[length * 0.95] | round),
    p99_duration_ms: (map(.duration_ms) | sort | .[length * 0.99] | round)
  })'
```

**Use Case:** Worker pool performance; identify bottlenecks.
**Output:** Average and percentile latencies per worker.

---

### 4. Sensitive Data Redaction Verification

```bash
kubectl logs -n ai-agents -l app=ai-agents --since=24h | \
  jq 'select(.message != null) | {message, extra}' | \
  grep -i "api_key\|password\|Bearer" || echo "âœ… No unredacted secrets found"
```

**Use Case:** Security audit; ensure secrets are masked.
**Output:** Any unredacted secrets (should be empty).

---

### 5. Context Gathering Performance

```bash
kubectl logs -n ai-agents -l component=worker --since=24h | \
  jq 'select(.operation | startswith("context")) | {operation, total_time_ms}' | \
  jq -s 'group_by(.operation) | map({
    operation: .[0].operation,
    count: length,
    avg_time_ms: (map(.total_time_ms) | add / length | round)
  })'
```

**Use Case:** Optimize workflow node latency.
**Output:** Average time per context gathering operation.

---

### 6. API Retry Analysis

```bash
kubectl logs -n ai-agents -l component=api --since=24h | \
  jq 'select(.operation == "api_call") | {ticket_id, status, extra}' | \
  jq 'select(.extra.retry_exhausted == true)'
```

**Use Case:** Identify persistent API failures.
**Output:** API calls that exhausted all retry attempts.

---

### 7. Correlation ID Statistics

```bash
kubectl logs -n ai-agents -l app=ai-agents --since=24h | \
  jq -s 'group_by(.correlation_id) | map({
    correlation_id: .[0].correlation_id,
    event_count: length,
    services: map(.service) | unique,
    duration_from_first_to_last: (.[length-1].timestamp - .[0].timestamp | floor)
  }) | sort_by(.event_count) | reverse | .[0:10]'
```

**Use Case:** Trace completeness; see what services touched each request.
**Output:** Top 10 most active correlation IDs with event counts and service list.

---

## Log Format Schema Reference

| Field | Type | Required | Description | Example |
|-------|------|----------|-------------|---------|
| `timestamp` | ISO-8601 | Yes | Event timestamp in UTC | `2025-11-03T14:23:45.123Z` |
| `level` | String | Yes | Log level | `INFO`, `WARNING`, `ERROR`, `DEBUG`, `CRITICAL` |
| `message` | String | Yes | Human-readable message | `"Enhancement completed"` |
| `tenant_id` | String | Yes | Tenant identifier | `"tenant-abc"` |
| `ticket_id` | String | Yes | Ticket ID being processed | `"TKT-12345"` |
| `correlation_id` | UUID | Yes | Request correlation ID | `"550e8400-e29b-41d4-a716-446655440000"` |
| `operation` | String | Yes | Operation type | `"webhook_received"`, `"enhancement_started"`, `"api_call"`, `"enhancement_failed"` |
| `status` | String | Yes | Operation status | `"success"`, `"failure"`, `"started"`, `"received"`, `"queued"` |
| `service` | String | No | Service name | `"api"`, `"worker"`, `"workflow"` |
| `environment` | String | No | Deployment environment | `"production"`, `"staging"` |
| `duration_ms` | Integer | Conditional | Processing duration (enhancement_completed) | `5432` |
| `task_id` | String | Conditional | Celery task ID (worker logs) | `"7f8a9b0c-1d2e-3f4a-5b6c-7d8e9f0a1b2c"` |
| `worker_id` | String | Conditional | Celery worker hostname (worker logs) | `"celery@worker-1"` |
| `status_code` | Integer | Conditional | HTTP status code (api_call) | `200`, `404`, `500` |
| `endpoint` | String | Conditional | API endpoint (api_call) | `"servicedesk/v3/requests"` |
| `method` | String | Conditional | HTTP method (api_call) | `"POST"`, `"GET"`, `"PUT"` |
| `error_type` | String | Conditional | Exception type (failures) | `"ValidationError"`, `"TimeoutError"` |
| `error_message` | String | Conditional | Error description (failures) | `"Field validation failed"` |
| `extra` | Object | No | Additional context fields | `{attempt_number: 2, retry_count: 3}` |

---

## Real-World Examples

### Example 1: Debugging a Failed Enhancement

```bash
# Find the failure
TICKET="TKT-12345"
kubectl logs -n ai-agents -l app=ai-agents --since=1h | \
  jq "select(.ticket_id == \"$TICKET\" and .status == \"failure\")" | head -20

# Extract correlation ID
CORR_ID=$(kubectl logs -n ai-agents -l app=ai-agents --since=1h | \
  jq -r "select(.ticket_id == \"$TICKET\") | .correlation_id" | head -1)

# Trace through entire lifecycle
echo "=== Full trace for $CORR_ID ==="
kubectl logs -n ai-agents --all-containers --since=1h | \
  jq "select(.correlation_id == \"$CORR_ID\")" | \
  jq -s 'sort_by(.timestamp) | .[] |
    "\(.timestamp) [\(.level)] \(.operation) - \(.message)"'
```

### Example 2: Generate Daily Metrics Report

```bash
#!/bin/bash
# Generate daily SLA report

DATE=$(date -u "+%Y-%m-%d")
NAMESPACE="ai-agents"

echo "# Daily Enhancement Report - $DATE"
echo ""

# Success rate
echo "## Success Rate"
kubectl logs -n $NAMESPACE -l app=ai-agents --since=24h | \
  jq 'select(.operation == "enhancement_completed" or .operation == "enhancement_failed")' | \
  jq -s '{
    total: length,
    success: map(select(.status == "success")) | length,
    failed: map(select(.status == "failure")) | length
  } | "Success: \(.success)/\(.total) (\((.success/.total*100) | round)%)"'

echo ""

# Average latency
echo "## Performance (ms)"
kubectl logs -n $NAMESPACE -l app=ai-agents --since=24h | \
  jq 'select(.operation == "enhancement_completed" and .duration_ms != null)' | \
  jq -s '{
    avg: (map(.duration_ms) | add / length | round),
    min: (map(.duration_ms) | min),
    max: (map(.duration_ms) | max),
    p95: (map(.duration_ms) | sort | .[length*0.95] | round)
  } | "Avg: \(.avg)ms | Min: \(.min)ms | Max: \(.max)ms | P95: \(.p95)ms"'
```

---

## Troubleshooting Common Issues

| Issue | Query | Solution |
|-------|-------|----------|
| High failure rate | Count `status == "failure"` by operation | Check error_message and error_type fields for patterns |
| Slow enhancements | Filter `duration_ms > 60000` | Review context gathering and LLM synthesis latencies |
| Missing logs | Check `service` field | Verify all components are running and logging |
| Correlation ID gaps | Count events per correlation_id | Check for service-to-service call failures |
| Data privacy concerns | Grep for patterns (API keys, PII) | Verify SensitiveDataFilter is active |

---

## Integration with Monitoring Systems

### Prometheus Metrics from Logs

```bash
# Extract error rate for Prometheus
kubectl logs -n ai-agents -l app=ai-agents --since=5m | \
  jq 'select(.operation == "enhancement_failed")' | \
  wc -l > /metrics/enhancement_errors_5m.txt
```

### Grafana Dashboard Query (LogQL)

```
{job="ai-agents"} | json | status = "failure" | line_format "{{.error_type}}"
```

### Alert Rules (Prometheus)

```yaml
groups:
  - name: ai-agents
    rules:
      - alert: HighEnhancementFailureRate
        expr: rate(log_error_total[5m]) > 0.05
        annotations:
          summary: "Enhancement failure rate > 5%"
```

---

## Best Practices

1. **Always include timestamp**: Use `--since=` for all queries to limit data volume.
2. **Filter by namespace**: Use `-n ai-agents` to isolate your logs.
3. **Start with basic queries**: Build complex queries incrementally.
4. **Use correlation IDs**: They're your best friend for distributed tracing.
5. **Monitor redaction**: Regularly audit logs for unredacted sensitive data.
6. **Archive logs**: Retention is 90 days; export to S3/GCS for long-term compliance.

---

## Additional Resources

- [Kubernetes Logging Architecture](https://kubernetes.io/docs/concepts/cluster-administration/logging/)
- [Fluentd Documentation](https://docs.fluentd.org/)
- [jq Manual](https://stedolan.github.io/jq/manual/)
- [Loguru Documentation](https://loguru.readthedocs.io/)
