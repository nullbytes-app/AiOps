<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>5</epicId>
    <storyId>5.1</storyId>
    <title>Provision Production Kubernetes Cluster</title>
    <status>drafted</status>
    <generatedAt>2025-11-03</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/5-1-provision-production-kubernetes-cluster.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>As a DevOps engineer,</asA>
    <iWant>I want a production Kubernetes cluster configured for high availability,</iWant>
    <soThat>So that the platform runs reliably for real clients.</soThat>
    <tasks>
      <task id="1.1">Evaluate cloud providers (AWS EKS, GCP GKE, Azure AKS) and document selection decision</task>
      <task id="1.2">Create Terraform/Pulumi configuration for cloud provider setup (provider authentication, project/account setup)</task>
      <task id="1.3">Provision Kubernetes cluster with HA configuration (3+ nodes in separate availability zones)</task>
      <task id="1.4">Verify cluster creation: `kubectl get nodes` shows 3+ Ready nodes across multiple AZs</task>
      <task id="2.1">Configure auto-scaling policies: HPA rules for min/max node count (1-10 nodes)</task>
      <task id="2.2">Define network policies: default deny ingress, explicit allow for ingress controller, inter-pod communication</task>
      <task id="2.3">Configure RBAC: create service accounts for application, Prometheus, pod-level role bindings</task>
      <task id="2.4">Deploy pod security policies (or pod security standards in newer K8s versions)</task>
      <task id="2.5">Verify configuration: run kubectl auth can-i checks for all service accounts</task>
      <task id="3.1">Provision managed PostgreSQL (RDS/Cloud SQL) in same region as cluster</task>
      <task id="3.2">Configure backups: automated daily backups, 30-day retention</task>
      <task id="3.3">Enable encryption at rest using cloud provider KMS keys</task>
      <task id="3.4">Configure database for multi-tenant isolation: enable row-level security</task>
      <task id="3.5">Set up read replica or Multi-AZ deployment for high availability</task>
      <task id="3.6">Verify connectivity from cluster pods to database (test via test pod with psql)</task>
      <task id="4.1">Provision managed Redis (ElastiCache/MemoryStore) in same region as cluster</task>
      <task id="4.2">Enable persistence: RDB snapshots at least hourly</task>
      <task id="4.3">Configure replication: primary + replica setup for failover</task>
      <task id="4.4">Set memory eviction policy: allkeys-lru (appropriate for message queue + caching)</task>
      <task id="4.5">Verify connectivity from cluster pods to Redis (test via test pod with redis-cli)</task>
      <task id="5.1">Deploy ingress controller (nginx-ingress recommended for cloud-agnostic deployment)</task>
      <task id="5.2">Install cert-manager for automatic TLS certificate provisioning</task>
      <task id="5.3">Configure Let's Encrypt ACME issuer (staging for testing, production for live)</task>
      <task id="5.4">Create ingress resource with TLS certificate annotations</task>
      <task id="5.5">Verify TLS: curl HTTPS endpoint, verify certificate validity and renewal automation</task>
      <task id="6.1">Configure cloud provider Container Insights / Cloud Logging / Azure Monitor</task>
      <task id="6.2">Enable cluster logging: API server, control plane, audit logs</task>
      <task id="6.3">Set up log retention (90 days minimum for compliance)</task>
      <task id="6.4">Create cloud dashboards showing cluster health (node status, pod count, resource utilization)</task>
      <task id="6.5">Configure log streaming to existing ELK/Splunk if available, or use cloud provider solution</task>
      <task id="7.1">Create comprehensive Terraform/Pulumi modules with clear variable definitions</task>
      <task id="7.2">Document all variables: what they control, valid values, default values</task>
      <task id="7.3">Create example tfvars files for each cloud provider (aws-example.tfvars, gcp-example.tfvars)</task>
      <task id="7.4">Write terraform/pulumi documentation: "Getting Started", "Cluster Configuration", "Updating Production"</task>
      <task id="7.5">Test IaC: destroy and recreate cluster using IaC scripts to verify reproducibility</task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="1">Cloud Provider Cluster Provisioned: Production Kubernetes cluster deployed on AWS EKS, GCP GKE, or Azure AKS with high availability configuration</criterion>
    <criterion id="2">Cluster Configuration: Minimum 3 nodes, auto-scaling enabled (1-10 nodes based on load), network policies enforced, RBAC configured</criterion>
    <criterion id="3">Managed Database: Production-grade PostgreSQL with automated backups, encryption at rest, and automated failover (RDS/Cloud SQL)</criterion>
    <criterion id="4">Managed Cache: Production Redis with persistence enabled, automated backups, and replication (ElastiCache/MemoryStore)</criterion>
    <criterion id="5">Ingress Configuration: Ingress controller deployed with TLS certificates provisioned (Let's Encrypt via cert-manager)</criterion>
    <criterion id="6">Cloud Monitoring Integration: Cluster metrics and logs streamed to cloud provider observability service (CloudWatch/Stackdriver/Azure Monitor)</criterion>
    <criterion id="7">Infrastructure as Code: All infrastructure provisioned via Terraform or Pulumi scripts with documentation for reproducible deployments</criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/PRD.md</path>
        <title>Product Requirements Document</title>
        <section>Non-Functional Requirements (NFR002 Scalability, NFR004 Security, NFR005 Observability)</section>
        <snippet>System shall support horizontal scaling via Kubernetes HPA to handle variable ticket volumes, automatically scaling worker pods from 1 to 10 based on Redis queue depth. System shall enforce data isolation between tenants using row-level security, encrypt credentials at rest using Kubernetes secrets, and apply input validation. System shall provide real-time visibility into agent operations through... audit logs retained for 90 days.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>System Architecture</title>
        <section>Deployment Infrastructure - Kubernetes 1.28+, Container Runtime, Orchestration</section>
        <snippet>Container orchestration via Kubernetes 1.28+ with HPA autoscaling, managed services (EKS/GKE/AKS), production-grade, and monitoring integration. Gunicorn + Uvicorn workers for Production ASGI server.</snippet>
      </doc>
      <doc>
        <path>docs/epics.md</path>
        <title>Epic 5: Production Deployment & Validation</title>
        <section>Story 5.1 - Provision Production Kubernetes Cluster</section>
        <snippet>Deploy the platform to production infrastructure, onboard the first real client, and validate system behavior with actual ticket data. Story 5.1 provisions production cluster on cloud provider (AWS EKS, GCP GKE, or Azure AKS) with high availability configuration, managed database, managed Redis, ingress controller with TLS, cloud monitoring integration, and infrastructure-as-code.</snippet>
      </doc>
      <doc>
        <path>docs/operations/README.md</path>
        <title>Operations Documentation Index</title>
        <section>Operational Readiness and Runbooks</section>
        <snippet>Centralized index of operational procedures established in Story 4.7. Includes patterns for "runbooks before incidents", runbook validation processes, and documentation-first operations approach. Story 5.1 should follow similar depth for production infrastructure documentation.</snippet>
      </doc>
      <doc>
        <path>docs/operations/distributed-tracing-setup.md</path>
        <title>Distributed Tracing Setup Guide</title>
        <section>Operational Integration with Production Infrastructure</section>
        <snippet>1650+ line operational guide from Story 4.6 demonstrating depth of infrastructure documentation. Shows pattern for comprehensive setup guides including architecture diagrams, configuration procedures, verification checklists.</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>k8s/</path>
        <kind>kubernetes-manifests</kind>
        <symbol>Kubernetes deployment definitions</symbol>
        <reason>Existing Kubernetes manifests for development/staging. Story 5.1 creates production-specific manifests in k8s/production/</reason>
      </artifact>
      <artifact>
        <path>infrastructure/</path>
        <kind>terraform-modules</kind>
        <symbol>Infrastructure as Code definitions (to be created)</symbol>
        <reason>Core deliverable: Terraform/Pulumi modules for cloud provider setup, cluster provisioning, database, cache, ingress configuration</reason>
      </artifact>
      <artifact>
        <path>docker-compose.yml</path>
        <kind>local-environment</kind>
        <symbol>Local development orchestration</symbol>
        <reason>Shows patterns for PostgreSQL, Redis, and Kubernetes configuration. Story 5.1 translates these patterns to managed cloud services.</reason>
      </artifact>
      <artifact>
        <path>docs/operations/</path>
        <kind>documentation</kind>
        <symbol>Operational runbooks and guides</symbol>
        <reason>Story 4.7 established pattern of comprehensive operational documentation. Story 5.1 needs parallel production cluster documentation.</reason>
      </artifact>
    </code>
    <dependencies>
      <dependency>
        <name>Terraform</name>
        <version>1.0+</version>
        <ecosystem>infrastructure-as-code</ecosystem>
        <purpose>Infrastructure provisioning for cloud resources (cluster, database, cache, ingress)</purpose>
      </dependency>
      <dependency>
        <name>kubectl</name>
        <version>1.28+</version>
        <ecosystem>kubernetes-client</ecosystem>
        <purpose>Kubernetes cluster management and verification</purpose>
      </dependency>
      <dependency>
        <name>helm</name>
        <version>3.0+</version>
        <ecosystem>kubernetes-package-manager</ecosystem>
        <purpose>Deploy nginx-ingress controller and cert-manager via Helm charts</purpose>
      </dependency>
      <dependency>
        <name>cert-manager</name>
        <version>1.10+</version>
        <ecosystem>kubernetes</ecosystem>
        <purpose>Automatic TLS certificate provisioning and renewal via Let's Encrypt</purpose>
      </dependency>
      <dependency>
        <name>nginx-ingress-controller</name>
        <version>4.0+</version>
        <ecosystem>kubernetes</ecosystem>
        <purpose>Cloud-agnostic ingress controller for routing traffic to application</purpose>
      </dependency>
      <dependency>
        <name>docker</name>
        <version>20.10+</version>
        <ecosystem>containerization</ecosystem>
        <purpose>Container runtime for Kubernetes nodes, container image building</purpose>
      </dependency>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>
      <category>cloud-agnostic</category>
      <rule>Avoid hard-coding cloud-specific features. Implementation should work on AWS EKS, GCP GKE, or Azure AKS without modification. Use Terraform variables to abstract cloud-specific differences.</rule>
      <source>Architecture.md</source>
    </constraint>
    <constraint>
      <category>high-availability</category>
      <rule>Kubernetes control plane: cloud provider manages this (replicated across AZs by default in EKS/GKE/AKS). Worker nodes must span 3+ availability zones for rack-aware redundancy. Database and cache must use multi-AZ or read replica setup with automatic failover.</rule>
      <source>Story 5.1 Dev Notes</source>
    </constraint>
    <constraint>
      <category>security-posture</category>
      <rule>RBAC: principle of least privilege - service accounts only have permissions they need. Network policies: default deny, explicit allow for required traffic. Secrets management: use Kubernetes secrets (encrypted at rest via cloud provider), migrate to vault later if needed. Pod security: enable pod security policies to prevent privileged containers, root users. Encryption: database encryption at rest via cloud KMS, TLS in transit for all connections.</rule>
      <source>Story 5.1 Dev Notes</source>
    </constraint>
    <constraint>
      <category>compliance</category>
      <rule>Log retention must be 90 days minimum for audit requirements (PRD NFR005). All infrastructure changes must be version-controlled and reviewable in Terraform code.</rule>
      <source>PRD.md</source>
    </constraint>
    <constraint>
      <category>operational-readiness</category>
      <rule>Production cluster documentation must follow Story 4.7 pattern: "runbooks before incidents". Documentation-first operations approach requires operational procedures exist before transitioning to production. Infrastructure documentation should be 1000+ lines with architecture diagrams, setup procedures, verification checklists.</rule>
      <source>Story 5.1 Dev Notes - Learnings from Story 4.7</source>
    </constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>Kubernetes API</name>
      <kind>REST endpoint</kind>
      <signature>kubectl connect to cluster endpoint, authenticate with service account tokens or IAM roles</signature>
      <path>Cloud provider control plane (AWS/GCP/Azure managed)</path>
    </interface>
    <interface>
      <name>Cloud Provider IAM</name>
      <kind>API</kind>
      <signature>AWS: IAM roles/policies. GCP: Service accounts/IAM roles. Azure: Role-based access control (RBAC)</signature>
      <path>Cloud provider authentication layer</path>
    </interface>
    <interface>
      <name>PostgreSQL Connection</name>
      <kind>Database connection</kind>
      <signature>Connection string: postgresql://user:password@host:5432/dbname (from RDS/Cloud SQL endpoint)</signature>
      <path>Application deployment manifests</path>
    </interface>
    <interface>
      <name>Redis Connection</name>
      <kind>Cache connection</kind>
      <signature>Connection string: redis://password@host:6379 (from ElastiCache/MemoryStore endpoint)</signature>
      <path>Application deployment manifests</path>
    </interface>
    <interface>
      <name>Ingress Controller</name>
      <kind>Kubernetes API</kind>
      <signature>Ingress resource with annotations for cert-manager TLS provisioning</signature>
      <path>k8s/production/ingress.yaml</path>
    </interface>
    <interface>
      <name>Cloud Monitoring API</name>
      <kind>REST/gRPC API</kind>
      <signature>CloudWatch (AWS), Cloud Logging (GCP), Azure Monitor (Azure)</signature>
      <path>Cloud provider observability service</path>
    </interface>
  </interfaces>

  <tests>
    <standards>
      Story 5.1 focuses on infrastructure provisioning rather than application code, so testing emphasizes verification and operational readiness. Testing framework follows patterns from Story 4.6 (Distributed Tracing) and Story 4.7 (Operational Runbooks): comprehensive operational procedures with manual verification checklists before declaring production-ready. Key testing patterns: Infrastructure validation (terraform plan/apply idempotency), cluster health checks (node readiness, pod startup), connectivity verification (pod-to-database, pod-to-cache, pod-to-ingress), security validation (RBAC permissions, network policy rules), observability testing (logs/metrics flowing to cloud provider), and disaster recovery testing (database failover, cache persistence, node termination recovery).
    </standards>
    <locations>
      Kubernetes cluster test procedures: kubectl commands for cluster health checks. Cloud provider test procedures: Console-based verification (CloudWatch dashboards, Cloud Logging queries, container insights). Terraform testing: terraform plan validation, terraform destroy/recreate cycles. Documentation: docs/operations/production-cluster-setup.md will contain step-by-step verification procedures.
    </locations>
    <ideas>
      Test AC1 (Cloud Provider Cluster): Verify cluster creation by running `kubectl get nodes` - all nodes showing Ready status across 3+ AZs. Verify via cloud provider console (EC2 instances for AWS, Compute Engine for GCP, Virtual Machines for Azure). Test AC2 (Cluster Configuration): Run `kubectl auth can-i list pods --as=system:serviceaccount:default:app` to verify RBAC. Verify network policies by attempting cross-pod communication (should fail by default, then succeed after explicit allow rules). Test AC3 (Database): Spin up test pod and connect to RDS/Cloud SQL endpoint using psql. Verify backup automation in cloud console. Test AC4 (Cache): Spin up test pod and connect to ElastiCache/MemoryStore using redis-cli. Verify persistence by triggering failover and checking data retention. Test AC5 (Ingress): Curl HTTPS endpoint, verify certificate chain validity, trigger Let's Encrypt renewal and verify automation. Test AC6 (Cloud Monitoring): Generate test logs and metrics, verify they appear in cloud dashboards within 1-2 minutes. Test AC7 (IaC): Terraform destroy entire infrastructure and terraform apply to recreate - should be identical (idempotent).
    </ideas>
  </tests>
</story-context>
