<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>8</epicId>
    <storyId>1</storyId>
    <title>LiteLLM Proxy Integration</title>
    <status>drafted</status>
    <generatedAt>2025-11-05</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/8-1-litellm-proxy-integration.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>a platform engineer</asA>
    <iWant>LiteLLM proxy integrated as a Docker service</iWant>
    <soThat>the platform can route LLM requests through a unified gateway with multi-provider support, cost tracking, and automatic fallbacks</soThat>
    <tasks>
### Task 1: Add LiteLLM Service to Docker Compose (AC: #1, #3)
- 1.1 Read current docker-compose.yml to understand existing service structure
- 1.2 Add litellm service definition (image, container, ports, network, volumes, command)
- 1.3 Configure environment variables (DATABASE_URL, LITELLM_MASTER_KEY, LITELLM_SALT_KEY, provider API keys)
- 1.4 Add depends_on: postgres
- 1.5 Add healthcheck configuration

### Task 2: Create LiteLLM Configuration File (AC: #2, #5, #6)
- 2.1 Create config/ directory if needed
- 2.2 Create config/litellm-config.yaml structure (model_list, router_settings, general_settings)
- 2.3 Define fallback chain: gpt-4 → azure-gpt-4 → claude-3-5-sonnet
- 2.4 Configure retry logic (3 retries, exponential backoff, 30s timeout)
- 2.5 Configure router_settings (simple-shuffle, fallbacks, context_window_fallbacks)
- 2.6 Configure general_settings (master_key, database_url, litellm_settings)

### Task 3: Update Environment Variables (AC: #4)
- 3.1 Read .env.example structure
- 3.2 Add LITELLM_* variables with examples
- 3.3 Add comments explaining each variable
- 3.4 Verify .env in .gitignore
- 3.5 Add setup script for LITELLM_SALT_KEY generation

### Task 4: Verify Database Integration (AC: #3)
- 4.1-4.5 Start services, verify LiteLLM table creation
- 4.6 Test virtual key creation via API

### Task 5: Test Fallback Chain Configuration (AC: #5)
- 5.1 Create test script
- 5.2-5.4 Test primary and fallback providers
- 5.5-5.6 Document results

### Task 6: Test Retry Logic (AC: #6)
- 6.1 Create test script
- 6.2-6.4 Test exponential backoff, timeout, allowed_fails

### Task 7: Verify Health Check Endpoint (AC: #7)
- 7.1-7.4 Test health endpoint, docker health check, failure scenarios

### Task 8: Update Documentation (AC: #8)
- 8.1-8.8 Add LiteLLM section to README.md (overview, setup, configuration, virtual keys, monitoring, troubleshooting)

### Task 9: Create Unit Tests
- 9.1-9.4 Configuration parsing, environment validation tests

### Task 10: Create Integration Tests
- 10.1-10.6 Health check, chat completion, virtual key tests

### Task 11: Quality Assurance and Validation
- 11.1 Verify all ACs met
- 11.2 Run all tests
- 11.3 Security validation
- 11.4 Performance validation
- 11.5 Documentation quality check
</tasks>
  </story>

  <acceptanceCriteria>
AC1: LiteLLM service added to docker-compose.yml (image: ghcr.io/berriai/litellm-database:main-stable)
AC2: config/litellm-config.yaml created with default providers (OpenAI, Anthropic, Azure fallback)
AC3: LiteLLM uses existing PostgreSQL database for virtual key storage
AC4: Environment variables configured: LITELLM_MASTER_KEY, OPENAI_API_KEY, ANTHROPIC_API_KEY
AC5: Fallback chain configured: gpt-4 → azure-gpt-4 → claude-3-5-sonnet
AC6: Retry logic configured: 3 attempts, exponential backoff, 30s timeout
AC7: Health check endpoint verified: /health returns 200
AC8: Documentation updated: README.md with LiteLLM setup instructions
</acceptanceCriteria>

  <artifacts>
    <docs>
      <doc path="docs/epics.md" title="Epic 8 Definition" section="Epic 8: AI Agent Orchestration Platform">
        Story 8.1 is the foundation story for Epic 8, introducing LiteLLM proxy as a unified LLM gateway. Epic transforms platform from single-purpose (ticket enhancement) to general-purpose agent orchestration. Uses ghcr.io/berriai/litellm-database:main-stable for PostgreSQL integration. Fallback chain: gpt-4 → azure-gpt-4 → claude-3-5-sonnet. Retry logic: 3 attempts, exponential backoff, 30s timeout.
      </doc>
      <doc path="docs/architecture.md" title="Technology Stack" section="Core Technologies">
        Existing infrastructure: PostgreSQL 17 (database), Redis 7.x (message broker + cache), FastAPI (async web framework), Celery 5.x (task queue), Docker Compose (local dev). Network: backend (bridge network). Database: ai_agents (user: aiagents, password: password). LiteLLM integrates with existing PostgreSQL for virtual key storage, no schema changes needed.
      </doc>
      <doc path="Context7 MCP: /berriai/litellm" title="LiteLLM Docker Deployment (2025)" section="Database Integration Best Practices">
        Docker image: ghcr.io/berriai/litellm-database:main-stable includes psycopg3 driver for PostgreSQL integration. LiteLLM automatically creates tables: litellm_verificationtoken (virtual API keys), litellm_usertable (multi-tenancy), litellm_budgettable (spend tracking). Set STORE_MODEL_IN_DB=True and DATABASE_URL to enable database storage. Docker command: docker run -e DATABASE_URL=postgresql://user:pass@host:port/dbname -p 4000:4000 ghcr.io/berriai/litellm-database:main-stable --config /app/config.yaml
      </doc>
      <doc path="Context7 MCP: /berriai/litellm" title="Fallback Chain Configuration" section="Model Routing">
        LiteLLM uses model_name as routing key. Multiple providers with same model_name enable automatic fallback. Example: All providers use model_name: gpt-4. Primary: openai/gpt-4. Fallback 1: azure/gpt-4. Fallback 2: anthropic/claude-3-5-sonnet-20241022. Configure router_settings with fallbacks array and routing_strategy: simple-shuffle for load balancing.
      </doc>
      <doc path="Context7 MCP: /berriai/litellm" title="Retry and Timeout Configuration" section="Reliability Settings">
        Exponential backoff: 2s, 4s, 8s delays between retries. Configure num_retries: 3, retry_policy: exponential_backoff_retry, timeout: 30 (seconds), allowed_fails: 3 (switch to fallback after 3 consecutive failures on primary provider). Total max time: 30s timeout + 3 retries = ~44s worst case.
      </doc>
      <doc path="Context7 MCP: /berriai/litellm" title="Security Best Practices" section="Environment Variables">
        LITELLM_MASTER_KEY: Admin API key, format must be sk-*, changeable after setup. LITELLM_SALT_KEY: Encryption key for API credentials, CANNOT change after first model added, use 1Password or similar for cryptographically secure random generation (32-byte base64). Store in .env file (gitignored) or Kubernetes secrets in production.
      </doc>
      <doc path="WebSearch: LiteLLM 2025 Best Practices" title="Docker Compose Setup" section="Production Configuration">
        March 2025 guide: Configure load balancing, fallbacks, and caching for multiple LLM providers. Launch two proxy instances with PostgreSQL database and Redis. Set LITELLM_MASTER_KEY and LITELLM_SALT_KEY. Retry settings: num_retries: 2 in router_settings, timeout: 30. Production: Match Uvicorn workers to CPU count --num_workers "$(nproc)", recycle workers after 10000 requests --max_requests_before_restart "10000" to mitigate memory leaks.
      </doc>
      <doc path="WebSearch: LiteLLM 2025 Best Practices" title="Production Optimizations" section="Performance Settings">
        proxy_batch_write_at: 60 (batch write spend updates every 60s), database_connection_pool_limit: 10 (limit database connections), allow_requests_on_db_unavailable: True (when running LiteLLM on VPC). Use Redis for load balancing state when using multiple litellm proxy deployments.
      </doc>
    </docs>
    <code>
      <artifact path="docker-compose.yml" kind="docker-compose" symbol="N/A" lines="1-100" reason="Existing Docker Compose structure defines postgres, redis, api, worker, prometheus services. LiteLLM service will be added with same patterns (healthcheck, restart: unless-stopped, backend network, env_file: .env). PostgreSQL service on port 5433 (host) → 5432 (container), user: aiagents, password: password, database: ai_agents."/>
      <artifact path=".env.example" kind="environment-config" symbol="N/A" lines="1-267" reason="Template for environment variables. Currently has 267 lines of configuration for PostgreSQL, Redis, Celery, OpenAI, OpenRouter, monitoring, tracing, alerting. LiteLLM variables will be added: LITELLM_MASTER_KEY, LITELLM_SALT_KEY, OPENAI_API_KEY (already exists line 96), ANTHROPIC_API_KEY (new), AZURE_API_KEY (new), AZURE_API_BASE (new)."/>
      <artifact path="tests/conftest.py" kind="test-fixtures" symbol="N/A" lines="N/A" reason="Pytest configuration and shared fixtures. Reference for testing patterns used across project (httpx mock, async fixtures, database fixtures, etc.). Story 8.1 tests will follow same patterns."/>
      <artifact path="tests/unit/" kind="test-directory" symbol="N/A" lines="N/A" reason="Unit tests directory. Story 8.1 will add test_litellm_config.py with tests for config validation, environment variable checks, fallback chain structure, retry policy configuration."/>
      <artifact path="tests/integration/" kind="test-directory" symbol="N/A" lines="N/A" reason="Integration tests directory. Story 8.1 will add test_litellm_integration.py with tests for health check, chat completions, virtual key creation/usage, fallback chain simulation."/>
    </code>
    <dependencies>
      <python>
        <existing path="pyproject.toml">
          - fastapi ~= 0.104.0 (web framework, already installed)
          - sqlalchemy ~= 2.0.0 (ORM, already installed for PostgreSQL integration)
          - pytest ~= 7.4.0 (testing framework, already installed)
          - pytest-asyncio ~= 0.21.0 (async test support, already installed)
          - httpx ~= 0.24.0 (HTTP client, already installed for external API calls)
        </existing>
        <new>
          - litellm (NO - runs as Docker service, not Python dependency)
          - psycopg3 (NO - included in litellm-database Docker image)
        </new>
      </python>
      <docker>
        <new-service>
          - ghcr.io/berriai/litellm-database:main-stable (LiteLLM proxy with PostgreSQL driver)
        </new-service>
      </docker>
      <config>
        <new-file path="config/litellm-config.yaml">
          - model_list: gpt-4 model definitions (OpenAI, Azure, Anthropic)
          - router_settings: fallback configuration, retry logic
          - general_settings: master_key, database_url, litellm_settings
        </new-file>
      </config>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint id="C1" source="CLAUDE.md">File size limit: 500 lines max. Config file ~80 lines, test files ~150-200 lines each, README section ~200 lines. All compliant.</constraint>
    <constraint id="C2" source="Epic 8 Architecture">Foundation story, no dependencies. Enables Stories 8.2-8.17 (agent management, UI, tools).</constraint>
    <constraint id="C3" source="PRD + Architecture">Reuse existing PostgreSQL database (ai_agents). LiteLLM creates own tables with litellm_ prefix, no conflicts with existing schema.</constraint>
    <constraint id="C4" source="Epic 3 Security">No hardcoded API keys. Use environment variables only. LITELLM_SALT_KEY unique per environment (production vs staging). .env file in .gitignore.</constraint>
    <constraint id="C5" source="Docker Best Practices">Health check pattern: curl -f http://localhost:4000/health, interval 30s, timeout 10s, retries 3, start_period 40s.</constraint>
    <constraint id="C6" source="Testing Standards">Minimum 15+ tests total (8+ unit, 5+ integration). Follow pytest patterns from existing tests/conftest.py. Use httpx mock for fallback simulation.</constraint>
    <constraint id="C7" source="Documentation Standards">README.md update ~200 lines: overview, setup instructions, configuration, virtual keys, monitoring, troubleshooting. All curl examples tested and working.</constraint>
    <constraint id="C8" source="Performance Requirements">Health check &lt;500ms (p95), Chat completion &lt;5s (p95), Fallback overhead &lt;1s, Container memory &lt;512MB under load.</constraint>
    <constraint id="C9" source="2025 LiteLLM Best Practices">Use litellm-database:main-stable (not litellm:main-stable) for PostgreSQL integration. Set STORE_MODEL_IN_DB=True. Configure exponential backoff retry with num_retries: 3.</constraint>
    <constraint id="C10" source="Multi-Tenancy (Epic 3)">Virtual keys foundation for per-tenant cost tracking (Story 8.9 will implement). LiteLLM tables prefixed with litellm_, no foreign keys to existing tables in Story 8.1.</constraint>
  </constraints>

  <interfaces>
    <interface name="LiteLLM Health Check" kind="REST endpoint" signature="GET /health" path="litellm:4000/health">
      Returns: HTTP 200 with {"status": "healthy"} or similar. Used by Docker healthcheck. Response time &lt;500ms required.
    </interface>
    <interface name="LiteLLM Chat Completions" kind="REST endpoint" signature="POST /chat/completions" path="litellm:4000/chat/completions">
      OpenAI-compatible chat endpoint. Accepts: model, messages, temperature, max_tokens. Returns: OpenAI format response with choices array. Requires: virtual key or master key in Authorization header.
    </interface>
    <interface name="LiteLLM Virtual Key Generation" kind="REST endpoint" signature="POST /key/generate" path="litellm:4000/key/generate">
      Admin endpoint (requires LITELLM_MASTER_KEY). Creates virtual key for multi-tenant use. Returns: {key: "sk-...", ...}. Key stored in litellm_verificationtoken table.
    </interface>
    <interface name="LiteLLM Metrics" kind="REST endpoint" signature="GET /metrics" path="litellm:4000/metrics">
      Prometheus metrics endpoint (future integration with Epic 4). Returns: Prometheus text format metrics.
    </interface>
    <interface name="PostgreSQL Database" kind="database" signature="postgresql://aiagents:password@postgres:5432/ai_agents" path="docker-compose postgres service">
      Existing database service. LiteLLM connects via DATABASE_URL environment variable. LiteLLM creates tables: litellm_verificationtoken, litellm_usertable, litellm_budgettable, litellm_modeltable (if STORE_MODEL_IN_DB=True).
    </interface>
    <interface name="Docker Backend Network" kind="docker-network" signature="backend (bridge network)" path="docker-compose.yml">
      Existing network for inter-container communication. LiteLLM service joins this network. Services communicate via container names (postgres, redis, api, litellm).
    </interface>
  </interfaces>

  <tests>
    <standards>
      Project uses pytest with pytest-asyncio for async test support. Testing framework established in Epic 1 (Story 1.8). Test structure: tests/unit/ for isolated tests, tests/integration/ for end-to-end tests. Fixtures in tests/conftest.py and tests/fixtures/. Mocking with httpx mock for external API calls. Coverage target: 80%+ for critical paths. Test evidence required: all tests passing, performance validation (&lt;500ms health check, &lt;5s completion).
    </standards>
    <locations>
      - tests/unit/test_litellm_config.py (NEW - configuration validation tests)
      - tests/integration/test_litellm_integration.py (NEW - end-to-end LiteLLM tests)
      - scripts/test-litellm-fallback.sh (NEW - manual fallback chain testing)
      - scripts/test-litellm-retry.sh (NEW - manual retry logic testing)
    </locations>
    <ideas>
      <test-idea id="TI1" ac="AC2,AC5,AC6">Unit test: Load config/litellm-config.yaml and validate structure (model_list, router_settings, general_settings, fallback chain array, retry policy settings).</test-idea>
      <test-idea id="TI2" ac="AC4">Unit test: Validate environment variable formats (LITELLM_MASTER_KEY starts with sk-, LITELLM_SALT_KEY is 32+ bytes base64, DATABASE_URL is valid postgresql:// URL).</test-idea>
      <test-idea id="TI3" ac="AC7">Integration test: GET /health returns HTTP 200 with response time &lt;500ms. Verify docker health check passes.</test-idea>
      <test-idea id="TI4" ac="AC1,AC3">Integration test: LiteLLM service starts successfully, connects to PostgreSQL, creates litellm_* tables.</test-idea>
      <test-idea id="TI5" ac="AC5">Integration test: POST /key/generate with master key creates virtual key, stores in database, returns valid response.</test-idea>
      <test-idea id="TI6" ac="AC5,AC6">Integration test (with httpx mock): Simulate OpenAI failure (500 error), verify LiteLLM retries 3 times then falls back to Azure. Check logs for exponential backoff delays (2s, 4s, 8s).</test-idea>
      <test-idea id="TI7" ac="AC6">Manual test (scripts/test-litellm-retry.sh): Simulate 429 rate limit error, verify 3 retries with exponential backoff. Simulate timeout, verify 30s timeout enforced.</test-idea>
      <test-idea id="TI8" ac="AC8">Documentation test: Execute all curl examples from README.md LiteLLM section, verify commands work and return expected results.</test-idea>
      <test-idea id="TI9" ac="AC4">Security test: Verify .env file in .gitignore. Scan config files for hardcoded secrets (should be none). Run Bandit security scan: bandit -r config/ -ll.</test-idea>
      <test-idea id="TI10" ac="AC1,AC7">Performance test: Measure health check response time (&lt;500ms p95), chat completion response time (&lt;5s p95), Docker container memory usage (&lt;512MB).</test-idea>
    </ideas>
  </tests>
</story-context>
