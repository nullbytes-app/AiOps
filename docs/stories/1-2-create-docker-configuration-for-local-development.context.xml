<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>1</epicId>
    <storyId>1.2</storyId>
    <title>Create Docker Configuration for Local Development</title>
    <status>drafted</status>
    <generatedAt>2025-11-01</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/1-2-create-docker-configuration-for-local-development.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>developer</asA>
    <iWant>Docker containers for all infrastructure components</iWant>
    <soThat>I can run the entire stack locally without manual service installation</soThat>
    <tasks>
      - Task 1: Create FastAPI Dockerfile with multi-stage build (AC: #1)
      - Task 2: Create docker-compose.yml for local stack (AC: #2, #3)
      - Task 3: Update .env.example with Docker configuration (AC: #3)
      - Task 4: Configure volume mounts and hot-reload (AC: #4)
      - Task 5: Implement health checks for all services (AC: #6)
      - Task 6: Test complete stack startup (AC: #5)
      - Task 7: Update README.md with Docker instructions (AC: #7)
      - Task 8: Write integration tests for Docker environment (AC: #5)
    </tasks>
  </story>

  <acceptanceCriteria>
    1. Dockerfile created for FastAPI application with multi-stage build
    2. docker-compose.yml includes PostgreSQL, Redis, and FastAPI services
    3. Environment variables configured via .env file (database credentials, Redis connection)
    4. Volume mounts configured for local development hot-reload
    5. All services start successfully with `docker-compose up`
    6. Health checks configured for each service
    7. Documentation updated with Docker setup instructions
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/architecture.md</path>
        <title>AI Agents - Decision Architecture</title>
        <section>Project Structure</section>
        <snippet>Defines directory structure with docker/ folder for Dockerfiles, docker-compose.yml at project root. Specifies Python 3.12, FastAPI, PostgreSQL 17, Redis 7 as core technologies.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>AI Agents - Decision Architecture</title>
        <section>Decision Summary</section>
        <snippet>Container Base: python:3.12-slim (Official image, smaller size, Debian-based). Orchestration: Docker + Docker Compose for local dev. Database: PostgreSQL 17. Message Broker: Redis 7.x.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-1.md</path>
        <title>Epic Technical Specification: Foundation & Infrastructure Setup</title>
        <section>Services and Modules</section>
        <snippet>FastAPI Application (src/main.py, src/api/), PostgreSQL Database (Docker container, src/database/), Redis Queue (Docker container, src/cache/), Docker Compose (docker-compose.yml - Local development orchestration).</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-1.md</path>
        <title>Epic Technical Specification: Foundation & Infrastructure Setup</title>
        <section>Workflows and Sequencing</section>
        <snippet>Local Development Workflow: Developer runs `docker-compose up` to start all services. Services start in order: PostgreSQL → Redis → API → Workers. Developer accesses API at http://localhost:8000. Code changes trigger hot-reload in API container.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-1.md</path>
        <title>Epic Technical Specification: Foundation & Infrastructure Setup</title>
        <section>Non-Functional Requirements - Security</section>
        <snippet>Docker Security: Non-root user in containers (USER directive), Multi-stage builds to minimize image size and attack surface, Official base images only (python:3.12-slim), Regular security scanning in CI pipeline.</snippet>
      </doc>
      <doc>
        <path>docs/epics.md</path>
        <title>AI Agents - Epic Breakdown</title>
        <section>Story 1.2</section>
        <snippet>As a developer, I want Docker containers for all infrastructure components, so that I can run the entire stack locally without manual service installation. Prerequisites: Story 1.1 (project structure exists).</snippet>
      </doc>
    </docs>
    <code>
      <file>
        <path>src/config.py</path>
        <kind>configuration</kind>
        <symbol>Settings</symbol>
        <lines>15-86</lines>
        <reason>Pydantic Settings class already configured with database_url, redis_url, celery_broker_url fields. Use this in Docker containers with AI_AGENTS_ environment prefix. Container service names (postgres, redis) will replace localhost in connection strings.</reason>
      </file>
      <file>
        <path>src/main.py</path>
        <kind>application</kind>
        <symbol>app</symbol>
        <lines>13-19</lines>
        <reason>FastAPI application initialization. This will be the main entry point for the Docker container. Contains /health endpoint that needs to be enhanced to check PostgreSQL and Redis connections for container health checks.</reason>
      </file>
      <file>
        <path>src/api/health.py</path>
        <kind>endpoint</kind>
        <symbol>health_check, readiness_check</symbol>
        <lines>13-34</lines>
        <reason>Basic health check endpoints. The readiness_check function (line 25) explicitly mentions it "Will be expanded in future stories to check database and Redis connectivity" - this story should implement those checks for Docker health check validation.</reason>
      </file>
      <file>
        <path>.env.example</path>
        <kind>configuration</kind>
        <symbol>N/A</symbol>
        <lines>1-51</lines>
        <reason>Environment configuration template. Needs Docker-specific updates: Change localhost to postgres/redis service names, add POSTGRES_* variables for container initialization, add comments about Docker networking.</reason>
      </file>
      <file>
        <path>pyproject.toml</path>
        <kind>dependencies</kind>
        <symbol>N/A</symbol>
        <lines>10-22</lines>
        <reason>Python dependencies to install in Dockerfile. All versions specified here (fastapi>=0.104.0, sqlalchemy>=2.0.23, etc.) must be installed in Docker builder stage using pip install -e .</reason>
      </file>
      <file>
        <path>tests/conftest.py</path>
        <kind>test_fixture</kind>
        <symbol>N/A</symbol>
        <lines>all</lines>
        <reason>Pytest fixture definitions. This story will create tests/integration/test_docker_stack.py that may need fixtures for Docker environment testing (database connections, Redis connections).</reason>
      </file>
    </code>
    <dependencies>
      <python>
        <core>
          fastapi>=0.104.0
          uvicorn[standard]>=0.24.0
          pydantic>=2.5.0
          pydantic-settings>=2.1.0
          sqlalchemy[asyncio]>=2.0.23
          alembic>=1.12.1
          asyncpg>=0.29.0
          redis>=5.0.1
          celery[redis]>=5.3.4
          httpx>=0.25.2
          loguru>=0.7.2
        </core>
        <dev>
          pytest>=7.4.3
          pytest-asyncio>=0.21.1
          black>=23.11.0
          ruff>=0.1.6
          mypy>=1.7.1
        </dev>
      </python>
      <docker>
        <images>
          python:3.12-slim (base image for FastAPI)
          postgres:17-alpine (database service)
          redis:7-alpine (cache and message broker)
        </images>
      </docker>
    </dependencies>
  </artifacts>

  <constraints>
    - Use multi-stage Docker build to minimize final image size (builder stage for dependencies, final stage for runtime)
    - Non-root user required in Docker containers for security (per architecture.md Docker Security section)
    - Official base images only: python:3.12-slim, postgres:17-alpine, redis:7-alpine
    - AI_AGENTS_ environment variable prefix must be maintained for consistency with Story 1.1
    - Service names in docker-compose: postgres, redis, api (use these in connection strings, not localhost)
    - Volume mounts required for hot-reload: ./src:/app/src, ./tests:/app/tests, ./alembic:/app/alembic
    - Health checks required for all services with intervals: 10s, timeout: 5s, retries: 3
    - Port mappings: 8000 (FastAPI), 5432 (PostgreSQL), 6379 (Redis)
    - Data persistence via volumes: ./data/postgres and ./data/redis (git-ignored)
    - PostgreSQL AOF persistence enabled: redis-server --appendonly yes
    - All files must be under 500 lines (per CLAUDE.md)
    - Use Black, Ruff, Mypy for code quality (configured in pyproject.toml)
    - Python 3.12.12 must be used (same version from Story 1.1)
    - .dockerignore must exclude: venv/, __pycache__/, .git/, *.pyc, .env, .pytest_cache/, .mypy_cache/, .ruff_cache/
  </constraints>

  <interfaces>
    <interface>
      <name>Health Check Endpoint</name>
      <kind>REST endpoint</kind>
      <signature>GET /health -> {"status": "healthy"}</signature>
      <path>src/main.py:37-45</path>
      <note>Currently returns static response. Should be enhanced to validate PostgreSQL and Redis connections for Docker health check accuracy.</note>
    </interface>
    <interface>
      <name>Readiness Check Endpoint</name>
      <kind>REST endpoint</kind>
      <signature>GET /api/v1/ready -> {"status": "ready"}</signature>
      <path>src/api/health.py:24-34</path>
      <note>Explicitly documented to need database and Redis connectivity checks. This story should implement those checks.</note>
    </interface>
    <interface>
      <name>Settings Configuration</name>
      <kind>Pydantic Settings class</kind>
      <signature>Settings(database_url, redis_url, celery_broker_url, environment, log_level)</signature>
      <path>src/config.py:15-86</path>
      <note>Loads from environment variables with AI_AGENTS_ prefix. Docker containers will provide these via .env file and docker-compose environment section.</note>
    </interface>
    <interface>
      <name>Docker Compose Services</name>
      <kind>Container orchestration</kind>
      <signature>services: postgres, redis, api (to be created)</signature>
      <path>docker-compose.yml (new file)</path>
      <note>Service names become DNS names in Docker network. Use postgres:5432 and redis:6379 in connection strings, not localhost.</note>
    </interface>
  </interfaces>

  <tests>
    <standards>
      Testing framework: Pytest with pytest-asyncio for async support (configured in pyproject.toml:56-62). Test file pattern: test_*.py in tests/ directory. Fixtures defined in tests/conftest.py for reusable test setup. Integration tests for Docker environment should verify: PostgreSQL connection initialization, Redis client connection, FastAPI startup with dependencies, health endpoint accuracy. Tests should be runnable inside Docker container: docker-compose exec api pytest tests/integration/. Target: 80% code coverage (per CLAUDE.md and architecture.md).
    </standards>
    <locations>
      tests/integration/ - Integration tests for multi-service scenarios
      tests/integration/test_docker_stack.py - New file for Docker environment validation (this story)
      tests/conftest.py - Shared pytest fixtures
    </locations>
    <ideas>
      AC #1 (Dockerfile multi-stage): Verify builder stage installs dependencies, final stage is minimal size, non-root user configured
      AC #2 (docker-compose.yml): Test all services start, verify service-to-service networking (api can reach postgres and redis)
      AC #3 (Environment variables): Test Settings loads from .env with Docker service names, validate AI_AGENTS_ prefix works
      AC #4 (Volume mounts): Test code hot-reload by modifying src/main.py and verifying uvicorn reloads
      AC #5 (Services start): Test docker-compose up brings all services to healthy state, test endpoints are accessible
      AC #6 (Health checks): Test health check endpoints return correct status, test PostgreSQL/Redis connection validation
      AC #7 (Documentation): Manual verification of README.md instructions completeness
    </ideas>
  </tests>
</story-context>
