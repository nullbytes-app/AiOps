<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>5</epicId>
    <storyId>4</storyId>
    <title>Conduct Production Validation Testing</title>
    <status>drafted</status>
    <generatedAt>2025-11-04</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/5-4-conduct-production-validation-testing.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>QA engineer</asA>
    <iWant>to validate system behavior with real client ticket data</iWant>
    <soThat>we confirm the platform works as designed in production conditions</soThat>
    <tasks>
### AC1: Validation Test Plan Executed
- Task 1.1: Create production-validation-test-plan.md with test scenarios (10+ ticket tests, error scenarios, performance benchmarks, security tests)
- Task 1.2: Identify or create 10+ real production tickets (varied complexity: simple config, complex troubleshooting, network issues)
- Task 1.3: Execute end-to-end tests: Send tickets via webhook, monitor processing, verify enhancements posted to ServiceDesk Plus
- Task 1.4: Document test execution results: ticket IDs, processing time, enhancement content, success/failure status

### AC2: Enhancement Quality Reviewed
- Task 2.1: Create client feedback survey template (5-point scale: relevance, accuracy, usefulness, overall quality)
- Task 2.2: Coordinate with client to survey 3+ technicians who received enhanced tickets
- Task 2.3: Conduct structured interviews or email surveys to collect qualitative feedback
- Task 2.4: Document feedback with quantitative scores and representative quotes in client-feedback-survey-results.md

### AC3: Performance Metrics Measured
- Task 3.1: Configure Prometheus queries to collect performance metrics over 24-48 hour period
- Task 3.2: Measure p50/p95/p99 latency using request_duration_seconds histogram metric
- Task 3.3: Calculate enhancement success rate (%) from enhancement_success_rate_total counter
- Task 3.4: Analyze queue depth trends (queue_depth_gauge) to identify processing bottlenecks
- Task 3.5: Document baseline metrics in performance-baseline-metrics.md with comparison to NFR001 targets

### AC4: Error Scenarios Tested
- Task 4.1: Test Knowledge Base timeout: Simulate KB API slow response (>30s), verify graceful degradation with partial context
- Task 4.2: Test ServiceDesk Plus API failure: Disable ServiceDesk API temporarily, verify retry logic and dead-letter queue
- Task 4.3: Test partial context availability: Limit ticket history to 1 result, verify enhancement still generated
- Task 4.4: Test invalid webhook signature: Send webhook with incorrect HMAC, verify rejection with 401 Unauthorized
- Task 4.5: Document error handling results: response codes, retry attempts, error messages, recovery time

### AC5: Multi-Tenant Isolation Validated
- Task 5.1: Execute tenant-isolation-validation.sh script (7 RLS tests) to verify database-level isolation
- Task 5.2: Test cross-tenant access via API: Attempt to query Tenant A's data with Tenant B's credentials, verify 403 Forbidden
- Task 5.3: If multiple clients: Verify Grafana dashboards show only respective tenant data per login
- Task 5.4: Test namespace isolation (if premium tier): Verify Tenant A pods cannot access Tenant B secrets or config
- Task 5.5: Document isolation test results with pass/fail status, security posture assessment

### AC6: Monitoring Alerts Verified
- Task 6.1: Trigger high latency alert: Inject 3+ minute delay in worker processing, verify alert fires within 5 minutes
- Task 6.2: Trigger failed enhancement alert: Send malformed webhook, verify alert fires and routes to correct channel (Slack/PagerDuty)
- Task 6.3: Trigger queue backup alert: Queue 50+ jobs without processing, verify alert fires when threshold exceeded
- Task 6.4: Verify alert routing: Confirm notifications received in configured channels (Slack #alerts, PagerDuty on-call)
- Task 6.5: Test alert resolution: Resolve triggered alerts, verify status updates in Grafana Alert History dashboard

### AC7: Validation Report Documented
- Task 7.1: Create production-validation-report.md with executive summary, test results, performance data
- Task 7.2: Include client feedback analysis: average quality scores, common themes, improvement suggestions
- Task 7.3: Document issues found during validation: severity, root cause, remediation plan
- Task 7.4: Provide recommendations: performance optimizations, feature enhancements, operational improvements
- Task 7.5: Share report with stakeholders: operations team, product manager, engineering lead, client account manager
    </tasks>
  </story>

  <acceptanceCriteria>
AC1: Validation Test Plan Executed - Comprehensive test plan executed covering 10+ real production tickets successfully processed end-to-end (webhook → enhancement → ServiceDesk Plus update)

AC2: Enhancement Quality Reviewed - Client technicians provide feedback on enhancement quality (relevance, accuracy, usefulness) via structured survey or interview, feedback documented with quality scores

AC3: Performance Metrics Measured - Key performance metrics collected and analyzed: p50/p95/p99 latency, enhancement success rate (%), processing time per ticket, queue depth trends

AC4: Error Scenarios Tested - System behavior validated under error conditions: Knowledge Base timeout, ServiceDesk Plus API failure, partial context availability, invalid webhook signatures

AC5: Multi-Tenant Isolation Validated - Tenant isolation confirmed via Row-Level Security (RLS) tests, cross-tenant access attempts, and namespace isolation checks (if multiple clients onboarded)

AC6: Monitoring Alerts Verified - Alert system validated by triggering test alerts (high latency, failed enhancement, queue backup), confirming notifications sent to correct channels

AC7: Validation Report Documented - Comprehensive validation report created with test results, performance data, issues found, client feedback, and recommendations for improvements
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <!-- Epic 5 Requirements -->
      <artifact>
        <path>docs/epics.md</path>
        <title>Epic 5: Production Deployment & Validation</title>
        <section>Story 5.4: Conduct Production Validation Testing (Lines 1001-1017)</section>
        <snippet>User Story: As a QA engineer, I want to validate system behavior with real client ticket data, so that we confirm the platform works as designed in production conditions. 7 ACs: validation test plan (10+ tickets), quality review (client feedback), performance metrics (p50/p95/p99 latency), error scenarios (KB timeout, API failures), multi-tenant isolation, monitoring alerts, validation report. Prerequisites: Story 5.3 (client onboarded).</snippet>
      </artifact>

      <!-- Architecture - Multi-Tenant Isolation -->
      <artifact>
        <path>docs/architecture.md</path>
        <title>System Architecture</title>
        <section>Multi-Tenant Isolation Strategy (Lines 12, 246-248, 332-353)</section>
        <snippet>Multi-tenant AI-powered ticket enhancement platform prioritizing production readiness, multi-tenant isolation, observability. Row-Level Security (RLS) policies on all tenant tables (enhancement_history, ticket_history, tenant_configs, system_inventory). Pool-based architecture with shared infrastructure + database-level isolation via RLS.</snippet>
      </artifact>

      <!-- Architecture - Monitoring Stack -->
      <artifact>
        <path>docs/architecture.md</path>
        <title>System Architecture</title>
        <section>Monitoring & Observability (Lines 50-53, 92-93, 201, 247)</section>
        <snippet>Prometheus (industry standard, Kubernetes native) for metrics collection, Grafana for dashboards and alerting, OpenTelemetry for distributed tracing. Custom metrics in src/monitoring/metrics.py. Epic 4 components: Prometheus, Grafana, K8s HPA.</snippet>
      </artifact>

      <!-- Story 5.3 Operational Foundation -->
      <artifact>
        <path>docs/stories/5-3-onboard-first-production-client.md</path>
        <title>Story 5.3: Onboard First Production Client</title>
        <section>Operational Documentation (Status: done, Code Review: APPROVED)</section>
        <snippet>Story 5.3 created comprehensive operational documentation (3,579 lines): client-onboarding-runbook.md (1,036 lines), tenant-troubleshooting-guide.md (810 lines), client-handoff-guide.md (682 lines), tenant-onboarding-test.sh (8 tests), tenant-isolation-validation.sh (7 RLS security tests). All production infrastructure operational: K8s cluster, PostgreSQL RLS, FastAPI (2 replicas), Celery workers (3 replicas), Prometheus+Grafana+Jaeger monitoring.</snippet>
      </artifact>

      <!-- Client Onboarding Runbook -->
      <artifact>
        <path>docs/operations/client-onboarding-runbook.md</path>
        <title>Client Onboarding Runbook</title>
        <section>Overview & Architecture Context (Lines 1-50)</section>
        <snippet>Comprehensive procedures for onboarding MSP clients following AWS Well-Architected SaaS Lens. Pool-based multi-tenant architecture with shared infrastructure (single K8s cluster, PostgreSQL, Redis). Tenant isolation via PostgreSQL RLS. Tier-based: Basic (shared namespace + RLS, 2-3 hours) vs Premium (dedicated namespace + RLS + quotas, 4-6 hours). Includes validation checklist, rollback procedures, troubleshooting.</snippet>
      </artifact>

      <!-- Tenant Troubleshooting Guide -->
      <artifact>
        <path>docs/operations/tenant-troubleshooting-guide.md</path>
        <title>Tenant Troubleshooting Guide</title>
        <section>Common Issues & Diagnostics (810 lines total)</section>
        <snippet>Diagnostic procedures for 5 common production issues: webhook delivery failures, enhancement processing stuck, ServiceDesk Plus integration errors, RLS access denied, performance degradation. Step-by-step troubleshooting with SQL queries, kubectl commands, log analysis.</snippet>
      </artifact>

      <!-- Alert Runbooks (Epic 4) -->
      <artifact>
        <path>docs/operations/alert-runbooks.md</path>
        <title>Alert Response Runbooks</title>
        <section>Prometheus Alert Handling</section>
        <snippet>Runbooks for responding to Prometheus alerts: high latency (>60s p95), failed enhancements (>5% error rate), queue backup (>25 jobs), worker down, database connection errors. Each runbook includes severity, impact, diagnostic steps, resolution procedures, escalation criteria.</snippet>
      </artifact>

      <!-- Latest 2025 Best Practices (Web Search) -->
      <artifact>
        <path>external://web-research-2025</path>
        <title>2025 SaaS Multi-Tenant Testing Best Practices</title>
        <section>Production Validation & Continuous Monitoring</section>
        <snippet>Daily automated regression tests in production detect broken flows, ensure functional integrity. Multi-tenant isolation testing: verify tenant boundary enforcement, row-level security, concurrent access, noisy-neighbor simulations (90% load on one tenant, measure impact on others). Load/performance validation: support SLAs under multi-tenant load, auto-scaling. Self-healing tests adapt to continuous deployments. Release management: canary deployments (≤10% tenants before full rollout). CI/CD: validate each commit against replicated production staging.</snippet>
      </artifact>

      <!-- Latest 2025 Prometheus/Grafana Practices (Web Search) -->
      <artifact>
        <path>external://web-research-2025</path>
        <title>2025 Prometheus & Grafana Performance Monitoring</title>
        <section>Baseline Metrics & Performance Analysis</section>
        <snippet>Prometheus scrapes metrics at regular intervals, stores in time-series database for efficient querying. Grafana dynamic dashboards visualize Prometheus data. Key baseline metrics: request latencies (p50/p95/p99), throughput, error rates, resource utilization (CPU, RAM, network). Custom application metrics using Prometheus client libraries. PromQL queries for baseline analysis: histogram_quantile() for percentile calculations, rate() for success rates. 2025 trend: AI Metrics dashboards for ML workloads (April 2025).</snippet>
      </artifact>

      <!-- Latest 2025 OpenTelemetry Practices (Web Search) -->
      <artifact>
        <path>external://web-research-2025</path>
        <title>2025 OpenTelemetry Distributed Tracing Validation</title>
        <section>Production Readiness & Validation Approaches</section>
        <snippet>OpenTelemetry production-ready in 2025: stable profiling, zero-code instrumentation, 12+ language support. Validation best practices: verify trace propagation, test failure scenarios, validate performance impact (<10% degradation typical). Common validation: confirm instrumentation captures expected telemetry, test header propagation across services/protocols, check exporter connectivity, tune batch/sampling settings. Integration testing: 1-2 weeks for load testing, performance validation. Google Cloud Trace Telemetry API (Sept 2025) processes 4-6 TB traces daily.</snippet>
      </artifact>

      <!-- Tenant Isolation Best Practices (Ref MCP) -->
      <artifact>
        <path>external://nile-tenant-isolation-docs</path>
        <title>Nile Tenant Virtualization - Tenant Isolation</title>
        <section>Data Isolation & Performance Isolation</section>
        <snippet>SaaS tenant isolation ensures each tenant's data is inaccessible to others (data isolation) and one tenant's activity doesn't impact others' performance (performance isolation). Data isolation through row-level security, bucket ACLs, storage segregation. Performance isolation prevents noisy neighbor scenarios through resource quotas, connection pooling, query optimization.</snippet>
      </artifact>
    </docs>
    <code>
      <!-- Monitoring Infrastructure (Epic 4) -->
      <artifact>
        <path>src/monitoring/metrics.py</path>
        <kind>monitoring</kind>
        <symbol>enhancement_requests_total, enhancement_duration_seconds, enhancement_success_rate, queue_depth, worker_active_count</symbol>
        <lines>all</lines>
        <reason>Prometheus metrics for performance baseline collection (AC3). Used to measure p50/p95/p99 latency, success rate, queue depth trends via PromQL queries.</reason>
      </artifact>

      <!-- Webhook Endpoint (Entry Point) -->
      <artifact>
        <path>src/api/webhooks.py</path>
        <kind>api-endpoint</kind>
        <symbol>receive_webhook, store_resolved_ticket</symbol>
        <lines>all</lines>
        <reason>Webhook receiver endpoint for testing end-to-end ticket processing (AC1). Validates webhook signature (AC4 - invalid signature test). Entry point for 10+ ticket validation tests.</reason>
      </artifact>

      <!-- ServiceDesk Plus Integration -->
      <artifact>
        <path>src/services/servicedesk_client.py</path>
        <kind>integration</kind>
        <symbol>update_ticket_with_enhancement, should_retry, _handle_http_error</symbol>
        <lines>all</lines>
        <reason>ServiceDesk Plus API client with retry logic (MAX_RETRIES, RETRY_DELAYS). Critical for AC4 error scenario testing (API failure, retry behavior). Contains error handling patterns for validation.</reason>
      </artifact>

      <!-- Enhancement Workflow (Core Logic) -->
      <artifact>
        <path>src/workflows/enhancement_workflow.py</path>
        <kind>workflow</kind>
        <symbol>LangGraph enhancement workflow</symbol>
        <lines>all</lines>
        <reason>LangGraph workflow orchestrating context gathering + AI synthesis. Core component tested in end-to-end validation (AC1). Error scenarios (KB timeout, partial context) tested here (AC4).</reason>
      </artifact>

      <!-- Tenant Service (Multi-Tenant Isolation) -->
      <artifact>
        <path>src/services/tenant_service.py</path>
        <kind>service</kind>
        <symbol>tenant context management, RLS enforcement</symbol>
        <lines>all</lines>
        <reason>Tenant isolation enforcement via Row-Level Security. Critical for AC5 multi-tenant isolation validation. set_tenant_context function validates tenant and sets session variable.</reason>
      </artifact>

      <!-- Existing Test Scripts (Story 5.3 + 5.2) -->
      <artifact>
        <path>scripts/production-smoke-test.sh</path>
        <kind>test-script</kind>
        <symbol>7 smoke tests (health, readiness, webhook validation, E2E)</symbol>
        <lines>1-100+</lines>
        <reason>Production infrastructure smoke tests from Story 5.2. Foundation for Story 5.4 validation test suite. Tests health endpoints, webhook signature, database/Redis connectivity.</reason>
      </artifact>

      <artifact>
        <path>scripts/tenant-onboarding-test.sh</path>
        <kind>test-script</kind>
        <symbol>8 tenant onboarding tests (config, credentials, webhook, E2E)</symbol>
        <lines>1-100+</lines>
        <reason>Automated tenant validation from Story 5.3. Tests tenant configuration, encrypted credentials, webhook processing, end-to-end enhancement workflow. Reusable for Story 5.4 validation.</reason>
      </artifact>

      <artifact>
        <path>scripts/tenant-isolation-validation.sh</path>
        <kind>test-script</kind>
        <symbol>7 RLS security tests (cross-tenant access prevention)</symbol>
        <lines>all</lines>
        <reason>Row-Level Security validation script from Story 5.3. CRITICAL for AC5 multi-tenant isolation testing. Tests database-level isolation, cross-tenant queries, session context enforcement.</reason>
      </artifact>

      <!-- Prometheus Alert Rules (AC6 Testing) -->
      <artifact>
        <path>k8s/prometheus-alert-rules.yaml</path>
        <kind>config</kind>
        <symbol>EnhancementSuccessRateLow, QueueDepthHigh, WorkerDown, HighLatency alerts</symbol>
        <lines>1-72+</lines>
        <reason>Prometheus alert definitions for AC6 monitoring alerts verification. Contains alert thresholds: success rate <95%, queue depth >100, worker count 0, p95 latency >120s. Includes runbook URLs for operational context.</reason>
      </artifact>

      <!-- Prometheus Configuration -->
      <artifact>
        <path>k8s/prometheus-config.yaml</path>
        <kind>config</kind>
        <symbol>Prometheus scrape configuration</symbol>
        <lines>all</lines>
        <reason>Prometheus scrape targets configuration. Defines metric collection intervals, service discovery, relabeling rules. Required for understanding metric collection behavior during performance testing (AC3).</reason>
      </artifact>
    </code>
    <dependencies>
      <!-- Python Runtime -->
      <python version="3.12+" />

      <!-- Core Web Framework -->
      <package ecosystem="python" name="fastapi" version=">=0.104.0" purpose="REST API framework for webhook receiver, health endpoints" />
      <package ecosystem="python" name="uvicorn" version=">=0.24.0" purpose="ASGI server for FastAPI application" />
      <package ecosystem="python" name="pydantic" version=">=2.5.0" purpose="Data validation and settings management" />

      <!-- Database & ORM -->
      <package ecosystem="python" name="sqlalchemy" version=">=2.0.23" purpose="Async ORM for PostgreSQL (tenant_configs, enhancement_history, ticket_history)" />
      <package ecosystem="python" name="asyncpg" version=">=0.29.0" purpose="Async PostgreSQL driver" />
      <package ecosystem="python" name="alembic" version=">=1.12.1" purpose="Database migrations (RLS policies, tenant tables)" />

      <!-- Message Queue & Workers -->
      <package ecosystem="python" name="redis" version=">=5.0.1" purpose="Redis client for message broker" />
      <package ecosystem="python" name="celery" version=">=5.3.4" purpose="Async task processing (enhancement workflow)" />

      <!-- AI & Workflow -->
      <package ecosystem="python" name="langgraph" version=">=0.0.1" purpose="LangGraph workflow orchestration for context gathering + synthesis" />
      <package ecosystem="python" name="openai" version=">=1.3.0" purpose="OpenAI GPT-4o-mini for enhancement generation" />

      <!-- Monitoring & Observability -->
      <package ecosystem="python" name="prometheus-client" version=">=0.19.0" purpose="Custom metrics instrumentation (enhancement_duration_seconds, success_rate, queue_depth)" />
      <package ecosystem="python" name="opentelemetry-api" version=">=1.20.0" purpose="Distributed tracing API (OpenTelemetry standard)" />
      <package ecosystem="python" name="opentelemetry-sdk" version=">=1.20.0" purpose="OpenTelemetry SDK for trace collection" />
      <package ecosystem="python" name="opentelemetry-instrumentation-fastapi" version=">=0.41b0" purpose="Auto-instrumentation for FastAPI requests" />
      <package ecosystem="python" name="opentelemetry-instrumentation-celery" version=">=0.41b0" purpose="Auto-instrumentation for Celery tasks" />
      <package ecosystem="python" name="opentelemetry-exporter-otlp-proto-grpc" version=">=1.20.0" purpose="Export traces to Jaeger via OTLP" />

      <!-- HTTP Client -->
      <package ecosystem="python" name="httpx" version=">=0.25.2" purpose="Async HTTP client for ServiceDesk Plus API integration" />

      <!-- Logging -->
      <package ecosystem="python" name="loguru" version=">=0.7.2" purpose="Structured logging with tenant_id context" />

      <!-- Testing (Dev Dependencies) -->
      <package ecosystem="python" name="pytest" version=">=7.4.3" purpose="Test framework (unit, integration, E2E tests)" />
      <package ecosystem="python" name="pytest-asyncio" version=">=0.21.1" purpose="Async test support" />

      <!-- Code Quality (Dev Dependencies) -->
      <package ecosystem="python" name="black" version=">=23.11.0" purpose="Code formatting (100 char line length)" />
      <package ecosystem="python" name="ruff" version=">=0.1.6" purpose="Fast linting (replaces Flake8)" />
      <package ecosystem="python" name="mypy" version=">=1.7.1" purpose="Static type checking" />

      <!-- Infrastructure Components (Docker/Kubernetes) -->
      <infrastructure>
        <component name="PostgreSQL" version="17-alpine" purpose="Multi-tenant database with Row-Level Security (RLS)" />
        <component name="Redis" version="7-alpine" purpose="Message broker for Celery task queue" />
        <component name="Prometheus" version="latest" purpose="Metrics collection and time-series database (AC3 performance metrics)" />
        <component name="Grafana" version="latest" purpose="Dashboard visualization for metrics (AC3)" />
        <component name="Alertmanager" version="latest" purpose="Alert routing to Slack/PagerDuty (AC6)" />
        <component name="Jaeger" version="latest" purpose="Distributed tracing backend via OpenTelemetry (AC1 E2E validation)" />
      </infrastructure>

      <!-- External Integrations (Testing Dependencies) -->
      <external-service>
        <service name="ServiceDesk Plus" api="REST API v3" purpose="Ticket management system - webhook source and enhancement destination (AC1, AC4 API failure testing)" />
        <service name="OpenAI GPT-4o-mini" api="OpenAI API" purpose="AI model for enhancement generation (AC4 error scenarios if rate limited)" />
        <service name="Slack" api="Webhook" purpose="Alert notifications (AC6 alert verification)" />
        <service name="PagerDuty" api="Events API v2" purpose="Critical alert escalation (AC6 alert verification)" />
      </external-service>

      <!-- Kubernetes Components (Production Environment) -->
      <kubernetes>
        <deployment name="api" replicas="2" purpose="FastAPI webhook receiver (AC1 testing target)" />
        <deployment name="worker" replicas="3" purpose="Celery workers for enhancement processing (AC1, AC4 error scenarios)" />
        <deployment name="postgres" purpose="PostgreSQL database (AC5 RLS validation)" />
        <deployment name="redis" purpose="Redis message queue (AC3 queue_depth metric)" />
        <deployment name="prometheus" purpose="Metrics scraping and storage (AC3)" />
        <deployment name="grafana" purpose="Dashboard UI (AC3 visualization)" />
        <deployment name="alertmanager" purpose="Alert routing (AC6)" />
        <deployment name="jaeger" purpose="Trace visualization (AC1 E2E validation)" />
      </kubernetes>

      <!-- Shell Dependencies (Test Scripts) -->
      <shell-tools>
        <tool name="bash" version="4.0+" purpose="Test script execution (production-validation-test.sh, tenant-onboarding-test.sh)" />
        <tool name="curl" version="latest" purpose="HTTP requests for webhook testing, health checks, Prometheus queries" />
        <tool name="jq" version="latest" purpose="JSON parsing in test scripts for response validation" />
        <tool name="kubectl" version="latest" purpose="Kubernetes cluster interaction for pod inspection, log retrieval" />
        <tool name="psql" version="17+" purpose="PostgreSQL client for RLS validation queries (AC5)" />
        <tool name="redis-cli" version="7+" purpose="Redis inspection for queue depth validation" />
      </shell-tools>
    </dependencies>
  </artifacts>

  <constraints>
    <!-- Development Constraints -->
    <constraint type="implementation">
      <rule>NO CODE CHANGES ALLOWED - Story 5.4 is purely testing and validation. All testing infrastructure exists from previous stories (Epic 4, Stories 5.2-5.3). Create NEW test scripts and documentation only.</rule>
    </constraint>

    <constraint type="testing-approach">
      <rule>REAL PRODUCTION TICKETS REQUIRED - Use actual client tickets from ServiceDesk Plus (not synthetic test data). If no real client available, use realistic test tickets matching actual ServiceDesk Plus ticket formats.</rule>
    </constraint>

    <constraint type="multi-tenant-security">
      <rule>RLS ISOLATION TESTING MANDATORY (AC5) - Execute tenant-isolation-validation.sh (7 tests) to verify Row-Level Security prevents cross-tenant data access. Test with multiple tenant contexts if available.</rule>
    </constraint>

    <constraint type="performance-baseline">
      <rule>24-48 HOUR METRIC COLLECTION (AC3) - Collect performance metrics over extended period for statistical significance. Use Prometheus histogram_quantile() for p50/p95/p99 calculations, rate() for success rates.</rule>
    </constraint>

    <constraint type="error-scenarios">
      <rule>FAULT INJECTION REQUIRED (AC4) - Test graceful degradation under failures: KB timeout (>30s), ServiceDesk Plus API down, partial context, invalid webhook signatures. Document system behavior, retry attempts, recovery time.</rule>
    </constraint>

    <constraint type="alert-verification">
      <rule>INTENTIONAL ALERT TRIGGERING (AC6) - Actively trigger production alerts (high latency, failed enhancement, queue backup) to verify notification routing (Slack, PagerDuty). DO NOT rely on passive observation.</rule>
    </constraint>

    <constraint type="client-feedback">
      <rule>STRUCTURED FEEDBACK COLLECTION (AC2) - Survey 3+ client technicians using 5-point scale (relevance, accuracy, usefulness, overall quality) + qualitative feedback. Document quantitative scores and representative quotes.</rule>
    </constraint>

    <constraint type="deliverables">
      <rule>DOCUMENTATION OUTPUTS - Create: production-validation-test-plan.md (test scenarios), performance-baseline-metrics.md (p50/p95/p99 data), client-feedback-survey-results.md (technician feedback), production-validation-report.md (comprehensive findings). NEW script: production-validation-test.sh (automated validation suite).</rule>
    </constraint>

    <constraint type="architectural-adherence">
      <rule>LEVERAGE EXISTING INFRASTRUCTURE - Use Prometheus+Grafana (Epic 4), OpenTelemetry+Jaeger (Story 4.6), operational runbooks (Story 5.3), existing test scripts (Stories 5.2-5.3). No new monitoring infrastructure required.</rule>
    </constraint>

    <constraint type="2025-best-practices">
      <rule>FOLLOW 2025 SaaS TESTING STANDARDS - Implement continuous production testing, noisy-neighbor simulations (90% load on one tenant, measure impact), self-healing test patterns, canary deployment validation (if applicable), trace-based testing with OpenTelemetry.</rule>
    </constraint>
  </constraints>

  <interfaces>
    <!-- REST API Interfaces (Testing Targets) -->
    <interface>
      <name>POST /api/v1/webhook</name>
      <kind>REST endpoint</kind>
      <signature>POST /api/v1/webhook - Receives ServiceDesk Plus webhooks with HMAC-SHA256 signature validation. Headers: X-ServiceDesk-Signature. Body: {tenant_id, ticket_id, event_type, payload}. Returns: 202 Accepted (queued), 401 Unauthorized (invalid signature), 400 Bad Request (validation error).</signature>
      <path>src/api/webhooks.py (receive_webhook function)</path>
    </interface>

    <interface>
      <name>POST /api/v1/tickets/resolved</name>
      <kind>REST endpoint</kind>
      <signature>POST /api/v1/tickets/resolved - Stores resolved tickets for history context. Body: {tenant_id, ticket_id, resolution_data}. Returns: 201 Created, 400 Bad Request.</signature>
      <path>src/api/webhooks.py (store_resolved_ticket function)</path>
    </interface>

    <interface>
      <name>GET /health</name>
      <kind>REST endpoint</kind>
      <signature>GET /health - Health check endpoint. Returns: {status: "healthy", dependencies: {database: "healthy", redis: "healthy"}}. HTTP 200 OK if all healthy, 503 Service Unavailable if degraded.</signature>
      <path>Tested by scripts/production-smoke-test.sh</path>
    </interface>

    <interface>
      <name>GET /api/v1/ready</name>
      <kind>REST endpoint</kind>
      <signature>GET /api/v1/ready - Readiness check for Kubernetes liveness probe. Returns: {ready: true/false}. HTTP 200 OK if ready, 503 if not ready.</signature>
      <path>Tested by scripts/production-smoke-test.sh</path>
    </interface>

    <!-- Prometheus Metrics Interface -->
    <interface>
      <name>GET /metrics</name>
      <kind>Prometheus metrics endpoint</kind>
      <signature>GET /metrics - Exposes Prometheus metrics in text format. Metrics: enhancement_requests_total (counter), enhancement_duration_seconds (histogram with p50/p95/p99 buckets), enhancement_success_rate (gauge, 0-100%), queue_depth (gauge), worker_active_count (gauge). All metrics labeled with tenant_id for per-tenant filtering.</signature>
      <path>src/monitoring/metrics.py</path>
    </interface>

    <!-- ServiceDesk Plus API (External Integration) -->
    <interface>
      <name>ServiceDesk Plus REST API - Update Ticket</name>
      <kind>REST API client</kind>
      <signature>PUT /api/v3/requests/{ticket_id}/notes - Adds enhancement note to ServiceDesk Plus ticket. Auth: OAuth2. Headers: {Authorization: Bearer {token}, Content-Type: application/json}. Body: {note: {text: HTML-formatted enhancement}}. Retry logic: 3 attempts with exponential backoff (1s, 2s, 4s). Retry on: 429 (rate limit), 500/502/503/504 (server errors). No retry on: 400/401/403/404 (client errors).</signature>
      <path>src/services/servicedesk_client.py (update_ticket_with_enhancement function)</path>
    </interface>

    <!-- Grafana Dashboards (Visualization Interface) -->
    <interface>
      <name>Grafana Dashboards</name>
      <kind>Visualization interface</kind>
      <signature>4+ Grafana dashboards consuming Prometheus data: 1) System Status Dashboard (health, queue depth, success rate), 2) Per-Tenant Metrics Dashboard (latency, throughput, errors by tenant_id), 3) Queue Health Dashboard (Redis queue depth, worker utilization, processing time), 4) Alert History Dashboard (triggered alerts, resolution time, trends). Access: https://grafana.ai-agents.production (tenant-aware filtering).</signature>
      <path>Configured in Epic 4 (Story 4.3)</path>
    </interface>

    <!-- Jaeger Distributed Tracing (Observability Interface) -->
    <interface>
      <name>Jaeger Tracing UI</name>
      <kind>Distributed tracing interface</kind>
      <signature>OpenTelemetry traces exported to Jaeger for end-to-end request visibility. Trace spans: webhook receipt → Redis enqueue → Celery worker pickup → context gathering (ticket history, KB search, monitoring data) → LLM synthesis → ServiceDesk Plus update. Trace IDs propagated via HTTP headers. Access: https://jaeger.ai-agents.production. Query by tenant_id, ticket_id, or trace_id.</signature>
      <path>Configured in Epic 4 (Story 4.6)</path>
    </interface>

    <!-- Alertmanager (Alert Routing Interface) -->
    <interface>
      <name>Alertmanager Notification Routing</name>
      <kind>Alert routing interface</kind>
      <signature>Prometheus alerts routed via Alertmanager to: 1) Slack (#alerts channel), 2) PagerDuty (on-call escalation for critical alerts), 3) Email (backup notification). Alert types: EnhancementSuccessRateLow (warning), QueueDepthHigh (warning), WorkerDown (critical), HighLatency (warning). Routing rules configured with severity-based escalation (warning → Slack, critical → Slack + PagerDuty).</signature>
      <path>Configured in Epic 4 (Story 4.5)</path>
    </interface>

    <!-- PostgreSQL Row-Level Security (Security Interface) -->
    <interface>
      <name>PostgreSQL RLS Policies</name>
      <kind>Database security interface</kind>
      <signature>Row-Level Security (RLS) policies enforce tenant isolation on tables: enhancement_history, ticket_history, tenant_configs, system_inventory. Helper function: set_tenant_context(p_tenant_id UUID) validates tenant and sets session variable app.current_tenant_id. All queries filtered by current_tenant_id automatically. RLS enabled on all tenant tables with policies: SELECT/INSERT/UPDATE/DELETE WHERE tenant_id = current_setting('app.current_tenant_id')::uuid.</signature>
      <path>Database migrations + src/services/tenant_service.py</path>
    </interface>
  </interfaces>
  <tests>
    <standards>
**Production Validation Testing Standards (Story 5.4)**

Story 5.4 is a comprehensive validation testing story (NOT traditional unit/integration testing). Testing approach follows 2025 SaaS multi-tenant production validation best practices:

**Testing Framework Stack:**
- **Manual Testing:** Structured test plans with documented scenarios, expected results, and actual observations
- **Automated Scripts:** Bash scripts using curl, jq, kubectl, psql for repeatable validation checks
- **Existing Test Infrastructure:** Leverage scripts from Stories 5.2-5.3 (production-smoke-test.sh, tenant-onboarding-test.sh, tenant-isolation-validation.sh)
- **Prometheus PromQL:** Performance metrics collection via histogram_quantile(), rate() queries
- **Grafana Dashboards:** Visual validation of metrics trends, alert history
- **Jaeger Tracing:** Distributed trace inspection for end-to-end request visibility

**Testing Patterns:**
1. **End-to-End Validation (AC1):** Real production tickets processed webhook → queue → worker → enhancement → ServiceDesk Plus update
2. **Performance Baseline Collection (AC3):** 24-48 hour metric collection with Prometheus, statistical analysis (p50/p95/p99)
3. **Fault Injection Testing (AC4):** Intentionally trigger error scenarios (KB timeout, API failures, invalid signatures), observe graceful degradation
4. **Security Isolation Testing (AC5):** Multi-tenant RLS validation, cross-tenant access prevention, database-level isolation
5. **Alert Verification (AC6):** Active alert triggering (not passive observation), notification routing confirmation
6. **Client Feedback Collection (AC2):** Structured surveys with quantitative scores (5-point scale) + qualitative feedback

**Testing Standards from Previous Stories:**
- **Pytest (Epics 2-3):** Unit tests (test_webhook_validator.py, test_tenant_service.py) and integration tests (test_e2e_enhancement_pipeline.py)
- **Shift-Left Testing:** Unit/integration tests in development, production validation as final gate before MVP release
- **Code Quality:** Black (formatting), Ruff (linting), Mypy (type checking), Bandit (security scanning)

**Test Data Requirements:**
- **REAL PRODUCTION TICKETS MANDATORY:** Use actual client tickets from ServiceDesk Plus (varied complexity: simple config, complex troubleshooting, network issues)
- **Synthetic Data Prohibited:** No artificially generated test data - use realistic ServiceDesk Plus ticket formats
- **Multi-Tenant Test Contexts:** Test with multiple tenant IDs if available (Tenant A, Tenant B) for cross-tenant isolation validation

**Documentation Standards:**
- **Test Plans:** production-validation-test-plan.md with scenarios, expected results, validation criteria
- **Test Results:** Document ticket IDs, processing times, enhancement content, success/failure status
- **Performance Baselines:** performance-baseline-metrics.md with p50/p95/p99 data, comparison to NFR001 targets (p95 <60s)
- **Client Feedback:** client-feedback-survey-results.md with quantitative scores (average across 3+ technicians) + representative quotes
- **Validation Report:** production-validation-report.md with executive summary, findings, issues, recommendations

**2025 Best Practices Applied:**
- **Continuous Production Testing:** Automated regression tests in production (daily/weekly cadence after initial validation)
- **Noisy Neighbor Simulation:** Load one tenant to 90%, measure latency impact on other tenants
- **Self-Healing Tests:** Adapt to continuous deployments without manual test updates
- **Trace-Based Testing:** Use OpenTelemetry traces to validate request propagation, identify bottlenecks
- **Canary Validation:** If applicable, validate canary deployments before full rollout
    </standards>

    <locations>
**Test Script Locations:**
- `scripts/production-smoke-test.sh` - 7 infrastructure health tests (EXISTING from Story 5.2)
- `scripts/tenant-onboarding-test.sh` - 8 tenant validation tests (EXISTING from Story 5.3)
- `scripts/tenant-isolation-validation.sh` - 7 RLS security tests (EXISTING from Story 5.3)
- `scripts/production-validation-test.sh` - NEW comprehensive validation suite for Story 5.4

**Test Documentation Locations:**
- `docs/testing/production-validation-test-plan.md` - NEW test plan with scenarios
- `docs/testing/performance-baseline-metrics.md` - NEW baseline performance data
- `docs/testing/client-feedback-survey-results.md` - NEW technician feedback
- `docs/operations/production-validation-report.md` - NEW comprehensive validation report

**Unit Test Locations (Existing from Epics 2-3):**
- `tests/unit/` - Component-level tests (webhook validator, tenant service, enhancement workflow)
- `tests/integration/` - End-to-end pipeline tests (test_e2e_enhancement_pipeline.py with 12 tests)
- `tests/` - Pytest configuration (pyproject.toml: testpaths=["tests"], asyncio_mode="auto")

**Operational Documentation (Reference for Validation):**
- `docs/operations/client-onboarding-runbook.md` - Onboarding procedures, validation checklist
- `docs/operations/tenant-troubleshooting-guide.md` - Diagnostic procedures for common issues
- `docs/operations/alert-runbooks.md` - Alert response procedures (reference for AC6)
- `docs/runbooks/` - Component-specific runbooks (webhook troubleshooting, worker failures, etc.)
    </locations>

    <ideas>
**Test Ideas Mapped to Acceptance Criteria:**

**AC1: Validation Test Plan Executed (10+ Real Tickets)**
- Test Idea 1.1: Execute production-validation-test-plan.md with 10+ real ServiceDesk Plus tickets (varied complexity)
- Test Idea 1.2: Monitor end-to-end processing via Jaeger traces (webhook → queue → worker → ServiceDesk Plus)
- Test Idea 1.3: Verify all tickets receive enhancements posted to ServiceDesk Plus notes (check via ServiceDesk Plus UI)
- Test Idea 1.4: Document results: ticket IDs, processing times (start to completion), enhancement quality, success/failure status
- Test Idea 1.5: Validate ticket types: network issues, database errors, user access requests, configuration questions (diverse scenarios)

**AC2: Enhancement Quality Reviewed (Client Feedback)**
- Test Idea 2.1: Create client feedback survey template with 5-point scale (relevance, accuracy, usefulness, overall quality)
- Test Idea 2.2: Coordinate with client account manager to identify 3+ technicians who received enhanced tickets
- Test Idea 2.3: Conduct structured email surveys or phone interviews (15-20 min each)
- Test Idea 2.4: Analyze feedback: calculate average scores per category, identify common themes, extract improvement suggestions
- Test Idea 2.5: Document in client-feedback-survey-results.md with quantitative summary + representative quotes

**AC3: Performance Metrics Measured (Baseline Establishment)**
- Test Idea 3.1: Configure 24-48 hour Prometheus metric collection period (sufficient for statistical significance)
- Test Idea 3.2: Measure p50/p95/p99 latency using PromQL: `histogram_quantile(0.95, rate(enhancement_duration_seconds_bucket[5m]))`
- Test Idea 3.3: Calculate success rate (%): `rate(enhancement_success_rate_total[1h]) * 100`
- Test Idea 3.4: Analyze queue depth trends: `queue_depth` gauge metric over time, identify peak periods
- Test Idea 3.5: Compare against NFR001 targets (p95 <60s, success rate >95%), document gaps in performance-baseline-metrics.md
- Test Idea 3.6: Visualize metrics in Grafana dashboards (System Status, Per-Tenant Metrics, Queue Health)

**AC4: Error Scenarios Tested (Fault Injection)**
- Test Idea 4.1: KB Timeout Test - Simulate Knowledge Base API slow response (>30s), verify system continues with partial context, logs warning, completes <75s
- Test Idea 4.2: ServiceDesk Plus API Failure Test - Disable ServiceDesk API temporarily, verify retry logic (3 attempts, exponential backoff), dead-letter queue, alert fires
- Test Idea 4.3: Partial Context Test - Limit ticket history to 1 result, no KB matches, verify enhancement still generated with available data
- Test Idea 4.4: Invalid Webhook Signature Test - Send webhook with incorrect HMAC-SHA256 signature, verify 401 Unauthorized response, no job queued
- Test Idea 4.5: Document error handling: HTTP response codes, retry attempts (1s, 2s, 4s delays), error messages, recovery time

**AC5: Multi-Tenant Isolation Validated (Security)**
- Test Idea 5.1: Execute tenant-isolation-validation.sh (7 automated RLS tests), verify all pass
- Test Idea 5.2: Cross-Tenant API Access Test - Query Tenant A's enhancement_history with Tenant B's credentials, verify 403 Forbidden (no data returned)
- Test Idea 5.3: Database-Level RLS Test - Run SQL queries with different app.current_tenant_id session variables, verify only respective tenant data visible
- Test Idea 5.4: Grafana Multi-Tenant Filtering Test (if applicable) - Login as different tenant users, verify dashboards show only respective tenant metrics
- Test Idea 5.5: Namespace Isolation Test (if Premium tier) - Attempt pod-to-pod communication across tenant namespaces, verify network policies block access
- Test Idea 5.6: Document security posture: RLS policy enforcement, cross-tenant access prevention, session context validation

**AC6: Monitoring Alerts Verified (Operational Readiness)**
- Test Idea 6.1: High Latency Alert - Inject 3+ minute delay in Celery worker processing (e.g., sleep 180s in workflow), verify alert fires within 5 minutes
- Test Idea 6.2: Failed Enhancement Alert - Send malformed webhook causing enhancement failure (e.g., invalid tenant_id), verify alert fires after 3 consecutive failures
- Test Idea 6.3: Queue Backup Alert - Queue 50+ jobs without worker processing (stop workers temporarily), verify alert fires when queue_depth >25
- Test Idea 6.4: Notification Routing Test - Confirm alerts received in Slack #alerts channel AND PagerDuty (for critical alerts)
- Test Idea 6.5: Alert Resolution Test - Resolve triggered alerts, verify status updates in Grafana Alert History dashboard
- Test Idea 6.6: Runbook Validation - Follow alert runbook procedures (docs/operations/alert-runbooks.md), verify diagnostic steps work as documented

**AC7: Validation Report Documented (Comprehensive Findings)**
- Test Idea 7.1: Create production-validation-report.md with executive summary (1-2 pages), test results overview
- Test Idea 7.2: Include performance data section: p50/p95/p99 latency graphs from Grafana, success rate trends, queue depth analysis
- Test Idea 7.3: Document client feedback analysis: average quality scores (e.g., relevance=4.2/5, accuracy=4.5/5), common themes (e.g., "enhanced troubleshooting steps helpful")
- Test Idea 7.4: Issues found section: severity classification (critical/high/medium/low), root cause analysis, remediation plan with timeline
- Test Idea 7.5: Recommendations section: performance optimizations (e.g., optimize LLM prompts for speed), feature enhancements (e.g., add ticket prioritization), operational improvements (e.g., tune alert thresholds)
- Test Idea 7.6: Share report with stakeholders: operations team, product manager, engineering lead, client account manager (as appendix to client review)

**Automated Test Script Ideas (NEW: production-validation-test.sh):**
- Automated Test 1: Verify 10 tickets processed successfully (call existing tenant-onboarding-test.sh logic)
- Automated Test 2: Measure p95 latency via Prometheus query, assert <60s (NFR001 compliance)
- Automated Test 3: Calculate success rate over 24h period, assert >95%
- Automated Test 4: Inject KB timeout, verify graceful degradation (enhancement still completes)
- Automated Test 5: Inject ServiceDesk Plus failure, verify retry logic executes (check logs for 3 attempts)
- Automated Test 6: Test invalid webhook signature rejection (assert 401 response)
- Automated Test 7: Run RLS isolation tests (call tenant-isolation-validation.sh)
- Automated Test 8: Trigger high latency alert, verify notification (check Slack API or Alertmanager API)
- Automated Test 9: Trigger queue backup alert, verify notification
- Automated Test 10: Verify Jaeger traces exist for all processed tickets (query Jaeger API by ticket_id tag)

**2025 Best Practice Test Ideas:**
- Noisy Neighbor Test: Load Tenant A to 90% capacity (queue 100+ jobs), measure Tenant B latency impact (should be minimal due to RLS + resource quotas)
- Trace-Based Validation: Use OpenTelemetry trace IDs to validate complete request propagation (webhook → queue → worker → ServiceDesk Plus spans present)
- Self-Healing Test: Deploy new application version (if applicable), re-run automated tests, verify no manual test updates required
- Continuous Production Regression: Schedule production-validation-test.sh to run daily (after initial validation), detect broken flows early
    </ideas>
  </tests>
</story-context>
