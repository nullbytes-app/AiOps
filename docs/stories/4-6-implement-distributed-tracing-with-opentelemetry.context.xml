<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>4</epicId>
    <storyId>4.6</storyId>
    <title>Implement Distributed Tracing with OpenTelemetry</title>
    <status>drafted</status>
    <generatedAt>2025-11-03</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/4-6-implement-distributed-tracing-with-opentelemetry.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>a developer</asA>
    <iWant>request traces showing end-to-end flow through FastAPI, Redis, and Celery components</iWant>
    <soThat>I can debug performance bottlenecks and failed enhancements with complete visibility into the ticket enhancement pipeline</soThat>
    <tasks>
      <task id="1">Install OpenTelemetry Dependencies (5 subtasks)</task>
      <task id="2">Deploy Jaeger Backend (9 subtasks - Docker Compose + Kubernetes)</task>
      <task id="3">Create OpenTelemetry Tracing Infrastructure (5 subtasks - init module, span processors)</task>
      <task id="4">Instrument FastAPI Application (5 subtasks - automatic instrumentation + custom attributes)</task>
      <task id="5">Instrument Celery Workers (4 subtasks - worker_process_init signal pattern)</task>
      <task id="6">Implement Context Propagation FastAPI→Celery (4 subtasks - manual carrier injection)</task>
      <task id="7">Add Custom Spans for Enhancement Workflow (4 subtasks - job_queued, context_gathering, llm_call, ticket_update)</task>
      <task id="8">Implement Slow Trace Detection (4 subtasks - custom span processor)</task>
      <task id="9">Performance Testing and Optimization (5 subtasks - measure overhead <5%)</task>
      <task id="10">Integration Testing (10 subtasks - in-memory span exporter, no external Jaeger)</task>
      <task id="11">End-to-End Trace Validation (9 subtasks - verify in Jaeger UI)</task>
      <task id="12">Documentation (7 subtasks - operational guide following Story 4.5 patterns)</task>
      <task id="13">Configuration Management (5 subtasks - env vars, docker-compose, k8s)</task>
      <task id="14">Validation and Readiness (7 subtasks - final checklist)</task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="AC1">OpenTelemetry instrumentation - FastAPI: opentelemetry-instrumentation-fastapi installed and configured; HTTP requests to webhook endpoint automatically generate spans with route, method, status code attributes</criterion>
    <criterion id="AC2">OpenTelemetry instrumentation - Celery: opentelemetry-instrumentation-celery installed; Celery task execution automatically generates spans with task name, queue, status, exception (if any)</criterion>
    <criterion id="AC3">Trace context propagation: Trace context flows from FastAPI request → Redis queue → Celery task; single trace ID spans all components (manual carrier injection implemented)</criterion>
    <criterion id="AC4">Span coverage - webhook_received: FastAPI span captures webhook request details (tenant_id, ticket_id, priority) as span attributes</criterion>
    <criterion id="AC5">Span coverage - job_queued: Custom span records Redis queue operation with queue name and job ID</criterion>
    <criterion id="AC6">Span coverage - context_gathering: Custom span(s) capture context gathering operations (ticket history search, doc search, IP lookup) with result counts</criterion>
    <criterion id="AC7">Span coverage - llm_call: Custom span captures OpenAI/OpenRouter API call with model name, token count, latency</criterion>
    <criterion id="AC8">Span coverage - ticket_update: Custom span captures ServiceDesk Plus ticket update API call with success/failure status</criterion>
    <criterion id="AC9">Jaeger backend deployment: Jaeger deployed as Docker Compose service (local dev) and Kubernetes Deployment (production); OTLP receiver enabled on ports 4317 (gRPC) and 4318 (HTTP)</criterion>
    <criterion id="AC10">Trace export configuration: Application configured to export traces via OTLP exporter to Jaeger endpoint; BatchSpanProcessor configured with production settings (max_queue_size=2048, schedule_delay_millis=5000)</criterion>
    <criterion id="AC11">Sample trace visualization: Trigger test ticket enhancement → verify complete trace visible in Jaeger UI (http://localhost:16686) showing all spans from webhook to ticket update with correct parent-child relationships</criterion>
    <criterion id="AC12">Slow trace detection: Traces exceeding 60-second duration automatically tagged with slow_trace=true attribute; Jaeger query can filter slow traces</criterion>
    <criterion id="AC13">Performance overhead: Measure CPU/memory overhead with tracing enabled vs disabled; verify &lt;5% increase (target: &lt;2%)</criterion>
    <criterion id="AC14">Security - data redaction: Custom span processor redacts sensitive attributes (API keys, webhook secrets, full ticket content) before export</criterion>
    <criterion id="AC15">Configuration documentation: Document OpenTelemetry configuration including exporter settings, sampling configuration, span processor configuration, and Jaeger UI access</criterion>
    <criterion id="AC16">Testing: Integration tests verify trace generation, context propagation, and span attributes; tests use in-memory span exporter (no external Jaeger dependency for tests)</criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/PRD.md</path>
        <title>AI Agents Product Requirements Document</title>
        <section>NFR005: Observability</section>
        <snippet>System shall provide real-time visibility into agent operations through Prometheus metrics and Grafana dashboards, with audit logs retained for 90 days and distributed tracing for debugging failed enhancements</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Decision Architecture</title>
        <section>Observability Stack</section>
        <snippet>OpenTelemetry listed under "Observability" as future component. Multi-service architecture requiring cross-process trace correlation between FastAPI, Redis, and Celery workers</snippet>
      </doc>
      <doc>
        <path>docs/epics.md</path>
        <title>Epic 4: Monitoring & Operations</title>
        <section>Story 4.6 (Lines 900-915)</section>
        <snippet>Distributed tracing acceptance criteria: OpenTelemetry instrumentation for FastAPI and Celery, traces include webhook_received/job_queued/context_gathering/llm_call/ticket_update spans, context propagation across service boundaries, Jaeger backend, slow trace detection (>60s), tracing overhead <5%</snippet>
      </doc>
      <doc>
        <path>External: OpenTelemetry Python Troubleshooting</path>
        <title>Pre-fork Server Issues (Celery Worker Initialization)</title>
        <section>Deploy with Gunicorn and UvicornWorker</section>
        <snippet>For multi-worker deployments with fork model, use worker_process_init signal to initialize tracer provider AFTER fork to preserve background processes and threads. Critical for Celery prefork workers to avoid BatchSpanProcessor threading issues</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>src/main.py</path>
        <kind>FastAPI Application Entry Point</kind>
        <symbol>app (FastAPI instance)</symbol>
        <lines>1-80</lines>
        <reason>FastAPI application initialization - will add OpenTelemetry FastAPIInstrumentor here before app creation (Task 4)</reason>
      </artifact>
      <artifact>
        <path>src/workers/celery_app.py</path>
        <kind>Celery Application</kind>
        <symbol>celery_app</symbol>
        <lines>37</lines>
        <reason>Celery app initialization - will add worker_process_init signal handler for OpenTelemetry tracer initialization (Task 5)</reason>
      </artifact>
      <artifact>
        <path>src/workers/tasks.py</path>
        <kind>Celery Task Definitions</kind>
        <symbol>enhance_ticket</symbol>
        <lines>119-648</lines>
        <reason>Main enhancement task - will add custom spans for context_gathering, llm_call, ticket_update phases (Task 7)</reason>
      </artifact>
      <artifact>
        <path>src/monitoring/metrics.py</path>
        <kind>Prometheus Metrics Module</kind>
        <symbol>enhancement_requests_total, enhancement_duration_seconds</symbol>
        <lines>1-110</lines>
        <reason>Existing Prometheus metrics patterns - reference for observability implementation consistency. Tracing will complement these metrics</reason>
      </artifact>
      <artifact>
        <path>tests/integration/test_alertmanager_integration.py</path>
        <kind>Integration Test Example</kind>
        <symbol>TestAlertmanagerHealthChecks</symbol>
        <lines>1-50</lines>
        <reason>Story 4.5 testing patterns - use similar structure for distributed tracing tests with in-memory span exporter (Task 10)</reason>
      </artifact>
      <artifact>
        <path>docker-compose.yml</path>
        <kind>Docker Compose Configuration</kind>
        <symbol>prometheus, alertmanager, grafana services</symbol>
        <lines>92-183</lines>
        <reason>Existing monitoring stack deployment patterns - will add Jaeger service following same structure (Task 2)</reason>
      </artifact>
    </code>
    <dependencies>
      <python>
        <package name="opentelemetry-api" version="latest" reason="Core OpenTelemetry API for creating spans and traces"/>
        <package name="opentelemetry-sdk" version="latest" reason="OpenTelemetry SDK for tracer provider and processors"/>
        <package name="opentelemetry-instrumentation" version="latest" reason="Base instrumentation package"/>
        <package name="opentelemetry-instrumentation-fastapi" version="latest" reason="Automatic FastAPI HTTP request tracing"/>
        <package name="opentelemetry-instrumentation-celery" version="latest" reason="Automatic Celery task tracing"/>
        <package name="opentelemetry-instrumentation-httpx" version="latest" reason="Automatic HTTPX client tracing for external API calls"/>
        <package name="opentelemetry-instrumentation-redis" version="latest" reason="Optional Redis broker instrumentation"/>
        <package name="opentelemetry-exporter-otlp-proto-grpc" version="latest" reason="OTLP gRPC exporter for production Jaeger export"/>
        <existing name="fastapi" version=">=0.104.0" reason="Web framework being instrumented"/>
        <existing name="celery" version=">=5.3.4" reason="Task queue being instrumented"/>
        <existing name="httpx" version=">=0.25.2" reason="HTTP client being instrumented"/>
        <existing name="redis" version=">=5.0.1" reason="Message broker (context propagation carrier)"/>
        <existing name="prometheus-client" version=">=0.19.0" reason="Existing metrics - tracing complements these"/>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>Performance overhead must remain below 5% (target: &lt;2%) measured via load testing with/without tracing enabled</constraint>
    <constraint>Trace context propagation requires manual carrier injection between FastAPI and Celery (automatic propagation doesn't work across Redis queue boundary)</constraint>
    <constraint>Celery worker initialization: Must use worker_process_init signal to initialize tracer provider AFTER fork to avoid BatchSpanProcessor threading issues</constraint>
    <constraint>Sensitive data redaction: Custom span processor must redact API keys, webhook secrets, and full ticket content before export</constraint>
    <constraint>Sampling required for production: 10% default sampling rate (configurable via OTEL_TRACES_SAMPLER_ARG environment variable)</constraint>
    <constraint>Integration tests must use in-memory span exporter - no external Jaeger dependency for automated testing</constraint>
    <constraint>Follow Story 4.5 patterns: Docker Compose service + Kubernetes manifests + comprehensive operational documentation + integration test suite</constraint>
    <constraint>Configuration via environment variables (no hardcoded endpoints): OTEL_EXPORTER_OTLP_ENDPOINT, OTEL_SERVICE_NAME, OTEL_TRACES_SAMPLER, OTEL_TRACES_SAMPLER_ARG</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>OpenTelemetry Tracer Provider Initialization</name>
      <kind>Function signature</kind>
      <signature>def init_tracer_provider() -> None</signature>
      <path>src/observability/tracing.py (NEW)</path>
    </interface>
    <interface>
      <name>RedactionSpanProcessor</name>
      <kind>Class interface</kind>
      <signature>class RedactionSpanProcessor(SpanProcessor): on_end(span) -> None</signature>
      <path>src/observability/span_processors.py (NEW)</path>
    </interface>
    <interface>
      <name>SlowTraceProcessor</name>
      <kind>Class interface</kind>
      <signature>class SlowTraceProcessor(SpanProcessor): on_end(span) -> None</signature>
      <path>src/observability/span_processors.py (NEW)</path>
    </interface>
    <interface>
      <name>FastAPI Instrumentation Hook</name>
      <kind>Function call</kind>
      <signature>FastAPIInstrumentor.instrument_app(app)</signature>
      <path>src/main.py (MODIFIED - add after app creation)</path>
    </interface>
    <interface>
      <name>Celery Worker Process Init Signal</name>
      <kind>Signal handler</kind>
      <signature>@worker_process_init.connect(weak=False) def init_celery_tracing(*args, **kwargs)</signature>
      <path>src/workers/celery_app.py (MODIFIED)</path>
    </interface>
    <interface>
      <name>Trace Context Carrier Injection</name>
      <kind>Function pattern</kind>
      <signature>inject(carrier: dict) -> None; extract(carrier: dict) -> Context</signature>
      <path>opentelemetry.propagate module (FastAPI → Celery)</path>
    </interface>
    <interface>
      <name>Jaeger OTLP Receiver Endpoints</name>
      <kind>Network endpoints</kind>
      <signature>Port 4317 (gRPC), Port 4318 (HTTP), Port 16686 (UI)</signature>
      <path>docker-compose.yml jaeger service (NEW), k8s/jaeger-deployment.yaml (NEW)</path>
    </interface>
  </interfaces>
  <tests>
    <standards>
Following patterns from Story 4.5 (Alertmanager Integration - 33 integration tests):
- **Framework:** pytest with pytest-asyncio for async test support
- **Test Structure:** Class-based test organization (TestClassName)
- **Fixtures:** Use pytest fixtures for setup/teardown of test dependencies
- **Mocking:** Mock external services using unittest.mock (MagicMock, patch)
- **In-Memory Testing:** Use in-memory span exporter (InMemorySpanExporter from opentelemetry.sdk.trace.export.in_memory_span_exporter) for integration tests - no external Jaeger required
- **Coverage Target:** >80% for new observability code (tracing.py, span_processors.py)
- **Test Types:** Unit tests (individual processors), Integration tests (instrumentation + span creation), E2E tests (manual Jaeger UI validation)
- **Configuration:** Load test config from docker-compose.yml and k8s manifests using yaml.safe_load()
- **Assertions:** Verify span attributes, parent-child relationships, trace IDs, redaction, and performance overhead
    </standards>
    <locations>tests/integration/test_distributed_tracing.py (NEW - main integration test suite), tests/unit/test_span_processors.py (NEW - processor unit tests)</locations>
    <ideas>
      <test ac="AC1" desc="Verify FastAPIInstrumentor creates HTTP spans with route, method, status_code attributes"/>
      <test ac="AC2" desc="Verify CeleryInstrumentor creates task spans with task_name, queue, status, exception attributes"/>
      <test ac="AC3" desc="Verify trace context propagation - single trace_id across FastAPI + Celery spans using carrier injection"/>
      <test ac="AC4-AC8" desc="Verify custom spans created (job_queued, context_gathering, llm_call, ticket_update) with correct attributes"/>
      <test ac="AC9" desc="Verify docker-compose.yml has jaeger service with correct ports (4317, 4318, 16686) and health check"/>
      <test ac="AC10" desc="Verify BatchSpanProcessor configuration (max_queue_size=2048, schedule_delay_millis=5000)"/>
      <test ac="AC12" desc="Verify SlowTraceProcessor tags spans >60s with slow_trace=true attribute"/>
      <test ac="AC13" desc="Load test: Measure CPU/memory with tracing disabled, then enabled, verify overhead &lt;5%"/>
      <test ac="AC14" desc="Verify RedactionSpanProcessor removes sensitive keys (api_key, secret, password, token) from span attributes"/>
      <test ac="AC16" desc="Full integration test: Trigger webhook -> verify complete trace with all expected spans in InMemorySpanExporter"/>
    </ideas>
  </tests>
</story-context>
