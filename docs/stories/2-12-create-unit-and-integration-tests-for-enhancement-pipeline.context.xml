<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>2</epicId>
    <storyId>2.12</storyId>
    <title>Create Unit and Integration Tests for Enhancement Pipeline</title>
    <status>drafted</status>
    <generatedAt>2025-11-02</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/2-12-create-unit-and-integration-tests-for-enhancement-pipeline.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>QA engineer</asA>
    <iWant>comprehensive test coverage for the enhancement pipeline</iWant>
    <soThat>changes don't break core functionality and regressions are caught early</soThat>
    <tasks>
### Task 1: Audit and Enhance Existing Unit Tests (AC: 1, 6)
- Run coverage analysis on existing tests
- Enhance unit tests for Story 2.8 (LangGraph workflow)
- Enhance unit tests for Story 2.9 (LLM synthesis)
- Verify unit tests for Story 2.10 (ServiceDesk Plus API)
- Create unit tests for Story 2.11 (end-to-end integration)
- Create unit tests for context gathering components

### Task 2: Create Comprehensive Integration Tests (AC: 2, 5)
- Review existing integration test structure
- Implement happy path integration test
- Implement partial context failure test
- Implement LLM synthesis failure test
- Implement ServiceDesk Plus API failure test
- Implement timeout handling test
- Implement missing tenant configuration test
- Create performance benchmark test

### Task 3: Create Reusable Test Fixtures (AC: 3)
- Create fixtures directory structure
- Create ticket fixtures
- Create context gathering fixtures
- Create LLM response fixtures
- Create API response fixtures
- Document fixtures in tests/README.md

### Task 4: Configure CI Pipeline for Testing (AC: 4)
- Review existing CI workflow
- Add unit test stage to CI
- Add integration test stage to CI
- Add coverage reporting stage
- Configure PR required checks
- Test CI workflow end-to-end

### Task 5: Establish Performance Baselines (AC: 5)
- Create performance baseline documentation
- Run performance benchmark tests
- Capture baseline metrics
- Document baseline in performance-baseline.md
- Create performance regression test

### Task 6: Create Test Documentation (AC: 6)
- Create tests/README.md
- Document test structure
- Document how to run tests
- Document how to add new tests
- Document fixtures usage
- Document troubleshooting

### Task 7: Run Full Test Suite and Verify (AC: 7)
- Run full test suite locally
- Verify coverage thresholds met
- Verify CI pipeline passes
- Verify test suite performance
- Create test execution summary
    </tasks>
  </story>

  <acceptanceCriteria>
1. **Unit Test Coverage >80%** - Each component in the enhancement pipeline has unit tests achieving >80% code coverage. Coverage measured with pytest-cov. Coverage report generated and reviewed for gaps. Critical paths (happy path, error handling) fully covered.

2. **Integration Tests Cover Key Scenarios** - Happy path: Full enhancement workflow succeeds (webhook → context → LLM → ticket update). Error scenarios: LLM failure, ServiceDesk Plus API failure, partial context failure, timeout handling. Performance validation: End-to-end latency measured and within acceptable range (<60s p95). All integration tests pass reliably without flakiness.

3. **Mock Data Fixtures Created** - Reusable fixtures for: sample tickets, context gathering results, LLM responses, ServiceDesk Plus API responses. Fixtures stored in tests/fixtures/ directory with clear naming. Fixtures cover various scenarios: success, partial failure, complete failure, edge cases. Documented in tests/README.md with usage examples.

4. **Tests Run in CI Pipeline** - GitHub Actions workflow (.github/workflows/ci.yml) runs tests on every PR and main branch commit. Workflow stages: linting → unit tests → integration tests → coverage report. Test failures block PR merges (required status check). Test results visible in PR checks with clear failure messages.

5. **Performance Benchmarks Established** - Baseline performance metrics captured: p50, p95, p99 latency for end-to-end workflow. Performance regression test alerts if latency increases >20% from baseline. Throughput measured: enhancements per minute under load. Benchmarks documented in docs/performance-baseline.md.

6. **Test Documentation Complete** - tests/README.md created with sections: Overview, Running Tests, Adding New Tests, Fixtures, Troubleshooting. Each test file has module docstring explaining purpose and scope. Complex test setups documented with inline comments. Test naming convention followed: test_<component>_<scenario>_<expected_result>.

7. **All Tests Pass Successfully** - 100% of unit tests passing. 100% of integration tests passing. No skipped tests without documented reason. Test suite completes in <5 minutes for fast feedback.
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/PRD.md</path>
        <title>AI Agents Product Requirements Document</title>
        <section>NFR001: Performance</section>
        <snippet>System shall complete ticket enhancement within 120 seconds from webhook receipt to ticket update, with p95 latency under 60 seconds under normal load</snippet>
      </doc>
      <doc>
        <path>docs/PRD.md</path>
        <title>AI Agents Product Requirements Document</title>
        <section>NFR003: Reliability</section>
        <snippet>System shall achieve 99% success rate for ticket enhancements, with automatic retry for transient failures and graceful degradation when external services are unavailable</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>AI Agents Decision Architecture</title>
        <section>Testing Stack</section>
        <snippet>Pytest + pytest-asyncio for testing framework with async test support, fixtures, parametrization, FastAPI integration. Black auto-formatting + Ruff fast linting for code quality.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-2.md</path>
        <title>Epic 2 Technical Specification</title>
        <section>Stories 2.11-2.12</section>
        <snippet>Story 2.11 implements end-to-end integration workflow. Story 2.12 creates comprehensive unit and integration tests with >80% coverage requirement and performance benchmarking.</snippet>
      </doc>
      <doc>
        <path>https://docs.pytest.org/en/stable/explanation/goodpractices.html</path>
        <title>Pytest Good Practices</title>
        <section>Best Practices</section>
        <snippet>Package installation with pip, conventions for test discovery, choosing test layout, using tox for test automation, checking code style with flake8-pytest-style</snippet>
      </doc>
      <doc>
        <path>https://fastapi.tiangolo.com/advanced/async-tests/</path>
        <title>FastAPI Async Testing Documentation</title>
        <section>Testing Async Functions</section>
        <snippet>Use pytest.mark.anyio marker to run tests asynchronously and HTTPX's AsyncClient to send requests to the application for testing FastAPI with async dependencies</snippet>
      </doc>
      <doc>
        <path>https://pytest-with-eric.com/pytest-advanced/pytest-asyncio/</path>
        <title>Practical Guide to Async Testing with Pytest-Asyncio</title>
        <section>Async Testing Patterns</section>
        <snippet>Comprehensive guide to pytest-asyncio plugin for testing async functions, writing async fixtures, and using async mocking for FastAPI applications</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>src/workflows/enhancement_workflow.py</path>
        <kind>workflow orchestration</kind>
        <symbol>build_enhancement_workflow, execute_context_gathering, WorkflowState</symbol>
        <lines>full file</lines>
        <reason>LangGraph workflow orchestration (Story 2.8) - parallel context gathering nodes need comprehensive testing</reason>
      </artifact>
      <artifact>
        <path>src/services/llm_synthesis.py</path>
        <kind>service</kind>
        <symbol>synthesize_enhancement, format_tickets, format_kb_articles, truncate_to_words</symbol>
        <lines>full file</lines>
        <reason>LLM synthesis service (Story 2.9) - OpenRouter API integration, prompt formatting, word truncation needs unit tests</reason>
      </artifact>
      <artifact>
        <path>src/services/servicedesk_client.py</path>
        <kind>service</kind>
        <symbol>update_ticket_with_enhancement, convert_markdown_to_html, should_retry</symbol>
        <lines>full file</lines>
        <reason>ServiceDesk Plus API client (Story 2.10) - already has 29 unit tests with 100% coverage, verify correlation_id support</reason>
      </artifact>
      <artifact>
        <path>src/workers/tasks.py</path>
        <kind>celery tasks</kind>
        <symbol>enhance_ticket</symbol>
        <lines>full file</lines>
        <reason>End-to-end enhancement task (Story 2.11) - orchestrates workflow → LLM → API update, needs integration tests</reason>
      </artifact>
      <artifact>
        <path>src/services/ticket_search_service.py</path>
        <kind>service</kind>
        <symbol>search_similar_tickets</symbol>
        <lines>full file</lines>
        <reason>Ticket history search (Story 2.5) - PostgreSQL FTS, needs unit tests for query building and result filtering</reason>
      </artifact>
      <artifact>
        <path>src/services/kb_search.py</path>
        <kind>service</kind>
        <symbol>search_knowledge_base</symbol>
        <lines>full file</lines>
        <reason>Knowledge base search (Story 2.6) - API integration with Redis caching, needs timeout and cache tests</reason>
      </artifact>
      <artifact>
        <path>src/services/ip_lookup.py</path>
        <kind>service</kind>
        <symbol>extract_ip_addresses, lookup_ip_info</symbol>
        <lines>full file</lines>
        <reason>IP address extraction (Story 2.7) - regex extraction and inventory lookup, needs edge case testing</reason>
      </artifact>
      <artifact>
        <path>tests/conftest.py</path>
        <kind>test infrastructure</kind>
        <symbol>setup_test_env, env_vars</symbol>
        <lines>full file</lines>
        <reason>Shared test fixtures and pytest configuration - provides environment setup for all tests</reason>
      </artifact>
      <artifact>
        <path>tests/unit/test_servicedesk_client.py</path>
        <kind>test example</kind>
        <symbol>all test functions</symbol>
        <lines>full file</lines>
        <reason>Reference implementation - 29 tests achieving 100% coverage, pattern to replicate for other modules</reason>
      </artifact>
      <artifact>
        <path>tests/integration/test_end_to_end_workflow.py</path>
        <kind>integration test</kind>
        <symbol>all test functions</symbol>
        <lines>full file</lines>
        <reason>Integration test foundation from Story 2.11 - expand with 7 comprehensive scenarios</reason>
      </artifact>
    </code>
    <dependencies>
      <python>
        <package>pytest</package>
        <version>>=7.4.3</version>
      </python>
      <python>
        <package>pytest-asyncio</package>
        <version>>=0.21.1</version>
      </python>
      <python>
        <package>pytest-cov</package>
        <version>latest</version>
      </python>
      <python>
        <package>black</package>
        <version>>=23.11.0</version>
      </python>
      <python>
        <package>ruff</package>
        <version>>=0.1.6</version>
      </python>
      <python>
        <package>mypy</package>
        <version>>=1.7.1</version>
      </python>
      <python>
        <package>httpx</package>
        <version>>=0.25.2</version>
      </python>
      <python>
        <package>fastapi</package>
        <version>>=0.104.0</version>
      </python>
      <python>
        <package>celery</package>
        <version>>=5.3.4</version>
      </python>
      <python>
        <package>langgraph</package>
        <version>>=0.0.1</version>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    - **Test Coverage**: All unit tests must achieve >80% code coverage measured with pytest-cov
    - **Test Isolation**: Unit tests must mock external dependencies (database, Redis, APIs) for fast execution
    - **Async Support**: Use pytest-asyncio with @pytest.mark.asyncio decorator for all async test functions
    - **Test Naming**: Follow convention test_&lt;component&gt;_&lt;scenario&gt;_&lt;expected_result&gt;
    - **Fixture Usage**: Reuse fixtures from tests/fixtures/ directory, create new fixtures in conftest.py
    - **Integration Tests**: Require Docker dependencies (PostgreSQL, Redis) running for integration test suite
    - **Performance**: Test suite must complete in &lt;5 minutes for fast feedback in CI pipeline
    - **CI Integration**: All tests must pass in GitHub Actions CI workflow before PR merge
    - **Type Hints**: All test functions should have type hints for parameters and return values
    - **Documentation**: Each test file requires module docstring, complex tests need inline comments
    - **Black Formatting**: All test code must be formatted with Black (line-length=100)
    - **Ruff Linting**: All test code must pass Ruff linting checks
    - **No Flaky Tests**: Tests must be deterministic and pass reliably without timing-dependent failures
  </constraints>
  <interfaces>
    <interface>
      <name>WorkflowState (TypedDict)</name>
      <kind>Data structure</kind>
      <signature>class WorkflowState(TypedDict): ticket_id, description, similar_tickets, kb_articles, ip_info, error</signature>
      <path>src/workflows/state.py</path>
    </interface>
    <interface>
      <name>synthesize_enhancement()</name>
      <kind>Async function</kind>
      <signature>async def synthesize_enhancement(state: WorkflowState, correlation_id: str) -> str</signature>
      <path>src/services/llm_synthesis.py</path>
    </interface>
    <interface>
      <name>update_ticket_with_enhancement()</name>
      <kind>Async function</kind>
      <signature>async def update_ticket_with_enhancement(ticket_id: str, enhancement_text: str, correlation_id: str) -> bool</signature>
      <path>src/services/servicedesk_client.py</path>
    </interface>
    <interface>
      <name>enhance_ticket (Celery task)</name>
      <kind>Celery task</kind>
      <signature>@celery_app.task(bind=True, max_retries=3) def enhance_ticket(self, ticket_id: str, tenant_id: str) -> dict</signature>
      <path>src/workers/tasks.py</path>
    </interface>
    <interface>
      <name>execute_context_gathering()</name>
      <kind>Async function</kind>
      <signature>async def execute_context_gathering(ticket_id: str, description: str) -> WorkflowState</signature>
      <path>src/workflows/enhancement_workflow.py</path>
    </interface>
    <interface>
      <name>pytest fixtures (conftest.py)</name>
      <kind>Test fixtures</kind>
      <signature>@pytest.fixture def env_vars(monkeypatch) -> Callable, @pytest.fixture(scope="session") def setup_test_env()</signature>
      <path>tests/conftest.py</path>
    </interface>
  </interfaces>
  <tests>
    <standards>
The project uses pytest 7.4.3+ with pytest-asyncio for async test support. Test framework configured in pyproject.toml with asyncio_mode="auto" for automatic async test detection. All tests must follow naming convention: test_&lt;component&gt;_&lt;scenario&gt;_&lt;expected_result&gt;. Unit tests use unittest.mock and AsyncMock for mocking external dependencies. Integration tests require Docker (PostgreSQL, Redis) running locally. Coverage measured with pytest-cov targeting >80% for all modules in src/. CI pipeline runs: Black formatting check → Ruff linting → Mypy type checking → Unit tests → Integration tests. Test files require module docstrings and complex tests need inline comments. Use @pytest.mark.asyncio decorator for all async test functions. Fixtures defined in tests/conftest.py (env_vars, setup_test_env) and tests/fixtures/ directory.
    </standards>
    <locations>
tests/unit/ - Fast, isolated unit tests with mocked dependencies
tests/integration/ - End-to-end tests with real Docker dependencies
tests/fixtures/ - Reusable test fixtures (tickets, context, LLM responses, API responses)
tests/conftest.py - Shared pytest configuration and fixtures
.github/workflows/ci.yml - GitHub Actions CI pipeline configuration
    </locations>
    <ideas>
**AC1 - Unit Test Coverage >80%:**
- Test LangGraph workflow state transitions and parallel node execution (test_langgraph_workflow.py)
- Test LLM synthesis with mocked OpenRouter API responses including timeout and rate limit scenarios (test_llm_synthesis.py)
- Test ServiceDesk Plus client retry logic and markdown conversion (verify existing 29 tests in test_servicedesk_client.py)
- Test Celery enhance_ticket task with mocked workflow and API dependencies (test_celery_tasks.py)
- Test context gathering services: ticket search FTS queries, KB search with cache, IP extraction regex (test_ticket_search_service.py, test_kb_search.py, test_ip_lookup.py)

**AC2 - Integration Tests Cover Key Scenarios:**
- Happy path: Full workflow from webhook to ticket update with all services succeeding
- Partial context failure: KB search times out but workflow continues with degraded context
- LLM synthesis failure: OpenRouter API fails, fallback formatting used
- ServiceDesk Plus API failure: All retries exhausted, enhancement_history marked failed
- Timeout handling: Context gathering exceeds 30s budget, workflow continues with partial results
- Missing tenant config: Fail fast with clear error message
- Performance validation: Measure p50/p95/p99 latency, assert p95 <60s

**AC3 - Mock Data Fixtures:**
- Sample ticket payloads for webhook testing (various priorities, with/without IP addresses)
- WorkflowState objects with full and partial context
- OpenRouter API responses (success, timeout, rate limit, 500 error)
- ServiceDesk Plus API responses (success, 401, 500, connection error)

**AC4 - CI Pipeline Integration:**
- Add dedicated unit-tests and integration-tests jobs to .github/workflows/ci.yml
- Unit tests run without Docker dependencies, fast feedback (<2 min)
- Integration tests run after unit tests pass, with docker-compose setup (<5 min)
- Coverage report generated and uploaded as artifact, badge updated

**AC5 - Performance Baselines:**
- Run 100 iterations of happy path test to establish baseline
- Capture p50, p95, p99 latencies for end-to-end workflow
- Document baselines in docs/performance-baseline.md with regression thresholds (+20%)
- Create performance regression test that alerts if p95 exceeds baseline * 1.20

**AC6 - Test Documentation:**
- Create tests/README.md with sections: Overview, Running Tests, Adding Tests, Fixtures, Troubleshooting
- Document how to run specific test subsets (unit only, integration only, single test)
- Document Docker setup required for integration tests (docker-compose up -d)
- Document common issues: port conflicts, missing env vars, Docker not running

**AC7 - All Tests Pass:**
- Run pytest tests/ -v --cov=src --cov-report=term-missing locally
- Verify 100% pass rate, no skipped tests (or documented skip reasons)
- Verify coverage >80% overall and for critical modules
- Verify test suite completes in <5 minutes
    </ideas>
  </tests>
</story-context>
