<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>1</epicId>
    <storyId>1.5</storyId>
    <title>Implement Celery Worker Setup</title>
    <status>drafted</status>
    <generatedAt>2025-11-01</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/1-5-implement-celery-worker-setup.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>a developer</asA>
    <iWant>Celery workers configured to process jobs from Redis queue</iWant>
    <soThat>enhancement jobs can be processed asynchronously with proper concurrency</soThat>
    <tasks>
- Task 1: Install and configure Celery with Redis broker (AC: #1)
  - Add Celery dependencies to pyproject.toml: celery[redis]>=5.3.4
  - Create src/workers/celery_app.py module
  - Configure Celery app with Redis broker URL from settings
  - Configure Redis result backend from settings
  - Set task serializer to JSON
  - Enable UTC timezone
  - Verify Celery imports successfully

- Task 2: Configure Celery worker settings for production (AC: #4, #7)
  - Set concurrency to 4 workers per tech spec
  - Configure prefetch multiplier: 1
  - Set task time limits: 120s hard, 100s soft
  - Enable late acknowledgement
  - Configure retry logic with exponential backoff
  - Set accept content and result serializer to JSON

- Task 3: Create basic test task for validation (AC: #3)
  - Create src/workers/tasks.py module
  - Implement test task add_numbers(x, y)
  - Decorate with @celery_app.task
  - Test task execution and verify result
  - Create placeholder enhance_ticket task
  - Add task registration in workers/__init__.py

- Task 4: Add worker container to docker-compose (AC: #5)
  - Add worker service to docker-compose.yml
  - Use same Dockerfile as API
  - Set command: celery -A src.workers.celery_app worker
  - Mount source code volume for hot-reload
  - Set environment variables from .env
  - Add depends_on: redis, postgres
  - Set restart policy

- Task 5: Configure structured logging for workers (AC: #6)
  - Extend src/utils/logger.py for Celery worker context
  - Configure Loguru to include worker_id, task_name, task_id
  - Set log level from environment
  - Enable JSON output for production
  - Test logs visible via docker-compose logs worker

- Task 6: Add worker health monitoring (AC: #8)
  - Configure Celery events
  - Document Celery Flower deployment option
  - Create health check script using inspect ping
  - Document worker monitoring commands in README.md

- Task 7: Update Settings configuration with Celery fields (AC: #1)
  - Update src/config.py Settings class
  - Add celery_broker_url and celery_result_backend fields
  - Add celery_worker_concurrency field
  - Update .env.example with Celery connection strings

- Task 8: Create integration tests for Celery tasks (AC: #3, #7)
  - Create tests/integration/test_celery_tasks.py
  - Test worker connection to Redis broker
  - Test basic task execution
  - Test task retry logic
  - Test task timeout enforcement
  - Test task result persistence

- Task 9: Update README.md with Celery worker documentation (AC: #5, #6, #8)
  - Add Celery Worker Setup section
  - Document worker startup and configuration
  - Document monitoring commands
  - Document troubleshooting
  - Add example of creating and executing tasks
    </tasks>
  </story>

  <acceptanceCriteria>
1. Celery installed and configured with Redis as broker
2. Worker process starts successfully (celery -A app worker)
3. Basic test task executes successfully and returns result
4. Worker configuration includes concurrency settings (4-8 workers)
5. Worker container added to docker-compose
6. Worker logs visible and structured
7. Task retry logic configured with exponential backoff
8. Worker health monitoring endpoint available
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture Document</title>
        <section>Technology Stack Details - Message Queue &amp; Workers</section>
        <snippet>Redis 7.x as message broker + caching layer. Celery 5.x distributed task queue for async processing with retry logic and horizontal scaling. Celery configuration includes JSON serialization, UTC timezone, late acknowledgement (task_acks_late=True), worker_prefetch_multiplier=1, time limits (120s hard, 100s soft).</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture Document</title>
        <section>Project Structure - Workers</section>
        <snippet>src/workers/ directory contains celery_app.py (Celery application config) and tasks.py (Celery tasks including enhance_ticket). Docker structure includes celeryworker.dockerfile. Kubernetes includes deployment-worker.yaml and hpa-worker.yaml for autoscaling.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-1.md</path>
        <title>Technical Specification - Epic 1</title>
        <section>Detailed Design - Celery Application Configuration</section>
        <snippet>Celery app initialized with broker=settings.celery_broker_url, backend=settings.celery_result_backend. Configuration: task_serializer=json, timezone=UTC, enable_utc=True, task_acks_late=True, worker_prefetch_multiplier=1, task_time_limit=120, task_soft_time_limit=100. Concurrency: 4 workers per pod.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-1.md</path>
        <title>Technical Specification - Epic 1</title>
        <section>Non-Functional Requirements - Performance</section>
        <snippet>Redis max connections: 10 per service, connection pool timeout: 5 seconds. AOF persistence for durability. Worker concurrency set to 4 workers per pod, configurable via environment variable.</snippet>
      </doc>
      <doc>
        <path>docs/PRD.md</path>
        <title>Product Requirements Document</title>
        <section>User Journeys - Worker Processing Flow</section>
        <snippet>Celery worker picks up job and runs LangGraph workflow: searches ticket history, searches documentation, searches IP inventory, LLM analyzes context and synthesizes enhancement. Workers process jobs in parallel with complete multi-tenant isolation.</snippet>
      </doc>
      <doc>
        <path>docs/epics.md</path>
        <title>Epic Breakdown</title>
        <section>Epic 1 - Story 1.5: Implement Celery Worker Setup</section>
        <snippet>Install Celery with Redis broker, configure worker concurrency (4-8 workers), implement retry logic with exponential backoff, add worker container to docker-compose, structured logging, health monitoring endpoint.</snippet>
      </doc>
      <doc>
        <path>docs/stories/1-4-configure-redis-queue-for-message-processing.md</path>
        <title>Story 1.4 - Redis Queue Configuration</title>
        <section>Dev Notes - Learnings and Integration Patterns</section>
        <snippet>Redis connection pattern: redis://localhost:6379/{db_index}. Use DB 1 for Celery broker, DB 0 for cache. Redis connection pool: 10 max connections, 5-second timeout. JSON serialization matches Celery task serializer. Integration tests use async pytest patterns.</snippet>
      </doc>
      <doc>
        <path>docs/stories/1-1-initialize-project-structure-and-development-environment.md</path>
        <title>Story 1.1 - Project Structure Initialization</title>
        <section>Dev Agent Record - Placeholder Modules</section>
        <snippet>Placeholder created at src/workers/celery_app.py with Celery configuration (lines 12-32). Settings class in src/config.py includes celery_broker_url field with AI_AGENTS_ prefix. Loguru logger configured in src/utils/logger.py for structured logging.</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>src/config.py</path>
        <kind>configuration</kind>
        <symbol>Settings</symbol>
        <lines>14-84</lines>
        <reason>Settings class already includes celery_broker_url and celery_result_backend fields (lines 60-64). Celery worker will import settings instance to access broker configuration. Pattern: Pydantic BaseSettings with AI_AGENTS_ env prefix.</reason>
      </artifact>
      <artifact>
        <path>src/workers/celery_app.py</path>
        <kind>worker</kind>
        <symbol>celery_app</symbol>
        <lines>12</lines>
        <reason>Placeholder Celery app variable exists. Needs full implementation with broker/backend configuration, task serialization settings, time limits, and retry logic per tech spec.</reason>
      </artifact>
      <artifact>
        <path>src/utils/logger.py</path>
        <kind>utility</kind>
        <symbol>configure_logging</symbol>
        <lines>14-46</lines>
        <reason>Loguru logging already configured with colorized output, file rotation, and settings.log_level integration. Worker logging should follow same pattern but add worker-specific context (worker_id, task_name, task_id).</reason>
      </artifact>
      <artifact>
        <path>src/cache/redis_client.py</path>
        <kind>client</kind>
        <symbol>get_redis_client</symbol>
        <lines>13-30</lines>
        <reason>Existing Redis connection pattern with connection pooling, timeout handling. Reference for Redis connection best practices. Celery will manage its own Redis connection to broker (DB 1) separate from cache (DB 0).</reason>
      </artifact>
      <artifact>
        <path>docker-compose.yml</path>
        <kind>infrastructure</kind>
        <symbol>api service</symbol>
        <lines>40-64</lines>
        <reason>Existing API service pattern to replicate for worker service. Same build context, env_file, volume mounts, health check approach. Worker will depend on postgres and redis, use same image with different command.</reason>
      </artifact>
      <artifact>
        <path>tests/integration/test_redis_queue.py</path>
        <kind>test</kind>
        <symbol>TestRedisConnection, TestQueueOperations</symbol>
        <lines>1-200</lines>
        <reason>Existing integration test structure with async pytest patterns, Redis fixture usage, test organization. Celery task tests should follow same async test patterns and class organization.</reason>
      </artifact>
    </code>
    <dependencies>
      <ecosystem name="python">
        <package name="celery" version="[redis]&gt;=5.3.4" reason="Task queue with Redis support - core dependency for worker infrastructure"/>
        <package name="redis" version="&gt;=5.0.1" reason="Already installed for Redis client, used by Celery for broker/backend"/>
        <package name="fastapi" version="&gt;=0.104" reason="Already installed for API framework"/>
        <package name="pydantic" version="&gt;=2.0" reason="Already installed, used in Settings configuration"/>
        <package name="pytest" version="latest" reason="Already installed for testing framework"/>
        <package name="pytest-asyncio" version="latest" reason="Already installed for async test support"/>
        <package name="loguru" version="latest" reason="Already installed for structured logging"/>
      </ecosystem>
      <ecosystem name="docker">
        <service name="postgres" version="17-alpine" reason="PostgreSQL database - already configured in docker-compose"/>
        <service name="redis" version="7-alpine" reason="Redis broker and cache - already configured with AOF persistence"/>
        <service name="python" version="3.12-slim" reason="Base image for API and worker containers"/>
      </ecosystem>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>Use Pydantic BaseSettings pattern with AI_AGENTS_ prefix for all environment variables (established in src/config.py)</constraint>
    <constraint>All Celery configuration must use JSON serialization (task_serializer=json, accept_content=["json"]) to match Redis queue format</constraint>
    <constraint>Worker concurrency fixed at 4 workers per pod per tech spec NFR (configurable via celery_worker_concurrency setting)</constraint>
    <constraint>Task time limits: 120 seconds hard limit, 100 seconds soft limit per tech spec (prevents runaway processes)</constraint>
    <constraint>Late acknowledgement required (task_acks_late=True) to prevent message loss on worker crashes</constraint>
    <constraint>Prefetch multiplier must be 1 (worker_prefetch_multiplier=1) for fair task distribution and memory efficiency</constraint>
    <constraint>Use Redis DB 1 for Celery broker/backend, DB 0 reserved for cache (established in Story 1.4)</constraint>
    <constraint>Follow Loguru logging pattern from src/utils/logger.py with structured JSON output for production</constraint>
    <constraint>All integration tests must use async pytest patterns established in tests/integration/</constraint>
    <constraint>Worker service in docker-compose must follow same patterns as API service (health checks, volume mounts, restart policy)</constraint>
    <constraint>File size limit: No file should exceed 500 lines per project CLAUDE.md standards</constraint>
    <constraint>All functions require Google-style docstrings with type hints per project standards</constraint>
    <constraint>Retry logic: Max 3 retries with exponential backoff (2s, 4s, 8s) and jitter per architecture.md error handling strategy</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>Settings.celery_broker_url</name>
      <kind>configuration field</kind>
      <signature>celery_broker_url: str (Pydantic Field, required, from AI_AGENTS_CELERY_BROKER_URL)</signature>
      <path>src/config.py</path>
      <usage>Used by Celery app initialization to connect to Redis broker (redis://localhost:6379/1)</usage>
    </interface>
    <interface>
      <name>Settings.celery_result_backend</name>
      <kind>configuration field</kind>
      <signature>celery_result_backend: str (Pydantic Field, required, from AI_AGENTS_CELERY_RESULT_BACKEND)</signature>
      <path>src/config.py</path>
      <usage>Used by Celery app to store task results in Redis (same URL as broker)</usage>
    </interface>
    <interface>
      <name>configure_logging</name>
      <kind>function</kind>
      <signature>def configure_logging() -&gt; None</signature>
      <path>src/utils/logger.py</path>
      <usage>Call at worker startup to initialize Loguru logging with colorized output (dev) and file rotation (prod). Extend to add worker context fields.</usage>
    </interface>
    <interface>
      <name>Celery task decorator pattern</name>
      <kind>decorator</kind>
      <signature>@celery_app.task(bind=True, autoretry_for=(Exception,), retry_kwargs={'max_retries': 3, 'countdown': 2}, retry_backoff=True)</signature>
      <path>src/workers/tasks.py</path>
      <usage>Decorate all task functions with Celery task decorator. Use bind=True for access to task instance (self). Configure retry logic per tech spec.</usage>
    </interface>
    <interface>
      <name>Docker health check pattern</name>
      <kind>infrastructure</kind>
      <signature>healthcheck: test: ["CMD", "command"], interval: 10s, timeout: 5s, retries: 3</signature>
      <path>docker-compose.yml</path>
      <usage>Worker service should implement health check using: celery -A src.workers.celery_app inspect ping</usage>
    </interface>
  </interfaces>
  <tests>
    <standards>
Testing framework: Pytest with pytest-asyncio for async test support. All integration tests must use async/await patterns established in tests/integration/. Test organization: Group tests into classes by feature area (TestCeleryConnection, TestTaskExecution, TestRetryLogic, TestTimeout, TestResultPersistence). Each test class focuses on a specific aspect of Celery worker functionality. Use descriptive test names following pattern: test_{feature}_{scenario}_{expected_outcome}. All tests require docstrings explaining purpose, setup, and assertions. Test coverage target: All acceptance criteria must have corresponding integration tests. Use pytest fixtures for Redis connection, Celery app setup, and test data. Tests run in Docker environment with real Redis and worker instances for realistic validation. Follow Black code formatting and include type hints for all test functions.
    </standards>
    <locations>
      <location>tests/integration/test_celery_tasks.py - Primary Celery task integration tests</location>
      <location>tests/unit/ - Unit tests for task functions (if isolating logic from Celery)</location>
      <location>Execute via: docker-compose exec api pytest tests/integration/test_celery_tasks.py -v</location>
    </locations>
    <ideas>
      <test id="AC1" acceptance_criteria="Celery installed and configured with Redis as broker">
        <name>test_celery_app_initialization</name>
        <description>Verify Celery app initializes successfully with correct broker URL, backend URL, and configuration settings (task_serializer=json, timezone=UTC, task_acks_late=True). Assert celery_app.conf values match tech spec requirements.</description>
      </test>
      <test id="AC1" acceptance_criteria="Celery installed and configured with Redis as broker">
        <name>test_celery_broker_connection</name>
        <description>Verify Celery worker can connect to Redis broker at DB index 1. Use celery inspect ping to confirm worker responsiveness. Assert connection succeeds within timeout.</description>
      </test>
      <test id="AC2" acceptance_criteria="Worker process starts successfully">
        <name>test_worker_startup_in_docker</name>
        <description>Start worker via docker-compose up worker and verify process starts without errors. Check docker logs for successful broker connection message. Assert worker enters ready state.</description>
      </test>
      <test id="AC3" acceptance_criteria="Basic test task executes successfully and returns result">
        <name>test_add_numbers_task_execution</name>
        <description>Call add_numbers.delay(2, 3) and verify result.get() returns 5. Assert task completes within reasonable time (under 5 seconds). Verify result stored in Redis backend.</description>
      </test>
      <test id="AC3" acceptance_criteria="Basic test task executes successfully and returns result">
        <name>test_task_result_retrieval</name>
        <description>Execute task, obtain task ID, retrieve result using AsyncResult(task_id). Assert result matches expected value and task status is SUCCESS.</description>
      </test>
      <test id="AC4" acceptance_criteria="Worker configuration includes concurrency settings">
        <name>test_worker_concurrency_setting</name>
        <description>Verify celery_app.conf.worker_concurrency equals 4 per tech spec. Use celery inspect stats to confirm 4 worker processes active. Assert pool configuration matches requirements.</description>
      </test>
      <test id="AC5" acceptance_criteria="Worker container added to docker-compose">
        <name>test_worker_service_exists_in_compose</name>
        <description>Parse docker-compose.yml and verify worker service defined with correct command (celery -A src.workers.celery_app worker), depends_on (redis, postgres), and volume mounts. Assert service configuration matches API service patterns.</description>
      </test>
      <test id="AC6" acceptance_criteria="Worker logs visible and structured">
        <name>test_worker_logs_structured_format</name>
        <description>Execute task and capture worker logs via docker-compose logs worker. Assert logs contain structured fields: timestamp, log level, worker_id, task_name, task_id, message. Verify JSON format for production environment.</description>
      </test>
      <test id="AC7" acceptance_criteria="Task retry logic configured with exponential backoff">
        <name>test_task_retry_on_failure</name>
        <description>Create task that raises exception on first 2 attempts, succeeds on 3rd. Verify task retries with exponential backoff (2s, 4s intervals). Assert final result SUCCESS after retries. Check retry count in task history.</description>
      </test>
      <test id="AC7" acceptance_criteria="Task retry logic configured with exponential backoff">
        <name>test_task_max_retries_exhausted</name>
        <description>Create task that always fails. Verify task retries exactly 3 times per max_retries config. Assert final status FAILURE after exhausting retries. Verify exception logged correctly.</description>
      </test>
      <test id="AC7" acceptance_criteria="Task retry logic configured with exponential backoff">
        <name>test_task_timeout_enforcement</name>
        <description>Create long-running task exceeding soft_time_limit (100s) and hard_time_limit (120s). Verify SoftTimeLimitExceeded raised at 100s, hard kill at 120s. Assert task marked as FAILURE with timeout exception.</description>
      </test>
      <test id="AC8" acceptance_criteria="Worker health monitoring endpoint available">
        <name>test_celery_inspect_ping</name>
        <description>Execute celery -A src.workers.celery_app inspect ping command. Assert worker responds with pong message. Verify response time under 1 second indicating healthy worker.</description>
      </test>
      <test id="AC8" acceptance_criteria="Worker health monitoring endpoint available">
        <name>test_celery_inspect_active_tasks</name>
        <description>Start long-running task, then execute celery inspect active. Assert active task list includes running task with correct task_id and name. Verify command returns current worker state.</description>
      </test>
      <test id="AC8" acceptance_criteria="Worker health monitoring endpoint available">
        <name>test_celery_inspect_registered_tasks</name>
        <description>Execute celery -A src.workers.celery_app inspect registered. Assert registered task list includes add_numbers and enhance_ticket (placeholder). Verify all decorated tasks appear in registry.</description>
      </test>
    </ideas>
  </tests>
</story-context>
