# Horizontal Pod Autoscaler (HPA)
# Story 5.2: Deploy Application to Production Environment
# Auto-scale Celery workers 3-10 replicas based on CPU and memory

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-agents-worker-hpa
  namespace: production
  labels:
    app: ai-agents
    component: worker
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-agents-worker
  minReplicas: 3
  maxReplicas: 10
  metrics:
    # CPU-based scaling
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70  # Scale up when avg CPU > 70%

    # Memory-based scaling
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80  # Scale up when avg memory > 80%

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60  # Wait 60s before scaling up
      policies:
        - type: Percent
          value: 50  # Scale up by 50% of current replicas
          periodSeconds: 60
        - type: Pods
          value: 2  # Or add 2 pods
          periodSeconds: 60
      selectPolicy: Max  # Use whichever policy adds more pods

    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 minutes before scaling down (prevent flapping)
      policies:
        - type: Percent
          value: 25  # Scale down by 25% of current replicas
          periodSeconds: 60
        - type: Pods
          value: 1  # Or remove 1 pod
          periodSeconds: 60
      selectPolicy: Min  # Use whichever policy removes fewer pods

---
# Optional: API HPA (if needed for high traffic scenarios)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-agents-api-hpa
  namespace: production
  labels:
    app: ai-agents
    component: api
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-agents-api
  minReplicas: 2
  maxReplicas: 6
  metrics:
    # CPU-based scaling
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 75

    # Memory-based scaling
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 85

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
        - type: Percent
          value: 100  # Double the replicas
          periodSeconds: 30
        - type: Pods
          value: 1
          periodSeconds: 30
      selectPolicy: Max

    scaleDown:
      stabilizationWindowSeconds: 180  # Wait 3 minutes before scaling down
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60
      selectPolicy: Min

---
# Note: For production, consider adding custom metrics for more intelligent scaling:
# - Redis queue depth (scale workers based on pending tasks)
# - Request latency (scale API based on response times)
# - Error rate (prevent scaling down during incidents)
#
# Custom metrics require Prometheus Adapter or KEDA (Kubernetes Event Driven Autoscaling)
# Example with Prometheus Adapter:
#
# - type: Pods
#   pods:
#     metric:
#       name: redis_queue_depth
#     target:
#       type: AverageValue
#       averageValue: "100"  # Scale when queue has >100 tasks per worker
