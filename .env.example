# AI Agents Environment Configuration Template
# Copy this file to .env and fill in your actual values
# NEVER commit the .env file - it contains sensitive information

# =============================================================================
# Docker Environment Configuration
# =============================================================================
# NOTE: When running with docker-compose, use service names (postgres, redis)
# instead of localhost for inter-container communication

# PostgreSQL Container Initialization
# These variables are used by the postgres Docker image to initialize the database
POSTGRES_USER=aiagents
POSTGRES_PASSWORD=password
POSTGRES_DB=ai_agents

# Database Configuration
# PostgreSQL connection string for asyncpg driver
# Format: postgresql+asyncpg://user:password@host:port/database
# IMPORTANT: Use 'postgres' as hostname when running in Docker, 'localhost' for local dev
AI_AGENTS_DATABASE_URL=postgresql+asyncpg://aiagents:password@postgres:5432/ai_agents

# Database connection pool size (default: 20)
AI_AGENTS_DATABASE_POOL_SIZE=20

# Database Admin Configuration (Story 3.1 - Row-Level Security)
# Admin role with BYPASSRLS for migrations and maintenance tasks
# SECURITY: Store credentials securely - DO NOT use in application code
# Format: postgresql+asyncpg://user:password@host:port/database
AI_AGENTS_DATABASE_ADMIN_URL=postgresql+asyncpg://db_admin:admin_secure_password_change_in_production@postgres:5432/ai_agents

# Redis Configuration
# Redis connection URL for caching
# Format: redis://host:port/db
# IMPORTANT: Use 'redis' as hostname when running in Docker, 'localhost' for local dev
AI_AGENTS_REDIS_URL=redis://redis:6379/0

# Maximum number of Redis connections (default: 10)
AI_AGENTS_REDIS_MAX_CONNECTIONS=10

# Celery Configuration
# Redis broker URL for Celery task queue
# Using database 1 to separate from cache (database 0)
# IMPORTANT: Use 'redis' as hostname when running in Docker, 'localhost' for local dev
AI_AGENTS_CELERY_BROKER_URL=redis://redis:6379/1

# Celery result backend (same as broker)
AI_AGENTS_CELERY_RESULT_BACKEND=redis://redis:6379/1

# Number of concurrent Celery workers per pod (default: 4)
# Range: 1-16, recommended: 4 for balanced throughput/memory
AI_AGENTS_CELERY_WORKER_CONCURRENCY=4

# Application Environment
# Options: development, staging, production
AI_AGENTS_ENVIRONMENT=development

# Logging Configuration
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
AI_AGENTS_LOG_LEVEL=INFO

# Security Configuration (Epic 2 & 3)
# Shared secret for HMAC-SHA256 webhook signature validation
# Generate with: openssl rand -hex 32
# IMPORTANT: Must be minimum 32 characters, use strong random value
AI_AGENTS_WEBHOOK_SECRET=your-webhook-secret-here-minimum-32-chars-required

# Admin API Key (minimum 32 characters)
# Generate with: openssl rand -base64 32
AI_AGENTS_ADMIN_API_KEY=your-admin-api-key-here-minimum-32-chars-required

# Encryption Key for Sensitive Fields (Story 3.2 & 3.3)
# Fernet symmetric encryption key for tenant configurations and secrets
# Generate with: python -c "from src.utils.encryption import generate_encryption_key; print(generate_encryption_key())"
# CRITICAL: Back up this key in a secure location. Loss means tenant configs cannot be decrypted.
AI_AGENTS_ENCRYPTION_KEY=gAAAAABlwXxk-k_Nz5mPqR-9jL2xF8vB3cZ1aQ_yH7mJ9dKwL-sA0pR1bC=

# Individual Secret Fields (Story 3.3 - Kubernetes Secrets)
# These are loaded from Kubernetes Secrets in production, .env in development
# For local development, fill in your own values
# In production, these are injected by Kubernetes Secrets via secretKeyRef

# PostgreSQL Password (minimum 12 characters)
# Must match the password configured in your PostgreSQL instance
# Generate with: openssl rand -base64 32
AI_AGENTS_POSTGRES_PASSWORD=your-postgres-password-here-min-12-chars

# Redis Password (minimum 12 characters)
# Must match the password configured in your Redis instance
# Generate with: openssl rand -base64 32
AI_AGENTS_REDIS_PASSWORD=your-redis-password-here-min-12-chars

# OpenAI API Key (for GPT models, NOT OpenRouter)
# Get from: https://platform.openai.com/account/api-keys
# Format: sk-proj-... or sk-...
AI_AGENTS_OPENAI_API_KEY=sk-proj-your-openai-api-key-here

# API Configuration (future stories)
# AI_AGENTS_API_HOST=0.0.0.0
# AI_AGENTS_API_PORT=8000

# ServiceDesk Plus Integration (Epic 2)
# AI_AGENTS_SERVICEDESK_API_URL=https://your-servicedesk-instance.com/api/v3
# AI_AGENTS_SERVICEDESK_API_KEY=your_api_key_here

# OpenRouter/LLM Configuration (Story 2.9)
# OpenRouter API key for multi-model LLM access via OpenAI SDK
# Get your key from: https://openrouter.ai/keys
# Format: sk-or-v1-...
AI_AGENTS_OPENROUTER_API_KEY=sk-or-v1-your-api-key-here

# OpenRouter API base URL (usually doesn't need to change)
AI_AGENTS_OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Site URL for HTTP-Referer header (used by OpenRouter for rankings/analytics)
# Set this to your deployment URL
AI_AGENTS_OPENROUTER_SITE_URL=https://ai-agents.yourcompany.com

# App name for X-Title header (used by OpenRouter for rankings)
AI_AGENTS_OPENROUTER_APP_NAME=AI Agents Enhancement Platform

# LLM Model selection (default: openai/gpt-4o-mini for cost efficiency)
# Options: openai/gpt-4o-mini (default), openai/gpt-4, openai/gpt-4-turbo, etc.
AI_AGENTS_LLM_MODEL=openai/gpt-4o-mini

# Maximum tokens for LLM response (default: 1000 ≈ 500 words)
# Increase if you need longer responses, decrease to save costs
AI_AGENTS_LLM_MAX_TOKENS=1000

# LLM temperature for output consistency (default: 0.3 for focused, deterministic output)
# Range: 0.0 (deterministic) to 1.0 (creative)
# 0.3 recommended for synthesis (consistent output), 0.7+ for creative tasks
AI_AGENTS_LLM_TEMPERATURE=0.3

# Timeout for LLM API calls in seconds (default: 30)
# Range: 5-120 seconds
# Set to 30 to provide budget within 120s overall enhancement workflow timeout
AI_AGENTS_LLM_TIMEOUT_SECONDS=30

# Monitoring (Epic 4)
# AI_AGENTS_METRICS_PORT=9090

# =============================================================================
# Distributed Tracing Configuration (Story 4.6)
# =============================================================================
# OpenTelemetry Configuration for Distributed Tracing with Jaeger
# Provides end-to-end visibility into ticket enhancement workflow across
# FastAPI (webhook) → Redis queue → Celery workers → External APIs

# Jaeger Collector Endpoint (gRPC protocol on port 4317)
# Local development: Use 'jaeger' as hostname when running in Docker
# Production: Update to your Jaeger collector service endpoint
# Format: http://host:4317
OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317

# Service Name
# Identifies this service in Jaeger traces and metrics
# Appears in Jaeger UI service dropdown
OTEL_SERVICE_NAME=ai-agents-enhancement

# Trace Sampling Strategy
# Options: always_off, always_on, traceidratio, parentbased_always_off, parentbased_always_on, parentbased_traceidratio
# Recommended: traceidratio for percentage-based sampling
OTEL_TRACES_SAMPLER=traceidratio

# Sampling Rate (0.0 to 1.0)
# 0.0 = no traces sampled (disable tracing)
# 0.01 = 1% sampling (10 traces/day for 1000 tickets, ~1MB/day storage)
# 0.1 = 10% sampling (100 traces/day for 1000 tickets, ~10MB/day storage) [DEFAULT]
# 0.5 = 50% sampling (500 traces/day, ~50MB/day storage)
# 1.0 = 100% sampling (all traces, production not recommended for high volume)
# For development/testing: Use 1.0
# For staging: Use 0.5
# For production: Use 0.01-0.1 depending on traffic and storage budget
OTEL_TRACES_SAMPLER_ARG=0.1

# Deployment Environment Tag
# Used to tag all spans with deployment environment for filtering in Jaeger
# Options: development, staging, production
DEPLOYMENT_ENV=development

# Batch Span Processor Configuration (Advanced)
# These settings control how spans are batched and exported to Jaeger
# Typically no changes needed, but tuning available for high-throughput scenarios:
#
# Max spans to buffer before forcing export:
#   - Default: 2048
#   - High-throughput: 4096-8192
#   - Low-memory: 512-1024
# OTEL_BSP_MAX_QUEUE_SIZE=2048
#
# Delay between exports (milliseconds):
#   - Default: 5000 (5 seconds)
#   - Faster export: 500-1000
#   - Batch optimization: 10000+
# OTEL_BSP_SCHEDULE_DELAY_MILLIS=5000
#
# Max spans per batch:
#   - Default: 512
#   - For larger batches: 1024-2048
# OTEL_BSP_MAX_EXPORT_BATCH_SIZE=512

# =============================================================================
# Alertmanager Configuration (Story 4.5)
# =============================================================================
# Slack Integration for Alert Notifications
# Create an Incoming Webhook in your Slack workspace:
#   1. Go to Slack App Directory → search "Incoming WebHooks"
#   2. Create New Webhook for your alert channel
#   3. Copy the webhook URL and paste here
# Format: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR_TEAM_ID/YOUR_BOT_ID/YOUR_WEBHOOK_TOKEN

# PagerDuty Integration for Critical Alert Escalation
# Set up PagerDuty integration in your service:
#   1. Go to PagerDuty → Service Settings → Integrations
#   2. Add integration for "Prometheus" or generic webhook
#   3. Copy the routing/integration key
# Format: String with integration key provided by PagerDuty
PAGERDUTY_INTEGRATION_KEY=your-pagerduty-integration-key-here

# SMTP Configuration for Email Alerts (Optional)
# SMTP server endpoint and credentials for email alert delivery
# Common providers: smtp.gmail.com (587), smtp.sendgrid.net (587), your-company-smtp (25/587)
SMTP_SMARTHOST=smtp.example.com:587

# SMTP authentication username (usually email address)
# For Gmail: your-email@gmail.com
# For SendGrid: apikey
# For company SMTP: your-username
SMTP_USERNAME=your-smtp-username@example.com

# SMTP authentication password
# For Gmail: use App Password (not regular password)
# For SendGrid: use API key as password
# For company SMTP: use your password
SMTP_PASSWORD=your-smtp-password-here

# Email sender address (the From: address for alert emails)
# This should be a valid email address
EMAIL_FROM=alerts@ai-agents.local

# Email recipient address (where alerts are sent)
# Separate multiple addresses with commas if supported by your setup
EMAIL_RECIPIENT=oncall@example.com
