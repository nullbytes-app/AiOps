# AI Agents Environment Configuration Template
# Copy this file to .env and fill in your actual values
# NEVER commit the .env file - it contains sensitive information

# =============================================================================
# Docker Environment Configuration
# =============================================================================
# NOTE: When running with docker-compose, use service names (postgres, redis)
# instead of localhost for inter-container communication

# PostgreSQL Container Initialization
# These variables are used by the postgres Docker image to initialize the database
POSTGRES_USER=aiagents
POSTGRES_PASSWORD=password
POSTGRES_DB=ai_agents

# Database Configuration
# PostgreSQL connection string for asyncpg driver
# Format: postgresql+asyncpg://user:password@host:port/database
# IMPORTANT: Use 'postgres' as hostname when running in Docker, 'localhost' for local dev
AI_AGENTS_DATABASE_URL=postgresql+asyncpg://aiagents:password@postgres:5432/ai_agents

# Database connection pool size (default: 20)
AI_AGENTS_DATABASE_POOL_SIZE=20

# Database Admin Configuration (Story 3.1 - Row-Level Security)
# Admin role with BYPASSRLS for migrations and maintenance tasks
# SECURITY: Store credentials securely - DO NOT use in application code
# Format: postgresql+asyncpg://user:password@host:port/database
AI_AGENTS_DATABASE_ADMIN_URL=postgresql+asyncpg://db_admin:admin_secure_password_change_in_production@postgres:5432/ai_agents

# Redis Configuration
# Redis connection URL for caching
# Format: redis://host:port/db
# IMPORTANT: Use 'redis' as hostname when running in Docker, 'localhost' for local dev
AI_AGENTS_REDIS_URL=redis://redis:6379/0

# Maximum number of Redis connections (default: 10)
AI_AGENTS_REDIS_MAX_CONNECTIONS=10

# Celery Configuration
# Redis broker URL for Celery task queue
# Using database 1 to separate from cache (database 0)
# IMPORTANT: Use 'redis' as hostname when running in Docker, 'localhost' for local dev
AI_AGENTS_CELERY_BROKER_URL=redis://redis:6379/1

# Celery result backend (same as broker)
AI_AGENTS_CELERY_RESULT_BACKEND=redis://redis:6379/1

# Number of concurrent Celery workers per pod (default: 4)
# Range: 1-16, recommended: 4 for balanced throughput/memory
AI_AGENTS_CELERY_WORKER_CONCURRENCY=4

# Application Environment
# Options: development, staging, production
AI_AGENTS_ENVIRONMENT=development

# Logging Configuration
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
AI_AGENTS_LOG_LEVEL=INFO

# Security Configuration (Epic 2 & 3)
# Shared secret for HMAC-SHA256 webhook signature validation
# Generate with: openssl rand -base64 32
# IMPORTANT: Must be minimum 32 characters, use strong random value
# NOTE: This secret is used by both the API and HMAC proxy to ensure signature validation works correctly
# All agents created will use this shared secret for webhook validation
AI_AGENTS_WEBHOOK_SECRET=your-webhook-secret-here-minimum-32-chars-required

# Admin API Key (minimum 32 characters)
# Generate with: openssl rand -base64 32
AI_AGENTS_ADMIN_API_KEY=your-admin-api-key-here-minimum-32-chars-required

# Encryption Key for Sensitive Fields (Story 3.2 & 3.3)
# Fernet symmetric encryption key for tenant configurations and secrets
# Generate with: python -c "from src.utils.encryption import generate_encryption_key; print(generate_encryption_key())"
# CRITICAL: Back up this key in a secure location. Loss means tenant configs cannot be decrypted.
AI_AGENTS_ENCRYPTION_KEY=gAAAAABlwXxk-k_Nz5mPqR-9jL2xF8vB3cZ1aQ_yH7mJ9dKwL-sA0pR1bC=

# Individual Secret Fields (Story 3.3 - Kubernetes Secrets)
# These are loaded from Kubernetes Secrets in production, .env in development
# For local development, fill in your own values
# In production, these are injected by Kubernetes Secrets via secretKeyRef

# PostgreSQL Password (minimum 12 characters)
# Must match the password configured in your PostgreSQL instance
# Generate with: openssl rand -base64 32
AI_AGENTS_POSTGRES_PASSWORD=your-postgres-password-here-min-12-chars

# Redis Password (minimum 12 characters)
# Must match the password configured in your Redis instance
# Generate with: openssl rand -base64 32
AI_AGENTS_REDIS_PASSWORD=your-redis-password-here-min-12-chars

# OpenAI API Key (for GPT models, NOT OpenRouter)
# Get from: https://platform.openai.com/account/api-keys
# Format: sk-proj-... or sk-...
AI_AGENTS_OPENAI_API_KEY=sk-proj-your-openai-api-key-here
OPENAI_API_KEY=${AI_AGENTS_OPENAI_API_KEY}

# =============================================================================
# LiteLLM Proxy Configuration (Story 8.1)
# =============================================================================
# LiteLLM provides a unified LLM gateway with multi-provider support, automatic
# fallbacks, cost tracking, and virtual keys for multi-tenant deployments
# Documentation: See README.md section "LiteLLM Proxy Integration"

# LiteLLM Master Key (Admin API key for proxy management)
# Must start with 'sk-' prefix, changeable after setup
# Used for: Creating virtual keys, managing users, accessing admin endpoints
# Generate with: openssl rand -hex 16 | awk '{print "sk-" $0}'
# SECURITY: Store securely - grants full access to LiteLLM proxy
LITELLM_MASTER_KEY=sk-1234

# LiteLLM Salt Key (Encryption key for API credentials)
# 32-byte base64-encoded key for encrypting provider API keys in database
# CRITICAL: CANNOT change after adding first model - loss means re-setup required
# Generate with: openssl rand -base64 32
# Recommended: Use 1Password or similar password generator
# Backup: Store in secure vault (1Password, AWS Secrets Manager, etc.)
LITELLM_SALT_KEY=sk-your-litellm-salt-key-here-use-openssl-rand-base64-32

# LiteLLM Webhook Secret (Story 8.10C - Budget Alert Webhook Signature Validation)
# Secret used to validate HMAC signatures on webhook requests from LiteLLM to budget alert endpoint
# Prevents unauthorized budget alert submissions from external sources
# Generate with: openssl rand -hex 32
# Used by: src/api/budget.py (webhook endpoint) for signature validation
# SECURITY: Store securely - compromised value allows spoofed budget alerts
AI_AGENTS_LITELLM_WEBHOOK_SECRET=your-webhook-secret-here-use-openssl-rand-hex-32

# Anthropic API Key (for Claude models, optional fallback provider)
# Get from: https://console.anthropic.com/settings/keys
# Format: sk-ant-...
# Used in fallback chain: gpt-4 → azure-gpt-4 → claude-3-5-sonnet
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# Azure OpenAI Configuration (optional fallback provider)
# Get from: https://portal.azure.com (Azure OpenAI Service)
# Used in fallback chain: gpt-4 → azure-gpt-4 → claude-3-5-sonnet

# Azure OpenAI API Key
# Format: hexadecimal key from Azure portal
AZURE_API_KEY=your-azure-openai-api-key-here

# Azure OpenAI Endpoint URL
# Format: https://your-resource.openai.azure.com/
# Example: https://my-ai-resource.openai.azure.com/
AZURE_API_BASE=https://your-resource.openai.azure.com/

# API Configuration (future stories)
# AI_AGENTS_API_HOST=0.0.0.0
# AI_AGENTS_API_PORT=8000

# ServiceDesk Plus Integration (Epic 2)
# AI_AGENTS_SERVICEDESK_API_URL=https://your-servicedesk-instance.com/api/v3
# AI_AGENTS_SERVICEDESK_API_KEY=your_api_key_here

# OpenRouter/LLM Configuration (Story 2.9)
# OpenRouter API key for multi-model LLM access via OpenAI SDK
# Get your key from: https://openrouter.ai/keys
# Format: sk-or-v1-...
AI_AGENTS_OPENROUTER_API_KEY=sk-or-v1-your-api-key-here

# OpenRouter API base URL (usually doesn't need to change)
AI_AGENTS_OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Site URL for HTTP-Referer header (used by OpenRouter for rankings/analytics)
# Set this to your deployment URL
AI_AGENTS_OPENROUTER_SITE_URL=https://ai-agents.yourcompany.com

# App name for X-Title header (used by OpenRouter for rankings)
AI_AGENTS_OPENROUTER_APP_NAME=AI Agents Enhancement Platform

# LLM Model selection (default: openai/gpt-4o-mini for cost efficiency)
# Options: openai/gpt-4o-mini (default), openai/gpt-4, openai/gpt-4-turbo, etc.
AI_AGENTS_LLM_MODEL=openai/gpt-4o-mini

# Maximum tokens for LLM response (default: 1000 ≈ 500 words)
# Increase if you need longer responses, decrease to save costs
AI_AGENTS_LLM_MAX_TOKENS=1000

# LLM temperature for output consistency (default: 0.3 for focused, deterministic output)
# Range: 0.0 (deterministic) to 1.0 (creative)
# 0.3 recommended for synthesis (consistent output), 0.7+ for creative tasks
AI_AGENTS_LLM_TEMPERATURE=0.3

# Timeout for LLM API calls in seconds (default: 30)
# Range: 5-120 seconds
# Set to 30 to provide budget within 120s overall enhancement workflow timeout
AI_AGENTS_LLM_TIMEOUT_SECONDS=30

# Monitoring (Epic 4)
# AI_AGENTS_METRICS_PORT=9090
AI_AGENTS_PROMETHEUS_URL=http://prometheus:9090

# =============================================================================
# Kubernetes Configuration (Story 6.7 - Worker Health Monitoring)
# =============================================================================
# Kubernetes namespace for worker operations
# Default: ai-agents (should match your deployment namespace)
AI_AGENTS_KUBERNETES_NAMESPACE=ai-agents

# Whether running inside Kubernetes cluster
# true = use in-cluster config (ServiceAccount credentials)
# false = use local kubeconfig file (~/.kube/config)
AI_AGENTS_KUBERNETES_IN_CLUSTER=true

# Number of log lines to fetch from worker pods (default: 100)
# Range: 10-1000
AI_AGENTS_WORKER_LOG_LINES=100

# Celery application name for worker discovery
# Must match the Celery app name configured in src/workers/celery_app.py
AI_AGENTS_CELERY_APP_NAME=ai_agents

# =============================================================================
# Distributed Tracing Configuration (Story 4.6 + Story 12.8)
# =============================================================================
# OpenTelemetry Configuration for Distributed Tracing
# Provides end-to-end visibility into ticket enhancement workflow across
# FastAPI (webhook) → Redis queue → Celery workers → External APIs
#
# Backend Options:
# - Jaeger: Lightweight, in-memory, ideal for local development (Story 4.6)
# - Uptrace: Production-grade with persistent storage, alerting, SQL queries (Story 12.8)

# Tracing Backend Selection (Story 12.8 AC4)
# Options: jaeger, uptrace
# Development: Use "jaeger" for local tracing with Jaeger all-in-one container
# Production: Use "uptrace" for persistent storage and advanced analytics
OTEL_BACKEND=jaeger

# Jaeger Collector Endpoint (gRPC protocol on port 4317)
# Used when OTEL_BACKEND=jaeger
# Local development: Use 'jaeger' as hostname when running in Docker
# Format: http://host:4317
OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317

# Service Name
# Identifies this service in Jaeger/Uptrace traces and metrics
# Appears in UI service dropdown
OTEL_SERVICE_NAME=ai-agents-enhancement

# Trace Sampling Strategy
# Options: always_off, always_on, traceidratio, parentbased_always_off, parentbased_always_on, parentbased_traceidratio
# Recommended: traceidratio for percentage-based sampling
OTEL_TRACES_SAMPLER=traceidratio

# Sampling Rate (0.0 to 1.0)
# 0.0 = no traces sampled (disable tracing)
# 0.01 = 1% sampling (10 traces/day for 1000 tickets, ~1MB/day storage)
# 0.1 = 10% sampling (100 traces/day for 1000 tickets, ~10MB/day storage) [DEFAULT]
# 0.5 = 50% sampling (500 traces/day, ~50MB/day storage)
# 1.0 = 100% sampling (all traces, production not recommended for high volume)
# For development/testing: Use 1.0
# For staging: Use 0.5
# For production: Use 0.01-0.1 depending on traffic and storage budget
OTEL_TRACES_SAMPLER_ARG=0.1

# Deployment Environment Tag
# Used to tag all spans with deployment environment for filtering in tracing UI
# Options: development, staging, production
DEPLOYMENT_ENV=development

# =============================================================================
# Uptrace Configuration (Story 12.8 AC4 - Production Tracing Backend)
# =============================================================================
# Uptrace provides production-grade distributed tracing with:
# - PostgreSQL/ClickHouse backend for persistent trace storage
# - Built-in alerting on trace patterns (e.g., alert when p95 MCP latency >5s)
# - SQL-like query language for advanced trace search
# - Automatic trace sampling and aggregation for cost optimization
#
# Usage: Set OTEL_BACKEND=uptrace to enable Uptrace backend

# Uptrace OTLP Endpoint (gRPC protocol on port 4317)
# Format: https://your-uptrace-instance.com:4317
# Get from Uptrace dashboard → Project Settings → OTLP Endpoint
UPTRACE_OTLP_ENDPOINT=https://uptrace.example.com:4317

# Uptrace Data Source Name (DSN) for Authentication
# Format: https://PROJECT_ID:TOKEN@uptrace.example.com
# Get from Uptrace dashboard → Project Settings → DSN
# SECURITY: Store as Kubernetes Secret in production (k8s/uptrace-secret.yaml)
# DO NOT commit actual DSN to version control - use placeholder only
UPTRACE_DSN=https://PROJECT_ID:TOKEN@uptrace.example.com

# Batch Span Processor Configuration (Advanced)
# These settings control how spans are batched and exported to Jaeger
# Typically no changes needed, but tuning available for high-throughput scenarios:
#
# Max spans to buffer before forcing export:
#   - Default: 2048
#   - High-throughput: 4096-8192
#   - Low-memory: 512-1024
# OTEL_BSP_MAX_QUEUE_SIZE=2048
#
# Delay between exports (milliseconds):
#   - Default: 5000 (5 seconds)
#   - Faster export: 500-1000
#   - Batch optimization: 10000+
# OTEL_BSP_SCHEDULE_DELAY_MILLIS=5000
#
# Max spans per batch:
#   - Default: 512
#   - For larger batches: 1024-2048
# OTEL_BSP_MAX_EXPORT_BATCH_SIZE=512

# =============================================================================
# Alertmanager Configuration (Story 4.5)
# =============================================================================
# Slack Integration for Alert Notifications
# Create an Incoming Webhook in your Slack workspace:
#   1. Go to Slack App Directory → search "Incoming WebHooks"
#   2. Create New Webhook for your alert channel
#   3. Copy the webhook URL and paste here
# Format: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/TXXXXXX/BXXXXXX/your-webhook-token-here

# PagerDuty Integration for Critical Alert Escalation
# Set up PagerDuty integration in your service:
#   1. Go to PagerDuty → Service Settings → Integrations
#   2. Add integration for "Prometheus" or generic webhook
#   3. Copy the routing/integration key
# Format: String with integration key provided by PagerDuty
PAGERDUTY_INTEGRATION_KEY=your-pagerduty-integration-key-here

# SMTP Configuration for Email Alerts (Optional)
# SMTP server endpoint and credentials for email alert delivery
# Common providers: smtp.gmail.com (587), smtp.sendgrid.net (587), your-company-smtp (25/587)
SMTP_SMARTHOST=smtp.example.com:587

# SMTP authentication username (usually email address)
# For Gmail: your-email@gmail.com
# For SendGrid: apikey
# For company SMTP: your-username
SMTP_USERNAME=your-smtp-username@example.com

# SMTP authentication password
# For Gmail: use App Password (not regular password)
# For SendGrid: use API key as password
# For company SMTP: use your password
SMTP_PASSWORD=your-smtp-password-here

# Email sender address (the From: address for alert emails)
# This should be a valid email address
EMAIL_FROM=alerts@ai-agents.local

# Email recipient address (where alerts are sent)
# Separate multiple addresses with commas if supported by your setup
EMAIL_RECIPIENT=oncall@example.com

# =============================================================================
# MCP Bridge Pooling (Story 11.2.3)
# =============================================================================
# Bridge-level pooling optimizes agent execution by reusing MCPToolBridge instances
# within the same execution context (not a full connection pool manager)
# Performance: 10x improvement (90% reduction in bridge initialization overhead)
# Note: No configuration needed - pooling uses execution context lifecycle

# =============================================================================
# Jira MCP Server Configuration
# =============================================================================
# Jira MCP Server provides AI agents with tools to interact with Jira Cloud:
# - jira_add_comment: Post comments to issues
# - jira_get_issue: Retrieve issue details
# - jira_search_issues: Search using JQL
# - jira_create_issue: Create new issues
# - and 20+ more tools for boards, projects, time tracking, etc.
#
# How to get Jira API Token:
#   1. Log in to Atlassian (https://id.atlassian.com)
#   2. Go to Security → API Tokens
#   3. Click "Create API token"
#   4. Give it a label (e.g., "AI Agents MCP Server")
#   5. Copy the token and paste below

# Jira Cloud instance base URL
# Format: https://your-company.atlassian.net (NO trailing slash)
# Example: https://acme-corp.atlassian.net
JIRA_BASE_URL=https://your-company.atlassian.net

# Jira user email (email associated with your Atlassian account)
# This email will be used for API authentication
# Format: your-email@company.com
JIRA_EMAIL=your-email@company.com

# Jira API Token (NOT your Atlassian password!)
# Generate from: https://id.atlassian.com/manage-profile/security/api-tokens
# Format: alphanumeric string (e.g., ATATT3xFfGF0...)
# SECURITY: Store securely - grants access to Jira API
JIRA_API_TOKEN=your-jira-api-token-here

# Jira MCP Server Log Level (optional)
# Options: ERROR, WARN, INFO, DEBUG
# Default: INFO
JIRA_MCP_LOG_LEVEL=INFO
