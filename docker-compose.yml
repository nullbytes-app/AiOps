version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:17-alpine
    container_name: ai-agents-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-aiagents}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
      POSTGRES_DB: ${POSTGRES_DB:-ai_agents}
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-aiagents}"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  # Redis Cache and Message Broker
  redis:
    image: redis:7-alpine
    container_name: ai-agents-redis
    command: redis-server --appendonly yes
    volumes:
      - ./data/redis:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  # FastAPI Application
  api:
    build:
      context: .
      dockerfile: docker/backend.dockerfile
    container_name: ai-agents-api
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    env_file:
      - .env
    volumes:
      - ./src:/app/src
      - ./tests:/app/tests
      - ./alembic:/app/alembic
      - ./alembic.ini:/app/alembic.ini
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Celery Worker for Async Task Processing
  worker:
    build:
      context: .
      dockerfile: docker/backend.dockerfile
    container_name: ai-agents-worker
    entrypoint: []
    command: celery -A src.workers.celery_app worker --loglevel=info --concurrency=4
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    env_file:
      - .env
    volumes:
      - ./src:/app/src
    healthcheck:
      test: ["CMD-SHELL", "celery -A src.workers.celery_app inspect ping -d celery@$$HOSTNAME"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Prometheus Metrics Server
  prometheus:
    image: prom/prometheus:latest
    container_name: ai-agents-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.retention.time=30d'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    depends_on:
      api:
        condition: service_healthy
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./alert-rules.yml:/etc/prometheus/alert-rules.yml:ro
      - ./data/prometheus:/prometheus
    ports:
      - "9091:9090"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  # Alertmanager for Alert Routing and Notifications
  # Story 4.5: Integrate Alertmanager for Alert Routing
  alertmanager:
    image: prom/alertmanager:latest
    container_name: ai-agents-alertmanager
    depends_on:
      prometheus:
        condition: service_healthy
    environment:
      # Load webhook URLs and API keys from environment variables
      SLACK_WEBHOOK_URL: ${SLACK_WEBHOOK_URL:-}
      PAGERDUTY_INTEGRATION_KEY: ${PAGERDUTY_INTEGRATION_KEY:-}
      SMTP_SMARTHOST: ${SMTP_SMARTHOST:-}
      SMTP_USERNAME: ${SMTP_USERNAME:-}
      SMTP_PASSWORD: ${SMTP_PASSWORD:-}
      EMAIL_FROM: ${EMAIL_FROM:-}
      EMAIL_RECIPIENT: ${EMAIL_RECIPIENT:-}
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - ./data/alertmanager:/alertmanager
    ports:
      - "9093:9093"
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  # Grafana Dashboards for Real-Time Monitoring
  # Note: macOS Docker Desktop - use port 3000 when full stack is running
  # (requires prometheus service healthy)
  grafana:
    image: grafana/grafana:latest
    container_name: ai-agents-grafana
    depends_on:
      prometheus:
        condition: service_healthy
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      GF_SERVER_ROOT_URL: "http://localhost:3002"
    volumes:
      # Provision Prometheus datasource (plain Grafana provisioning YAML, not a K8s ConfigMap)
      - ./k8s/grafana-datasource-provision.yaml:/etc/grafana/provisioning/datasources/prometheus.yaml:ro
      # Provision dashboards provider config (points to /var/lib/grafana/dashboards)
      - ./k8s/grafana-dashboard-provision.yaml:/etc/grafana/provisioning/dashboards/dashboard.yaml:ro
      # Mount dashboards JSON definitions
      - ./dashboards:/var/lib/grafana/dashboards:ro
      # Persistent Grafana data volume
      - grafana_data:/var/lib/grafana
    ports:
      - "3002:3000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Jaeger Distributed Tracing Backend
  # Story 4.6: Implement Distributed Tracing with OpenTelemetry
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: ai-agents-jaeger
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - COLLECTOR_OTLP_HOST_PORT=0.0.0.0:4317
    volumes:
      - ./data/jaeger:/badger
    ports:
      # Jaeger UI
      - "16686:16686"
      # OTLP gRPC receiver
      - "4317:4317"
      # OTLP HTTP receiver
      - "4318:4318"
      # Jaeger agent (used for old client libraries)
      - "6831:6831/udp"
      - "6832:6832/udp"
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:16686/"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  # Streamlit Admin UI
  # Story 6.1-6.5: Admin interface for operations management
  streamlit:
    build:
      context: .
      dockerfile: docker/streamlit.dockerfile
    container_name: ai-agents-streamlit
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    env_file:
      - .env
    environment:
      # Override database URL to use docker-compose postgres service (async driver for imports)
      AI_AGENTS_DATABASE_URL: postgresql+asyncpg://aiagents:password@postgres:5432/ai_agents
      AI_AGENTS_REDIS_URL: redis://redis:6379/1
      AI_AGENTS_CELERY_BROKER_URL: redis://redis:6379/1
      AI_AGENTS_CELERY_RESULT_BACKEND: redis://redis:6379/1
      API_BASE_URL: http://api:8000
      STREAMLIT_LOCAL_DEV: "true"
    volumes:
      - ./src:/app/src
      - ./.streamlit:/home/appuser/.streamlit
    ports:
      - "8501:8501"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8501/_stcore/health')"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # LiteLLM Proxy - Unified LLM Gateway
  # Story 8.1: LiteLLM Proxy Integration
  litellm:
    image: ghcr.io/berriai/litellm-database:main-stable
    container_name: litellm-proxy
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env
    environment:
      DATABASE_URL: postgresql://aiagents:password@postgres:5432/ai_agents
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}
      LITELLM_SALT_KEY: ${LITELLM_SALT_KEY}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      AZURE_API_KEY: ${AZURE_API_KEY:-}
      AZURE_API_BASE: ${AZURE_API_BASE:-}
      # Story 8.10C: Budget webhook alerting
      # LiteLLM sends budget threshold alerts to this endpoint
      WEBHOOK_URL: http://api:8000/api/v1/budget-alerts
    volumes:
      - ./config/litellm-config.yaml:/app/config.yaml:ro
    ports:
      - "4000:4000"
    command: ["--config", "/app/config.yaml", "--detailed_debug"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:
  alertmanager_data:
