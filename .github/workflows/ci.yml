name: AI Agents CI/CD Pipeline

# Workflow triggers: Run on push to main branch and all pull requests to main
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

# Workflow permissions: Minimum required for secure operation
permissions:
  contents: read        # Required to checkout code
  packages: write       # Required to push Docker images to registry

# Environment variables for consistency across jobs
env:
  REGISTRY: ghcr.io
  API_IMAGE_NAME: ai-agents-api
  WORKER_IMAGE_NAME: ai-agents-worker

jobs:
  # Job 1: Code quality checks and unit tests
  # Runs on every PR and main branch commit
  lint-and-test:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Prevent hung jobs

    steps:
      # Step 1: Checkout repository code
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better analysis

      # Step 2: Set up Python 3.12 environment
      # Reason: Python 3.12 is the project's target version (as per tech spec and previous stories)
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      # Step 3: Cache pip dependencies
      # Reason: Caching dependencies saves ~30s per run by avoiding re-download
      # Key is based on pyproject.toml hash to invalidate cache when deps change
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      # Step 4: Install dependencies from pyproject.toml
      # Install editable mode with [dev] extras to get all dev tools (black, ruff, mypy, pytest, etc.)
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      # Step 5: Verify dependency installation
      # List all installed packages to confirm all required tools are present
      - name: Verify dependency installation
        run: pip list | grep -E "black|ruff|mypy|pytest"

      # Step 6: Code formatting check with Black
      # Reason: Enforces consistent Python code formatting (PEP8-compliant)
      # Black is configured in pyproject.toml with line-length=100
      # Run in check mode (--check) to fail if formatting issues found
      # Tip: Run `black src/ tests/` locally to auto-format code before pushing
      - name: Check code formatting (Black)
        run: black --check src/ tests/
        continue-on-error: false

      # Step 7: Linting with Ruff
      # Reason: Fast linter that checks for code quality issues, security patterns, unused imports
      # Ruff is configured in pyproject.toml with select rules: E, F, I, N, W
      # Common Ruff fixes:
      #   - Remove unused imports: `ruff check --fix src/` (--fix flag auto-fixes some issues)
      #   - Undefined variables: Add missing import or define variable
      #   - Line too long: Either refactor or adjust pyproject.toml line-length setting
      - name: Lint code (Ruff)
        run: ruff check src/ tests/
        continue-on-error: false

      # Step 8: Type checking with Mypy
      # Reason: Static type checking catches type errors before runtime
      # Mypy is configured in pyproject.toml with disallow_untyped_defs=true (strict mode)
      # --ignore-missing-imports allows use of untyped third-party libraries
      # Type checking benefits:
      #   - Catches typos in variable/method names
      #   - Prevents passing wrong argument types
      #   - Documents expected types in function signatures
      - name: Type check (Mypy)
        run: mypy src/ --ignore-missing-imports
        continue-on-error: false

      # Step 9: Security scanning with Bandit (Story 3.4 AC7)
      # Reason: Static analysis security linter for Python code
      # Bandit detects common security issues: hardcoded secrets, SQL injection risks, weak crypto
      # Configured in pyproject.toml [tool.bandit] with exclude_dirs and severity=MEDIUM
      # -ll flag reports medium and high severity issues
      # CI will fail on high/medium severity findings (exit code 1)
      # nosec comments can be added to lines to suppress false positives with justification
      - name: Run security scanning (Bandit)
        run: |
          pip install bandit[toml]
          python -m bandit -r src/ -ll
        continue-on-error: false

      # Step 9.4: File size enforcement (Story 12.7 AC3)
      # Reason: Enforces 500-line maximum for better AI code editor compatibility
      # Based on 2025 Python best practices research (150-500 line sweet spot)
      # Configuration in scripts/file-size-config.yaml excludes migrations, generated code
      # CI will fail if any non-excluded file exceeds threshold
      # Refactoring guide: docs/refactoring-guide.md
      - name: Check file sizes (Story 12.7)
        run: |
          pip install pyyaml
          python scripts/check-file-size.py
        continue-on-error: false

      # Step 9.5: Set up Node.js for MCP test server (Story 12.2 AC3)
      # Reason: MCP integration tests require @modelcontextprotocol/server-everything npm package
      # Tests using mcp_stdio_client fixture need this dependency
      # Version 18+ required per package.json engines specification
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      # Step 9.6: Install MCP test server dependencies (Story 12.2 AC3)
      # Reason: Install @modelcontextprotocol/server-everything for MCP integration tests
      # Tests will skip gracefully if npx not available (via skip_if_no_mcp_server fixture)
      # But in CI we want all tests to run, including MCP integration tests
      - name: Install MCP test dependencies
        run: npm install

      # Step 10: Run unit tests with coverage
      # Reason: Validates code functionality and generates coverage report
      # Coverage must be >= 80% (per tech spec requirement)
      # pytest configuration in pyproject.toml specifies:
      #   - testpaths: tests/
      #   - addopts: --verbose, -ra (show all test details)
      # Coverage report generated in both terminal and XML format (for CI service integration)
      - name: Run unit tests with coverage
        run: |
          pytest tests/ \
            --cov=src \
            --cov-report=term \
            --cov-report=xml \
            --cov-report=html \
            --cov-fail-under=80
        continue-on-error: false

      # Step 10: Upload coverage report as artifact
      # Reason: Allows review of coverage report in GitHub Actions UI
      # XML report can be integrated with external coverage services
      - name: Upload coverage report
        uses: actions/upload-artifact@v3
        if: always()  # Upload even if tests fail
        with:
          name: coverage-report
          path: coverage.xml
          retention-days: 30

      # Step 11: Upload HTML coverage report for visualization
      - name: Upload HTML coverage report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: coverage-html
          path: htmlcov/
          retention-days: 30

      # Step 12: Run plugin unit tests (Story 7.6 - AC7)
      # Reason: Isolated execution of plugin unit tests for better visibility
      # Tests MockTicketingToolPlugin and plugin test utilities
      # Uses pytest markers: -m "unit and plugin"
      # Expected: 10+ tests covering factory methods, call tracking, custom configuration
      - name: Run plugin unit tests
        run: |
          pytest -m "unit and plugin" -v --tb=short
        continue-on-error: false

      # Step 13: Run plugin integration tests (Story 7.6 - AC7)
      # Reason: Tests full enhancement workflow with mock plugins
      # Validates integration between plugin, PluginManager, workflow components
      # Uses pytest markers: -m "integration and plugin"
      # Expected: 10+ tests covering success paths, failure modes, edge cases
      - name: Run plugin integration tests
        run: |
          pytest -m "integration and plugin" -v --tb=short
        continue-on-error: false

      # Step 14: Run security tests (Story 3.8 - AC1, AC2, AC3, AC4)
      # Reason: Automated security testing for OWASP Top 10 vulnerabilities
      # Security tests validate tenant isolation, input validation, webhook signatures
      # Tests must pass (exit code 0) to block merge on failures
      - name: Run security tests
        run: |
          pytest tests/security/ -v --tb=short
        continue-on-error: false

      # Step 13: Dependency scanning with safety (Story 3.8 - AC5)
      # Reason: Identifies known vulnerabilities in Python dependencies
      # Blocks merge on CRITICAL or HIGH severity vulnerabilities
      # Scans against Python Packaging Advisory Database
      - name: Run dependency scanning (safety)
        run: |
          pip install safety
          safety check -r requirements.txt --json
        continue-on-error: false

      # Step 14: Dependency scanning with pip-audit (Story 3.8 - AC5)
      # Reason: Additional supply chain security scanning tool
      # Detects CVEs and provides remediation guidance
      - name: Run dependency scanning (pip-audit)
        run: |
          pip install pip-audit
          pip-audit
        continue-on-error: false

      # Step 15: Generate pytest JSON report for baseline enforcement (Story 12.6 - AC4)
      # Reason: Baseline enforcement needs JSON output for threshold checks
      # Saved as artifact for ci-baseline-check.py to consume
      - name: Generate pytest JSON report
        if: always()  # Run even if tests fail
        run: |
          pytest tests/ \
            --json-report \
            --json-report-file=pytest-results.json \
            -q \
            --tb=no
        continue-on-error: true  # Don't fail on test failures (baseline check will handle)

      # Step 16: Check baseline thresholds (Story 12.6 - AC4, AC7)
      # Reason: Enforces configurable quality gates from ci-baseline-config.yaml
      # Blocks merge if CRITICAL thresholds breached
      # Warns if WARNING thresholds breached (merge allowed)
      - name: Check baseline thresholds
        run: |
          python scripts/ci-baseline-check.py \
            --config scripts/ci-baseline-config.yaml \
            --results pytest-results.json
        continue-on-error: false

      # Step 17: Upload test results as artifact
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: pytest-results
          path: pytest-results.json
          retention-days: 30

  # Job 2: E2E UI Workflow Tests with Playwright (Story 12.5)
  # Runs after lint-and-test job passes
  # Tests critical UI workflows to prevent integration bugs (Story 11.2.5 regression)
  e2e-tests:
    runs-on: ubuntu-latest
    needs: lint-and-test
    timeout-minutes: 10

    steps:
      # Step 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v4

      # Step 2: Set up Python 3.12
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      # Step 3: Cache pip dependencies
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      # Step 4: Install dependencies (including Playwright)
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      # Step 5: Install Playwright browsers
      # Reason: Playwright requires browser binaries for automation
      # --with-deps installs system dependencies (fonts, libs) needed by Chromium
      - name: Install Playwright browsers
        run: playwright install --with-deps chromium

      # Step 6: Set up Node.js for MCP test server
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      # Step 7: Install MCP test dependencies
      - name: Install MCP test dependencies
        run: npm install

      # Step 8: Start Streamlit app in background
      # Reason: E2E tests require running Streamlit app
      # Run on port 8502 to avoid conflicts
      # TESTING=true disables authentication for E2E tests
      - name: Start Streamlit app
        run: |
          TESTING=true streamlit run src/admin/app.py --server.port=8502 &
          echo $! > streamlit.pid
        env:
          TESTING: true

      # Step 9: Wait for Streamlit app to be healthy
      # Reason: Must ensure app is ready before tests run
      # Retry up to 20 times (40 seconds) with 2-second delays
      - name: Wait for Streamlit app
        run: |
          for i in {1..20}; do
            if curl -f http://localhost:8502/_stcore/health; then
              echo "Streamlit app is healthy"
              break
            fi
            echo "Waiting for Streamlit app... (attempt $i/20)"
            sleep 2
          done

      # Step 10: Run E2E tests with Playwright
      # Reason: Validates 3 critical UI workflows
      # Tests prevent Story 11.2.5 style bugs (UI integration failures)
      # pytest-playwright configured in tests/e2e/pytest.ini
      # --screenshot=on captures screenshots on failure
      # --video=retain-on-failure records video of failed tests
      - name: Run E2E tests
        run: |
          pytest tests/e2e/ \
            -v \
            --tb=short \
            --screenshot=only-on-failure \
            --video=retain-on-failure \
            --tracing=retain-on-failure
        continue-on-error: false

      # Step 11: Upload test artifacts on failure
      # Reason: Screenshots, videos, traces help debug E2E test failures
      # Artifacts available in GitHub Actions UI
      - name: Upload E2E test artifacts
        uses: actions/upload-artifact@v3
        if: failure()
        with:
          name: e2e-test-artifacts
          path: |
            tests/e2e/screenshots/
            tests/e2e/videos/
            tests/e2e/traces/
            test-results/
          retention-days: 30

      # Step 12: Stop Streamlit app
      - name: Stop Streamlit app
        if: always()
        run: |
          if [ -f streamlit.pid ]; then
            kill $(cat streamlit.pid) || true
          fi

  # Job 3: Docker image build and push
  # Depends on lint-and-test job passing (sequential execution)
  # Only pushes images on main branch (not on PRs)
  docker-build:
    runs-on: ubuntu-latest
    needs: lint-and-test  # Only run after lint-and-test succeeds
    timeout-minutes: 15

    steps:
      # Step 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v4

      # Step 2: Set up Docker Buildx
      # Reason: Buildx is modern Docker build engine with layer caching support
      # Layer caching saves ~60s per build by reusing unchanged layers
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Step 3: Log in to GitHub Container Registry (ghcr.io)
      # Reason: Authentication required for pushing private images
      # Uses secrets.GITHUB_TOKEN (automatically provided by GitHub Actions)
      # Security best practice: Never hardcode credentials in workflow
      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # Step 4: Extract repository name for image tagging
      # Converts github.repository (owner/repo) to lowercase for Docker compliance
      - name: Extract repository name
        id: repo
        run: echo "name=$(echo '${{ github.repository }}' | tr '[:upper:]' '[:lower:]')" >> $GITHUB_OUTPUT

      # Step 5: Build API Docker image
      # Reason: Validates Dockerfile syntax and builds working container
      # Image tagged with:
      #   - 'test' tag for PR verification
      #   - Commit SHA for traceability (full 40-char SHA)
      - name: Build API Docker image
        run: |
          docker build \
            -f docker/backend.dockerfile \
            -t ai-agents-api:test \
            -t ai-agents-api:${{ github.sha }} \
            .

      # Step 6: Build Worker Docker image
      # Reason: Validates Worker Dockerfile and ensures both services build
      # Celery worker image required for task processing
      - name: Build Worker Docker image
        run: |
          docker build \
            -f docker/celeryworker.dockerfile \
            -t ai-agents-worker:test \
            -t ai-agents-worker:${{ github.sha }} \
            .

      # Step 7: Verify images built successfully
      # Reason: Confirms both images are present and have correct naming
      - name: Verify Docker images built
        run: docker images | grep -E "ai-agents-api|ai-agents-worker"

      # Step 8: Tag and push API image to registry (main branch only)
      # Reason: Docker push should only happen on main branch to prevent PR commits from creating images
      # if condition ensures this job only pushes on main, not on PRs
      # Images tagged with:
      #   - 'latest' for stable production image
      #   - Commit SHA for specific version reference
      - name: Push API image to registry
        if: github.ref == 'refs/heads/main'
        run: |
          docker tag ai-agents-api:test ${{ env.REGISTRY }}/${{ steps.repo.outputs.name }}/${{ env.API_IMAGE_NAME }}:latest
          docker tag ai-agents-api:test ${{ env.REGISTRY }}/${{ steps.repo.outputs.name }}/${{ env.API_IMAGE_NAME }}:${{ github.sha }}
          docker push ${{ env.REGISTRY }}/${{ steps.repo.outputs.name }}/${{ env.API_IMAGE_NAME }}:latest
          docker push ${{ env.REGISTRY }}/${{ steps.repo.outputs.name }}/${{ env.API_IMAGE_NAME }}:${{ github.sha }}

      # Step 9: Tag and push Worker image to registry (main branch only)
      - name: Push Worker image to registry
        if: github.ref == 'refs/heads/main'
        run: |
          docker tag ai-agents-worker:test ${{ env.REGISTRY }}/${{ steps.repo.outputs.name }}/${{ env.WORKER_IMAGE_NAME }}:latest
          docker tag ai-agents-worker:test ${{ env.REGISTRY }}/${{ steps.repo.outputs.name }}/${{ env.WORKER_IMAGE_NAME }}:${{ github.sha }}
          docker push ${{ env.REGISTRY }}/${{ steps.repo.outputs.name }}/${{ env.WORKER_IMAGE_NAME }}:latest
          docker push ${{ env.REGISTRY }}/${{ steps.repo.outputs.name }}/${{ env.WORKER_IMAGE_NAME }}:${{ github.sha }}

      # Step 10: Verify images in registry (informational only)
      - name: Confirm images pushed
        if: github.ref == 'refs/heads/main'
        run: |
          echo "API image pushed: ${{ env.REGISTRY }}/${{ steps.repo.outputs.name }}/${{ env.API_IMAGE_NAME }}:latest"
          echo "Worker image pushed: ${{ env.REGISTRY }}/${{ steps.repo.outputs.name }}/${{ env.WORKER_IMAGE_NAME }}:latest"
